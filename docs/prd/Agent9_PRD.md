# Agent9 Primary Product Requirements Document
<!--- autogenerated, do not edit manually. run scripts/assemble_agent9_prd.py -->


---
<!-- Begin docs\prd\Agent9_Blueprint.md -->

# Agent9 Blueprint

_A high-level architecture & standards guide consolidating essential guidance for the Agent9 platform._  
**Audience:** architects, contributors, reviewers, Cascade.

---

## 1. Layered Reference Model
```
┌─────────────────────────┐
│     Interface Layer     │  (Decision Studio UI, HTTP/CLI APIs)
├─────────────────────────┤
│   Orchestration Layer   │  (A9_Orchestrator_Agent, LangGraph workflows)
├─────────────────────────┤
│    Capability Layer     │  (Situation, Analysis, Solution, etc.)
├─────────────────────────┤
│    Governance Layer     │  (Data Governance, KPI Registry, Compliance)
├─────────────────────────┤
│        Data Layer       │  (DuckDB test data, SAP Datasphere, contracts)
└─────────────────────────┘
```
Each agent & service maps to exactly **one primary layer**; cross-layer calls must traverse _downward_ only via the orchestrator.

## 2. Shared State Schema
All agents exchange a **typed Pydantic `Context`** object.
```python
from pydantic import BaseModel
class Context(BaseModel):
    principal_id: str
    workflow_id: str
    situation: SituationContext | None = None
    analysis: AnalysisContext | None = None
    solution: SolutionContext | None = None
    metadata: dict = {}
```
• `Context` is **append-only**; agents must not mutate previous sections.  
• Persist interim state via the **Agent Registry** at workflow transitions.

## 3. Tracer-Bullet MVP Workflow
1. `A9_Situation_Awareness_Agent` populates `context.situation`.  
2. `A9_Deep_Analysis_Agent` consumes it, writes `context.analysis`.  
3. `A9_Solution_Finder_Agent` consumes analysis, writes `context.solution`.  
4. HITL checkpoints via email action links.  
All orchestrated by `A9_Orchestrator_Agent` using **async `create_from_registry`** calls.

## 3.1 Workflow Catalogue (YAML source-of-truth)
| Workflow | YAML Path | Typical Trigger | Key Agents |
|----------|-----------|-----------------|------------|
| Automated Situational Awareness | `workflow_definitions/automated_situational_awareness.yaml` | Scheduled daily run / KPI update event | Orchestrator → KPI/Data agents → Situation Awareness |
| Problem Deep Analysis | `workflow_definitions/problem_deep_analysis.yaml` | HITL request after situation flagged | Deep Analysis, Data Product, Orchestrator |
| Opportunity Deep Analysis | `workflow_definitions/opportunity_deep_analysis.yaml` | HITL request or Innovation workflow | Opportunity Analysis, Deep Analysis |
| Solution Finding | `workflow_definitions/solution_finding.yaml` | After analysis / innovation | Solution Finder, Implementation Planner |
| Solution Deployment | `workflow_definitions/solution_deployment.yaml` | Once solution approved | Deployment Service / Implementation Tracker |
| Business Optimization | `workflow_definitions/business_optimization.yaml` | Executive optimisation initiative | Business Optimization agent suite |
| Innovation Driver | `workflow_definitions/innovation_driver.yaml` | Innovation campaign kickoff | Innovation Driver, GenAI Expert |
| Value Assurance | `workflow_definitions/value_assurance.yaml` | Post-deployment monitoring | Situation Awareness, KPI Registry |
| Environment Administration | `workflow_definitions/agent9_environment_administration.yaml` | Scheduled maintenance | Environment Admin, Registry services |

Each YAML defines nodes, edges, validations and outputs; the orchestrator loads it at runtime to build a LangGraph DAG.

## 4. Design-Standards Checklist (CI-Enforced)
| Rule | Target | Notes |
|------|--------|-------|
| File length ≤ 300 lines | *.py | Skip generated code |
| `A9_` prefix & naming convention | agents/* | See memory rules |
| Strict A2A I/O (Pydantic) | entrypoints | No raw dicts |
| Async registry creation | create_from_registry | No direct instantiation |
| Structured logging | all modules | `logger = structlog.get_logger()` |
| Error classes | agents/errors.py | ConfigurationError, ProcessingError, ValidationError |

## 5. Runtime KPIs & SLAs
| KPI | Target |
|-----|--------|
| Design turnaround | ≤ 8 h |
| Post-implementation defect rate | ≤ 5 % |
| Situation Awareness latency | < 2 min per run |
| Daily KPI coverage | 20 KPIs |

## 6. Cost-Quality Monitoring
Monitor token spend & composite quality score per build; flag regressions if cost ↑10 % without quality gain.

## 7. Open Questions
1. Agent unavailability strategy.  
2. Auth mechanism for internal calls (JWT vs mTLS).  
3. Compliance audit logging detail.

---
*Version: 0.1  |  Generated 2025-08-30*

<!-- End docs\prd\Agent9_Blueprint.md -->

---
<!-- Begin docs\prd\index.md -->




## Agent PRDs

| Agent | PRD Location | Description |
|-------|-------------|-------------|

| Orchestrator Agent | [agents/a9_orchestrator_agent_prd.md](agents/a9_orchestrator_agent_prd.md) | Coordinates innovation workflow and agent collaboration |
| Principal Context Agent | [agents/a9_principal_context_agent_prd.md](agents/a9_principal_context_agent_prd.md) | Manages principal context and preferences |
| Data Product Agent | [agents/a9_data_product_agent_prd.md](agents/a9_data_product_agent_prd.md) | Provides data product capabilities |
| Deep Analysis Agent | [agents/a9_deep_analysis_agent_prd.md](agents/a9_deep_analysis_agent_prd.md) | Performs deep analysis on situations |
| Situation Awareness Agent | [agents/a9_situation_awareness_agent_prd.md](agents/a9_situation_awareness_agent_prd.md) | Monitors KPIs and identifies situations |
| NLP Interface Agent | [agents/a9_nlp_interface_agent_prd.md](agents/a9_nlp_interface_agent_prd.md) | Provides natural language processing capabilities |
| LLM Service Agent | [agents/a9_llm_service_prd.md](agents/a9_llm_service_prd.md) | Manages LLM interactions |
| Solution Finder Agent | [agents/a9_solution_finder_agent_prd.md](agents/a9_solution_finder_agent_prd.md) | Identifies potential solutions to problems |
| Data Governance Agent | [agents/a9_data_governance_agent_prd.md](agents/a9_data_governance_agent_prd.md) | Ensures data governance compliance |
| Innovation Driver Agent | [agents/a9_innovation_driver_agent_prd.md](agents/a9_innovation_driver_agent_prd.md) | Drives innovation initiatives |
| Market Analysis Agent | [agents/a9_market_analysis_agent_prd.md](agents/a9_market_analysis_agent_prd.md) | Analyzes market trends and opportunities |
| Business Optimization Agent | [agents/a9_business_optimization_agent_prd.md](agents/a9_business_optimization_agent_prd.md) | Optimizes business processes |
| Change Management Agent | [agents/a9_change_management_agent_prd.md](agents/a9_change_management_agent_prd.md) | Manages organizational change |
| Implementation Tracker Agent | [agents/a9_implementation_tracker_agent_prd.md](agents/a9_implementation_tracker_agent_prd.md) | Tracks implementation progress |
| Stakeholder Analysis Agent | [agents/a9_stakeholder_analysis_agent_prd.md](agents/a9_stakeholder_analysis_agent_prd.md) | Analyzes stakeholder relationships |
| Stakeholder Engagement Agent | [agents/a9_stakeholder_engagement_agent_prd.md](agents/a9_stakeholder_engagement_agent_prd.md) | Manages stakeholder engagement |
| UI Design Agent | [agents/a9_ui_design_agent_prd.md](agents/a9_ui_design_agent_prd.md) | Provides user interface design capabilities |
| GenAI Expert Agent | [agents/a9_innovation_genai_expert_agent_prd.md](agents/a9_innovation_genai_expert_agent_prd.md) | Provides generative AI expertise |
| Opportunity Analysis Agent | [agents/a9_opportunity_analysis_agent_prd.md](agents/a9_opportunity_analysis_agent_prd.md) | Analyzes business opportunities |
| Performance Optimization Agent | [agents/a9_performance_optimization_agent_prd.md](agents/a9_performance_optimization_agent_prd.md) | Optimizes system performance |
| Quality Assurance Agent | [agents/a9_quality_assurance_agent_prd.md](agents/a9_quality_assurance_agent_prd.md) | Ensures quality standards |
| Risk Analysis Agent | [agents/a9_risk_analysis_agent_prd.md](agents/a9_risk_analysis_agent_prd.md) | Analyzes potential risks |
| Risk Management Agent | [agents/a9_risk_management_agent_prd.md](agents/a9_risk_management_agent_prd.md) | Manages identified risks |
| Solution Architect Agent | [agents/a9_solution_architect_agent_prd.md](agents/a9_solution_architect_agent_prd.md) | Designs solution architecture |

## Service PRDs

| Service | PRD Location | Description |
|---------|-------------|-------------|

| Data Product MCP Service | [services/a9_data_product_mcp_service_prd.md](services/a9_data_product_mcp_service_prd.md) | Provides data product capabilities as a microservice |

## Workflow Definitions

| Workflow | YAML Path |
|----------|-----------|
| Automated Situational Awareness | `workflow_definitions/automated_situational_awareness.yaml` |
| Problem Deep Analysis | `workflow_definitions/problem_deep_analysis.yaml` |
| Opportunity Deep Analysis | `workflow_definitions/opportunity_deep_analysis.yaml` |
| Solution Finding | `workflow_definitions/solution_finding.yaml` |
| Solution Deployment | `workflow_definitions/solution_deployment.yaml` |
| Business Optimization | `workflow_definitions/business_optimization.yaml` |
| Innovation Driver | `workflow_definitions/innovation_driver.yaml` |
| Value Assurance | `workflow_definitions/value_assurance.yaml` |
| Environment Administration | `workflow_definitions/agent9_environment_administration.yaml` |

---

## Implementation Guidelines

### Agent Development

1. **Protocol Compliance**
   - All agents must follow the A2A protocol
   - Use Pydantic models for input/output validation
   - Implement proper error handling and logging

2. **Registry Integration**
   - All agents must integrate with the Agent Registry
   - Use the `create_from_registry` factory method
   - Implement proper agent registration

3. **Testing**
   - Write unit tests for core functionality
   - Write integration tests with mock registry
   - Use test harnesses for end-to-end testing

### Service Development

1. **API Design**
   - Follow RESTful API design principles
   - Use OpenAPI/Swagger for API documentation
   - Implement proper error handling and status codes

2. **Integration**
   - Services should be containerizable
   - Support configuration via environment variables
   - Implement health check endpoints


- [Agent9 Agent Design Standards](../Agent9_Agent_Design_Standards.md)
- [Test Data Usage Guide](../Test_Data_Usage_Guide.md)
- [LLM Model Specifications](../LLM_Model_Specifications.md)
- [LLM Credit Estimation](../LLM_Credit_Estimation.md)


<!-- End docs\prd\index.md -->

---
<!-- Begin docs\prd\agents\a9_business_optimization_agent_prd.md -->

# A9_Business_Optimization_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->





### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Business_Optimization_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_business_optimization_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## Phase 1 Completion (2025-06-04)
- All Pydantic models and protocols are V2+ compliant and strictly enforced.
- No NoneType iterable errors possible (defensive coding for all iterables).
- All core and edge-case unit tests pass (no warnings, no deprecated usage).
- Orchestrator integration and agent card config are fully functional.
- Ready for MVP delivery and further integration.

---

## Overview
**Purpose:** Optimize business processes and outcomes through data-driven analysis, continuous improvement, and strategic recommendations.
**Agent Type:** Core Agent
**Version:** 1.0
**Template:** Follows A9_Agent_Template patterns

## Functional Requirements

### Core Capabilities
1. Process Optimization
   - Analyze business workflows
   - Identify inefficiencies and bottlenecks
   - Recommend process improvements
   - Track improvement outcomes

2. Performance Monitoring
   - Monitor key business metrics
   - Identify underperforming areas
   - Generate performance reports
   - Suggest corrective actions

3. Strategic Recommendations
   - Provide actionable insights for business growth
   - Support scenario analysis and forecasting
   - Recommend resource allocation strategies

### Integration Requirements (MVP Alignment)
- Support registry-based integration (A9_Agent_Template pattern)
- All optimization and analysis functions must be async
- Use absolute imports
- Standardize error handling (ConfigurationError, ProcessingError, ValidationError)
- Log optimization attempts, successes, and failures

### Test Requirements (MVP Alignment)
- Test process optimization recommendations (including edge cases)
- Test performance monitoring and reporting
- Test error handling for invalid input
- Test async operation and registry integration
- Document test coverage and assumptions

### Example Request/Response
```python
# Example: Optimize a business process
optimization_agent = ... # Registry lookup
recommendations = await optimization_agent.optimize_process({"workflow": "Order Fulfillment"})
# recommendations -> ["Reduce manual checks", "Automate inventory updates"]
```

### Input Requirements
1. Business Data
   - Workflow definitions
   - Performance metrics
   - Resource allocation data
   - Historical process data

2. Analysis Parameters
   - Optimization goals
   - Performance thresholds
   - Resource constraints

### Output Specifications
1. Optimization Artifacts
   - Process improvement plans
   - Performance reports
   - Strategic recommendations

2. Analytics
   - Optimization dashboards
   - Performance metrics
   - Scenario analysis results

## Technical Requirements

### Agent Implementation
- Follow A9_Agent_Template patterns
- Implement create_from_registry for registry-based instantiation
- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning Pydantic models for type safety, validation, and interoperability
- Use standard error handling patterns
- Support async operations
- **Pydantic Model Validation:** Uses A9BusinessOptimizationAgentConfig for strict config validation with exception handling for ValidationError
- **Defensive Configuration:** Ensures all required configuration fields exist with fallback values for missing fields
- **Structured Logging:** Uses A9_SharedLogger for consistent, structured logging with context
- **Numeric Value Parsing:** Uses centralized parse_numeric_value utility for consistent handling of numeric values with proper error handling
- **Market Report Integration:** Incorporates market insights from market_report if provided
- **Type Safety Enforcement:** Validates input types at runtime to ensure A2A protocol compliance
- **Comprehensive Error Logging:** Logs all errors with context for debugging and audit purposes
- **Metadata-Driven Validation:** Uses signal metadata to determine validation requirements (e.g., numeric values)

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for business processes and KPIs
- Must use registry data for context-aware optimization decisions
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

### Integration Points
1. Business Systems
   - Connect to ERP, CRM, and other business platforms
   - Interface with analytics tools
   - Integrate with reporting systems
   - Integrate with the Unified Registry Access Layer for business processes and KPIs

2. Output Systems
   - Generate reports
   - Create dashboards
   - Export metrics

### Performance Requirements
1. Analysis Time
   - Basic optimization: < 1 hour
   - Comprehensive optimization: < 4 hours
   - Real-time monitoring: < 15 minutes

2. Processing
   - Handle large business data volumes
   - Process complex workflows
   - Maintain data accuracy
   - Support concurrent operations

### Scalability
1. Support for multiple workflows
2. Handle large data volumes
3. Scale with increasing business complexity
4. Support cross-system analysis

## Error Handling
- Use standard error classes from A9_Agent_Template
- Error types:
  - ConfigurationError: Invalid configuration
  - RegistrationError: Failed to register with registry
  - ProcessingError: Failed to process data
  - ValidationError: Invalid input data
  - ConnectionError: Connection failures

## Security Requirements
1. Data Security
   - Secure business data access
   - Protect sensitive information
   - Secure audit trails
   - Secure documentation

2. Access Control
   - Role-based access
   - Secure data sharing
   - Audit trail for changes
   - Approval workflows

## Monitoring and Maintenance
1. Regular performance checks
2. Continuous process monitoring
3. Periodic optimization reviews
4. Regular access audits

## Success Metrics
1. Process efficiency improvement
2. Performance metric achievement
3. Recommendation adoption rate
4. Optimization impact
5. Business outcome enhancement

## Usage Flow
```
graph TD
    subgraph "Process Optimization"
        PO[Analyze Workflows] -->|Identify Inefficiencies| PO2[Recommend Improvements]
    end

    subgraph "Performance Monitoring"
        PM[Monitor Metrics] -->|Identify Issues| PM2[Generate Reports]
    end

    subgraph "Strategic Recommendations"
        SR[Provide Insights] -->|Support Decisions| SR2[Recommend Actions]
    end

    PO2 --> PM
    PM2 --> SR
    SR2 --> PO
```

## Compliance & Integration Update (2025-05-12)
- HITL (Human-in-the-Loop) enablement is fully implemented and enforced for all key actions:
  - The agent config supports a `hitl_enabled` flag.
  - Output protocol fields (`human_action_required`, `human_action_type`, `human_action_context`) are present and validated.
  - When HITL is enabled, all outputs require human approval and set these fields accordingly, with status 'pending_human_approval'.
- All integration and testing for this agent is orchestrator-driven and production-like:
  - Agent inputs and outputs are always real Pydantic model instances, never mocked or stubbed.
  - Integration tests simulate true production workflows, with the orchestrator coordinating all agent calls and data flows.
  - Static workflow tests and direct agent invocation in tests have been deprecated.
- Logging uses `A9_SharedLogger` and is propagated to the Orchestrator Agent.
- Error handling is standardized and protocol-compliant.
- Strict A2A protocol compliance (Pydantic models for all entrypoints/outputs) is maintained.
- Feedback, escalation, and resistance management are available for any optimization scenario.

## Notes
- Focuses on comprehensive business optimization
- Works with business systems for optimization
- Generates strategic recommendations
- Maintains optimization baselines
- Supports continuous business improvement

<!-- End docs\prd\agents\a9_business_optimization_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_change_management_agent_prd.md -->

# A9 Change Management Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Status (2025-06-04)
**Phase 1 (MVP) Complete**
- Protocol-compliant (A2A, Pydantic V2, registry/orchestrator integration)
- All unit and integration tests pass
- No deprecated patterns (config, logging, error handling, etc.)
- Agent is MVP-ready for all core change management scenarios
- Comprehensive implementation of change management models and analysis methods




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Change_Management_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_change_management_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## Purpose (Universal Change Enablement)
The A9 Change Management Agent provides comprehensive change management capabilities for any type of organizational change—including process, product, service, policy, and organizational changes (not just data)—ensuring smooth transitions and effective stakeholder engagement for all scenarios.

## Universal Change Enablement Vision (2025-04-30)
- The agent must support intake, planning, execution, and monitoring for any change type: process, product, service, policy, or organization.
- All features, models, and integration points must be agnostic to change type and support flexible context and stakeholder mapping.
- Event-driven handoff: The agent listens for `StakeholderEngagementCompleted` events (with sign-off, audit log, and change context) from the Stakeholder Engagement Agent for any change scenario.
- Must maintain strict A2A protocol compliance (Pydantic models for all entrypoints/outputs) and auditability for all change types.
- Feedback, escalation, and resistance management must be available for any change type.

## Compliance & Integration Update (2025-05-12)

- This agent is now fully compliant with Agent9 async, registry, error handling, and centralized logging standards.
- Follows the A9_Agent_Template pattern for config validation, registration, and logging (pattern, not inheritance).
- Registration is orchestrator-controlled (no self-registration or bootstrapping).
- All configuration is validated via Pydantic models.
- **HITL (Human-in-the-Loop) enablement is fully implemented and enforced for all key actions:**
  - Output protocol fields (`human_action_required`, `human_action_type`, etc.) are always present and validated.
  - HITL logic is enforced in code and verified by tests.
- **All integration and testing for this agent is orchestrator-driven and production-like:**
  - Agent inputs and outputs are always real Pydantic model instances, never mocked or stubbed.
  - Integration tests simulate true production workflows, with the orchestrator coordinating all agent calls and data flows.
  - Static workflow tests and direct agent invocation in tests have been deprecated.
- Test coverage is maintained via orchestrator-based persona and workflow scenarios, ensuring compliance with real-world business logic and end-to-end flows.
- Logging uses `A9_SharedLogger` and is propagated to the Orchestrator Agent.
- Error handling is standardized and protocol-compliant.
- Event-driven handoff: The agent listens for `StakeholderEngagementCompleted` events (with sign-off, audit log, and change context) from the Stakeholder Engagement Agent for any change scenario.
- Must maintain strict A2A protocol compliance (Pydantic models for all entrypoints/outputs) and auditability for all change types.
- Feedback, escalation, and resistance management must be available for any change type.

## Core Requirements

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for business processes, stakeholder information, and change initiatives
- Must use registry data for context-aware change management decisions
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

### Agent Template Compliance
- Implements A9_Agent_Template patterns
- Uses absolute imports
- Follows standardized error handling
- Implements create_from_registry method
- Uses consistent logging patterns

### Key Features
1. Change Management
   - Change impact analysis
   - Change readiness assessment
   - Change communication
   - Change execution
   - Change monitoring

2. Change Components
   - Impact Assessment
     - Business impact
     - Technical impact
     - Organizational impact
     - Cultural impact
   - Readiness Assessment
     - Stakeholder readiness
     - Resource readiness
     - System readiness
     - Process readiness
   - Communication Strategy
     - Stakeholder mapping
     - Communication plan
     - Change messaging
     - Feedback mechanisms
   - Execution Planning
     - Change phases
     - Resource allocation
     - Timeline management
     - Training requirements
   - Monitoring & Control
     - Key metrics
     - Progress tracking
     - Issue management
     - Risk management

3. Integration Points
   - Integrates with registry
   - Communicates with other agents (including event-driven handoff from Stakeholder Engagement Agent for any change type)
   - Handles configuration updates
   - Supports data validation
   - Interfaces with change management tools
   - Listens for and processes `StakeholderEngagementCompleted` events for all change scenarios
   - Emits audit and completion events for downstream reporting and analytics
   - Supports escalation and feedback integration for any change type

### Error Handling
- Uses standardized error classes
- Implements proper error logging
- Provides clear error messages
- Handles validation errors
- Handles change management failures
- Handles communication failures

## Implementation Guidelines

### Recent Technical Improvements (2025-06-04)
- All Pydantic models use `ConfigDict` (no deprecated `class Config`)
- Registry and orchestrator integration is robust and protocol-driven
- Defensive coding and error handling use standardized Agent9 patterns
- Centralized logging via `A9_SharedLogger`
- Unit and integration tests are protocol-compliant and pass without warnings or errors
- Robust timestamp handling with ISO format standardization
- Defensive type checking and data validation for all input models
- Structured error handling with specialized error classes
- Comprehensive model validation with fallback mechanisms

### Code Structure
- Single focused class
- Direct change management methods
- Clear data flow
- Basic but reliable performance
- Modular helper methods for component creation
- Defensive data handling with type checking
- Structured model generation for all outputs
- Timestamp standardization using ISO format
- Comprehensive input validation with fallback mechanisms

### Change Components
```python
class ChangeImpact:
    """Represents the impact of a change."""
    
    def __init__(self, category: str, level: float, description: str):
        """
        Initialize a change impact.
        
        Args:
            category: Impact category (business, technical, etc.)
            level: Impact level (0-1)
            description: Description of the impact
        """
        self.category = category
        self.level = level
        self.description = description
        self.mitigation = None
```

### Data Model
```python
class ChangeManagementPlan:
    """Represents a complete change management plan."""
    
    def __init__(self):
        self.change_impacts = []
        self.readiness_assessment = {
            'stakeholders': 0.0,
            'resources': 0.0,
            'systems': 0.0,
            'processes': 0.0
        }
        self.communication_plan = {
            'stakeholders': [],
            'messages': [],
            'timeline': [],
            'channels': []
        }
        self.execution_plan = {
            'phases': [],
            'resources': [],
            'timeline': None,
            'training': []
        }
        self.monitoring = {
            'metrics': [],
            'alerts': [],
            'reports': []
        }
        self.risk_management = {
            'risks': [],
            'mitigations': [],
            'contingencies': []
        }
        self.recommendations = []  # List of prioritized, actionable recommendations
        self.critical_alerts = []  # List of critical alerts for high-risk changes/adoption issues
        self.confidence_score = 0.0
        self.risk_level = 0.0
        self.timestamp = None
        self.value_realization = {
            'baseline': {},
            'projection': {},
            'target': {},
            'actual': {},
            'comparison': {}
        }
```

### Value Realization
- The agent is responsible for orchestrating value realization measurement and reporting. This includes:
  - **Explicit Change-to-KPI Linkage:** Each change must specify one or more KPIs it intends to impact, by name, in the change request input. (Not inferred by persona, story arc, or business process alone.)
  - The input model must include a `kpis` field: a list of KPI names (strings) referencing KPIs in the KPI registry.
  - The agent must use these declared KPIs as the sole basis for value realization tracking, reporting, and dashboard outputs for that change.
  - Capturing baseline, projected (counterfactual), and target KPI values at the start of the change.
  - Tracking actual KPI values over time (monthly, quarterly, on-demand).
  - Calculating value realized and value left (gap) for each KPI.
  - Providing structured explanations for value gaps (reason category, summary, details, source agent, confidence, recommendations).
  - Reporting value realization and value sharing in a dashboard-ready format.
  - Supporting demo outputs for UI and investor review.
  - Ensuring all value realization reporting is based on explicit, declared KPIs for each change (see above).
  - **Data Source Traceability:** The Data Product Agent and Data Governance Agent must be consulted or delegated to in order to identify and validate the authoritative data source for each KPI value (baseline, projection, target, actual). The Change Management Agent must integrate with these agents to ensure all KPI values used in value realization reporting are traceable, high quality, and auditable.

**Rationale:**
- This approach ensures traceability and business alignment for every change initiative.
- KPI selection is transparent and auditable, supporting clear value realization stories for leadership and stakeholders.
- Dashboards and reports will always reflect the KPIs that were actually targeted by the change, not just generic metrics.
- Delegating data source resolution to Data Product and Data Governance Agents ensures data quality, trust, and compliance.

### Agent9 Value Sharing (Optional Feature)

- For a limited period after implementation (e.g., 1 year), a configurable percentage of value realized may be shared with Agent9 as part of the value realization agreement.
- This value sharing period and percentage are tracked and reported alongside value realization metrics.
- The Change Management Agent should:
  - Allow configuration of value sharing terms (percentage, duration, start/end date)
  - Calculate Agent9's share based on realized value within the defined window
  - Report both gross value realized and Agent9's share in outputs and dashboards

### Value Realization Dashboard (Wireframe Vision)

**Purpose:**
- Provide a single view for leadership and stakeholders to track value delivered by changes, with context and explanations.
- Transparently display Agent9 value sharing terms, period, and amounts (if applicable).

**Dashboard Top Section:**
- **Total Value Realized Since Implementation:**
  - Displays the cumulative value delivered from the start of the value sharing period to present.
- **Agent9 Value Sharing (Monthly):**
  - Shows the current month's value realized, Agent9's 10% share, and the running total for the value sharing period (default: 12 months).
- **Estimated Change Not Realized (Value Left on the Table):**
  - Calculates the difference between target value and actual value realized.
  - For each KPI, tracks and displays the estimated value not realized.
  - Provides explanations for the gap (e.g., lack of adoption, unforeseen market conditions, other reasons).
  - Explanations are sourced from Situation Awareness and Stakeholder Analysis agents and are shared in the dashboard/report.
  - **Explanation Structure:**
    - `reason_category`: Enum (e.g., `lack_of_adoption`, `market_conditions`, `technical_issue`, `scope_change`, `other`)
    - `summary`: Short summary (1-2 sentences)
    - `details`: Longer explanation with supporting evidence
    - `source_agent`: Which agent provided the explanation
    - `confidence_score`: (Optional) 0.0–1.0
    - `recommendations`: (Optional) List of actionable steps
  - **Reporting Cadence:**
    - Default: Monthly (aligned with value sharing and KPI reporting)
    - Options: Quarterly and on-demand for leadership reviews
    - Audit trail: All explanations and updates are timestamped for traceability

**Core Features:**
- **KPI Comparison Table/Chart:**
  - Actual vs. Baseline
  - Actual vs. Projection (No Change)
  - Actual vs. Target
- **Trend Visualization:**
  - Line/bar charts showing all four series over time
- **Context Panel:**
  - Key events, risks, or opportunities surfaced by Situation Awareness Agent
  - Stakeholder sentiment/adoption (from Stakeholder Analysis Agent)
- **Recommendations/Alerts:**
  - Actionable recommendations if value is not realized
  - Critical alerts for at-risk KPIs
- **Audit Trail:**
  - Log of key value realization events and explanations

**Data Sources:**
- Change Management Agent (core logic, aggregation)
- Implementation Tracker Agent (completion triggers, process metrics)
- Situation Awareness Agent (context, risk, anomaly explanation)
- Stakeholder Analysis Agent (adoption, sentiment)

**Wireframe (Textual):**

| KPI         | Baseline | Projection | Target | Actual | Value Realized | Value Left | Status   | Explanation (Summary)           | Source Agent         | Recommendations         |
|-------------|----------|------------|--------|--------|----------------|------------|----------|-------------------------------|----------------------|------------------------|
| Revenue     | $10M     | $9.5M      | $11M   | $10.3M | +$0.3M         | $0.7M      | ⚠️      | User adoption lagged in Q2      | Stakeholder Analysis | Increase training, ... |
| Cycle Time  | 30 days  | 32 days    | 20 days| 23 days| -7 days        | 3 days     | ⚠️      | Market disruption in Q2         | Situation Awareness  | Monitor supply chain   |

- [Trend Chart Here]
- [Context Panel: "Market disruption in Q2", "Stakeholder resistance flagged"]
- [Recommendations/Alerts: "Increase training for adoption", "Monitor competitor activity"]
- [Audit Trail: "Value realization review completed 2025-04-30"]

**On click/expand:**
- Show `details`, `confidence_score`, and full recommendation list for each explanation.


### Output Specifications
- Change plans, impact/risk reports, and progress dashboards
- Audit trails and completion events for downstream agents
- **Prioritized, actionable recommendations:** All outputs must include recommendations with explicit priority (e.g., high, medium, critical) and clear, human-readable, demo-friendly language for change owners and leadership.
- **Critical alerts:** Outputs must flag high-risk changes, blockers, or adoption issues, surfacing actionable alerts for immediate mitigation and follow-up.
- **Demo/UI readiness:** All outputs must be structured for easy display in dashboards and reports, with priorities and critical alerts visually surfaced for users and demos.
- **Structured output models:** All outputs are provided as structured Pydantic models:
  - `ChangePlan`: Contains description, scope, timeline, and resources
  - `ImplementationStrategy`: Contains phases, key milestones, and basic requirements
  - `StakeholderPlan`: Contains key stakeholders, communication plan, and engagement strategy
  - `RiskAssessment`: Contains key risks, mitigation strategies, and risk metrics
  - `ChangeManagementOutput`: Comprehensive output model containing all analysis components
- **Defensive data handling:** All outputs include defensive type checking and fallback mechanisms for handling incomplete or malformed input data

### Implementation Details

#### Helper Methods
The Change Management Agent implements several helper methods to support its core functionality:

- `_get_current_timestamp()`: Standardizes timestamp generation in ISO format
- `_determine_scope()`: Analyzes requirements to determine change scope
- `_create_basic_timeline()`: Generates a standardized timeline structure
- `_estimate_basic_resources()`: Calculates resource requirements based on stakeholders and requirements
- `_identify_basic_milestones()`: Creates milestone list for implementation planning
- `_create_basic_communication_plan()`: Generates communication strategy
- `_determine_basic_engagement_strategy()`: Establishes stakeholder engagement approach
- `_identify_basic_risks()`: Identifies potential risks based on input data
- `_create_basic_mitigation_strategies()`: Generates risk mitigation approaches
- `_generate_basic_risk_metrics()`: Calculates risk metrics with confidence scores
- `_get_future_date()`: Calculates future dates in ISO format for timeline planning

#### Defensive Data Handling
The agent implements robust defensive data handling:
- Type checking for all input fields
- Fallback mechanisms for missing or malformed data
- Structured error handling with specialized error classes
- Standardized timestamp handling

### Error Classes
```python
class ChangeManagementError(Exception):
    """Base error for change management failures"""
    pass

class ImpactAnalysisError(ChangeManagementError):
    """Raised when impact analysis fails"""
    pass

class CommunicationError(ChangeManagementError):
    """Raised when communication fails"""
    pass

class ExecutionError(ChangeManagementError):
    """Raised when execution fails"""
    pass
```

### Logging
- Uses structured logging
- Logs change management process
- Tracks error conditions
- Logs impact assessments
- Logs communication activities
- Logs execution progress

## Testing
- All integration and scenario tests for A9_Change_Management_Agent are orchestrator-driven and use real agent inputs/outputs, never mocks or stubs.
- After agent migration is complete, all integration tests will be refactored to:
  - Use the orchestrator as the only entrypoint
  - Validate end-to-end workflows for C-level personas and business scenarios
  - Assert on real outputs, with no direct agent calls in test bodies
- This ensures tests reflect production-like behavior and validate the full agent ecosystem.

## Success Metrics
- Change adoption rate
- Stakeholder satisfaction
- Timeliness and effectiveness of change implementation
- Reduction of resistance/issues
- Alignment with organizational goals
- **Actionable, prioritized outputs:** Change management outputs are actionable, prioritized, and clearly flag high-risk changes or blockers for demo and production use.
- **Demo/investor feedback:** Demo and investor feedback confirms recommendations and alerts are understandable, actionable, and visually surfaced in the UI.

## Notes
- Focus on change management only
- Clear error handling
- Simple implementation
- Standard logging
- Comprehensive change components
- Stakeholder-centric approach
- Risk-aware planning
- Monitoring integration

<!-- End docs\prd\agents\a9_change_management_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_data_governance_agent_prd.md -->

# A9_Data_Governance_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Overview
**Purpose:** Ensure data quality, security, and compliance through comprehensive data governance, policy enforcement, and continuous monitoring. Leverage the Unified Registry Access Layer to manage business glossary terms, data asset locations, and governance policies.

**YAML Contract Context:**
The agent must read and respond to `yaml_contract_text` provided in the context by the orchestrator, supporting protocol-compliant workflows that leverage YAML-driven data product contracts for schema, mapping, dependencies, and governance enforcement.
**Agent Type:** Core Agent
**Version:** 1.0
**Template:** Follows A9_Agent_Template patterns

## Functional Requirements

- When present, the agent must use `yaml_contract_text` from the context to inform schema, mapping, dependency resolution, and policy enforcement.

### Calculated KPI Support
- The Data Governance Agent SHALL support calculated KPIs, where the value is computed from a formula defined in the KPI Provider from the Unified Registry Access Layer (e.g., Gross Margin = Gross Revenue - Cost of Goods Sold).
- The agent SHALL resolve dependencies between KPIs, allowing formulas to reference other KPIs recursively.
- All formulas and dependencies SHALL be documented, versioned, and auditable in the KPI Provider registry.
- The agent SHALL log and escalate (via HITL) any missing data or unmapped KPI references encountered during calculation.
- KPI definitions and relationships SHALL be managed through the Registry Factory and Unified Registry Access Layer.

### DuckDB View Strategy for KPI Calculation
- The primary mechanism for implementing calculated KPIs SHALL be via DuckDB views, where business logic and formulas are defined in the database schema.
- Agents SHALL map business terms to DuckDB view names and columns, leveraging the business glossary/registry for governance and traceability.
- Routine KPI calculations and multi-step formulas SHOULD be implemented in DuckDB SQL/views whenever feasible, using CTEs, CASE WHEN, and window functions as needed.
- The agent layer SHALL only escalate to HITL or perform Python-side calculation if a KPI cannot be computed via DuckDB view logic (e.g., missing data, ambiguous mapping, or business rule not expressible in SQL).
- View definitions SHOULD be version-controlled and referenced in the business glossary/registry for auditability.


### Business Glossary and Synonym Matching
- The Data Governance Agent SHALL use the Business Glossary Provider from the Unified Registry Access Layer as the authoritative source for business terms, synonyms, aliases, definitions, and technical attribute mappings.
- The glossary SHALL support dynamic synonym and alias matching for business term and filter translation, enabling robust NLP and business query support.
- The glossary SHALL be versioned, auditable, and updatable by data stewards/governance teams without code changes.
- The Data Governance Agent SHALL load and use the registry for all mapping operations, and log all unmapped or ambiguous terms for HITL escalation and audit.
- The Business Glossary Provider SHALL be initialized through the Registry Factory to ensure proper integration with the Unified Registry Access Layer.



### HITL Escalation for Unmapped Terms
If, after business glossary resolution, any business terms remain unmapped to canonical data product columns or KPIs, the agent must:
- Set `human_action_required: True` in the protocol output.
- Set `human_action_type: "clarification"` and provide a `human_action_context` that lists the unmapped terms and a message for the user.
- Downstream agents or the UI must prompt the principal for clarification or mapping of these terms before proceeding to query execution.

Example protocol output:
```json
{
  "resolved_terms": ["REVENUE_GROWTH"],
  "unmapped_terms": ["sales velocity"],
  "human_action_required": true,
  "human_action_type": "clarification",
  "human_action_context": {
    "unmapped_terms": ["sales velocity"],
    "message": "Please clarify or map these terms before proceeding."
  }
}
```

### Core Capabilities

- **YAML Contract Context Support:** Reads and applies YAML-driven data product contract metadata from the workflow context for schema, mapping, and policy enforcement.
1. Business-to-Technical Translation (NEW, MVP Alignment)
   - Translate business terms (responsibilities, filters) into technical attribute names and code values
   - Resolve ambiguities and map business language to data product metadata
   - Provide code value normalization for filter values
   - Implement Pydantic model-based input/output interfaces:
     - `async def translate_terms(input_model: A9_Data_Governance_TermTranslationInput) -> A9_Data_Governance_TermTranslationOutput`
     - `async def translate_filters(input_model: A9_Data_Governance_FilterTranslationInput) -> A9_Data_Governance_FilterTranslationOutput`
   - Return technical names and code values matching Data Product Agent expectations
   - Handle unmapped or ambiguous terms with robust error handling and logging
   - Provide usage statistics and metadata for translated terms
   - Calculate quality scores for term translations
   - Support key-value mapping for business-to-technical term translation
   - Implement configurable mapping dictionaries for terms and values
   - Log all translation activities with timestamps for audit purposes
   - Support protocol-compliant event logging for all translation operations

2. Dimensional Filter Translation and Forwarding
   - The agent must accept business filters (e.g., {"region": "North America"}) and translate both the filter keys (dimensions) and values to technical column names and code values as required by the Data Product/Data Source Agent.
   - After translation, all technical filters must be included in the agent protocol output and forwarded to the Data Source Agent for query processing.
   - If any filter key or value cannot be mapped, escalate to HITL for clarification, listing unmapped filters in `human_action_context`.

   Example protocol output:
   ```json
   {
     "resolved_terms": ["REVENUE_GROWTH"],
     "resolved_filters": {"REGION_CODE": "NA", "PROD_CODE": "CE"},
     "unmapped_terms": [],
     "unmapped_filters": {"region": "Europe"},
     "human_action_required": true,
     "human_action_type": "clarification",
     "human_action_context": {
       "unmapped_filters": {"region": "Europe"},
       "message": "Please clarify or map these filters before proceeding."
     }
   }
   ```

2. Data Quality Management
   - Monitor data quality through comprehensive metrics
   - Implement quality rules with configurable thresholds
   - Track quality metrics including completeness, accuracy, consistency, and timeliness
   - Generate detailed quality reports with scores and recommendations
   - Create data-driven improvement plans based on quality analysis
   - Support Pydantic model-based quality assessment
   - Provide structured input/output models for quality analysis
   - Implement threshold-based quality alerting
   - Generate actionable recommendations based on quality scores

3. Data Lineage Tracking
   - Track data sources
   - Resolve data asset paths via orchestrator-mediated workflows
   - Support role-based access control for data assets
   - Provide audit logs for data access and lineage

4. Protocol-Compliant Event Logging
   - Implement registry-based event logging for all agent operations
   - Log agent registration events with timestamps
   - Track all term and filter translation activities
   - Provide detailed debug logging for troubleshooting

5. Configuration Management
   - Support dynamic configuration updates with validation
   - Implement Pydantic v2 model validation for all configurations
   - Provide structured error handling for configuration issues
   - Support orchestrator-controlled registration compliance

6. KPI Registry Integration
   - Query KPIs by business process
   - Support business-to-technical term mapping
   - Implement KPI dependency resolution
   - Provide structured output models for KPI queries
   - Monitor data flow
   - Maintain data history
   - Generate lineage reports
   - Create lineage documentation

4. Data Access Control
   - Implement access policies
   - Manage user permissions
   - Track access logs
   - Generate access reports
   - Create audit trails

5. Policy Enforcement
   - Implement data policies
   - Monitor policy compliance
   - Generate policy reports
   - Create policy documentation
   - Update policy guidelines

6. Compliance Monitoring
   - Monitor regulatory compliance
   - Track compliance metrics
   - Generate compliance reports
   - Create compliance documentation
   - Update compliance status




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Data_Governance_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_data_governance_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Integrates with the Unified Registry Access Layer for business terms, technical terms, and data product contracts
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for business terms, technical terms, and translation mappings
- Must use registry data for context-aware business-to-technical term translation and data governance decisions
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation
- Direct enum usage (use registry providers instead)
- Hardcoded business glossary terms or data asset paths (use registry data)
- Initializing registry providers directly (use Registry Factory)

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

## Compliance: YAML Contract Context
- The Data Governance Agent must always check for and apply `yaml_contract_text` from the context if present, for all agent entrypoints. This is required for A2A protocol compliance and is enforced by orchestrator-driven workflow execution and tests.

### Example: Accessing YAML Contract Context in a Protocol-Compliant Method
```python
# In any protocol-compliant agent method:
def some_method(self, input_model, context=None):
    yaml_contract = context.get('yaml_contract_text') if context else None
    # Use yaml_contract for schema, mapping, governance enforcement, etc.
```

## New Core Capability: Semantic Query Validation & Enforcement

### Purpose
Ensure that all semantic queries generated for analytic requests are compliant with data governance, access control, and KPI definition policies.

### Function to Add
- `validate_semantic_query(query: str, user_context: dict) -> dict`:
  Validates the generated query for compliance, access, and policy. Returns approval, redactions, or warnings.

### Usage Flow
1. Orchestrator receives a semantic query from the Data Product Agent.
2. Orchestrator calls Data Governance Agent to validate the query before execution.
3. Data Governance Agent returns approval, warnings, or required changes.

### Test Cases
- Given a query and user context, the agent approves or blocks access based on policy.
- Redacts sensitive fields for unauthorized users.

### Modification History
- 2025-05-24: Added semantic query validation and enforcement function for NLP-driven analytics.

### Integration Requirements (NEW, MVP Alignment)
- Support registry-based integration (A9_Agent_Template pattern)
- All translation and governance functions must be async
- Use absolute imports
- Standardize error handling (ConfigurationError, ProcessingError, ValidationError)
- Log translation attempts, successes, and failures

### Test Requirements (NEW, MVP Alignment)
- Test translation of business terms to technical names (including edge cases and unmapped terms)
- Test filter value normalization (English → code values)
- Test error handling for ambiguous/unmapped terms
- Test async operation and registry integration
- Document test coverage and assumptions

### Example Request/Response
```python
# Example: Translate business responsibilities and filters
governance_agent = ... # Registry lookup
tech_terms = await governance_agent.translate_terms(["Revenue by Business Unit"])
tech_filters = await governance_agent.translate_filters({"Region": "North America"})
# tech_terms -> ["REV_BY_BU"]
# tech_filters -> {"REGION_CODE": "NA"}
```

### Input Requirements
1. Data Governance Data
   - Data quality metrics
   - Access logs
   - Policy documents
   - Compliance requirements
   - Audit records

2. Analysis Parameters
   - Quality thresholds
   - Compliance requirements
   - Access controls
   - Policy guidelines
   - Monitoring schedules

### Output Specifications
1. Governance Artifacts
   - Quality reports
   - Lineage documentation
   - Access logs
   - Policy documentation
   - Compliance reports

2. Analytics
   - Quality dashboards
   - Lineage metrics
   - Access metrics
   - Policy compliance
   - Compliance status

3. Reports
   - Quality analysis
   - Lineage reports
   - Access reports
   - Policy status
   - Compliance metrics

## Technical Requirements

### Agent Implementation
- Follow A9_Agent_Template patterns
- All agent instantiation must use the async `create_from_registry` factory method, with no direct construction or manual registry instantiation.
- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning **Pydantic model instances** for all inputs and outputs. Raw dicts/lists are not permitted.
- All registry lookups, agent creation, and agent-to-agent calls must be async and registry-compliant.
- Use standard error handling patterns from the template, including ConfigurationError, ProcessingError, ValidationError, and RegistrationError.
- Use absolute imports for all dependencies.

---

## Dynamic Data Asset Discovery & Path Resolution (2025-05-14)

### Overview
- Data Governance Agent is the authoritative source for all data asset locations in Agent9, using the Data Asset Provider from the Unified Registry Access Layer.
- All requests for data asset paths (e.g., KPI CSVs) must be resolved dynamically through the registry, not hardcoded.
- Asset path resolution is performed via async, registry-compliant APIs, delegating to the Data Product Agent for catalog lookups as needed.

### Responsibilities
- Maintain and validate the authoritative data sources registry (e.g., `data_sources.csv`).
- Implement and document `async get_data_asset_path(asset_name: str) -> DataAssetPathResponse`.
- Enforce role-based access control and compliance for asset path requests.
- Integrate with Data Product Agent for physical path resolution.
- Ensure all integration tests and orchestrator flows use dynamic path lookup.

### Integration Flow
1. Orchestrator or agent requests asset path from Data Governance Agent.
2. Data Governance Agent validates request, checks registry, and delegates to Data Product Agent as needed.
3. Data Product Agent returns the physical path, which is validated and returned to the caller.
4. All operations are async and fully registry-compliant.

### Checklist (see BACKLOG_REFOCUS.md for full tracking)
- [ ] Define Pydantic models for asset path requests/responses
- [ ] Implement `get_data_asset_path` in Data Governance Agent
- [ ] Implement `get_data_source_path` in Data Product Agent
- [ ] Refactor tests and orchestrator to use dynamic lookup
- [ ] Document and standardize the data sources registry
- [ ] Add/expand integration tests
- [ ] DRY up code and maintain organization

**See BACKLOG_REFOCUS.md for status and detailed tracking.**

## Protocol Compliance
- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning **Pydantic models** for type safety, validation, and interoperability.
- The agent must implement all MCP (Minimum Compliance Protocol) requirements, including compliance checks, reporting, and error handling.
- Protocol compliance is mandatory for registry integration and agent orchestration.
- All agent-to-agent communication and registry interactions must use the async `create_from_registry` pattern for instantiation and dependency management.
- All test scripts and integration scenarios must use Pydantic models for all agent calls and assertions.

---
- Implement `create_from_registry` method for all agent instantiation
- Use standard error handling patterns
- Support async operations

### Integration Points
1. Data Systems
   - Connect to data sources
   - Interface with data warehouses
   - Integrate with data lakes
   - Connect to analytics systems
   - Interface with security systems

2. Output Systems
   - Generate reports
   - Create dashboards
   - Export metrics
   - Generate documentation

### Performance Requirements
1. Analysis Time
   - Basic analysis: < 1 hour
   - Comprehensive analysis: < 4 hours
   - Real-time monitoring: < 15 minutes

2. Processing
   - Handle large data volumes
   - Process complex data relationships
   - Maintain data accuracy
   - Support concurrent operations

### Scalability
1. Support for multiple data sources
2. Handle large data volumes
3. Scale with increasing complexity
4. Support cross-system analysis

## Error Handling
- Use standard error classes from A9_Agent_Template
- Error types:
  - ConfigurationError: Invalid configuration
  - RegistrationError: Failed to register with registry
  - ProcessingError: Failed to process data
  - ValidationError: Invalid input data
  - ConnectionError: Connection failures

## Security Requirements
1. Data Security
   - Secure data access
   - Protect sensitive data
   - Secure audit trails
   - Secure documentation

2. Access Control
   - Role-based access
   - Secure data sharing
   - Audit trail for changes
   - Policy approval workflows

## Monitoring and Maintenance
1. Regular data quality checks
2. Continuous policy monitoring
3. Periodic compliance reviews
4. Regular access audits

## Success Metrics
1. Data quality improvement
2. Policy compliance rate
3. Access control effectiveness
4. Compliance status
5. Governance efficiency

## Usage Flow
```
graph TD
    subgraph "Data Quality Management"
        DQM[Monitor Quality] -->|Implement Rules| DQM2[Track Metrics]
    end

    subgraph "Data Lineage Tracking"
        DLT[Track Sources] -->|Monitor Flow| DLT2[Generate Reports]
    end

    subgraph "Access Control"
        AC[Implement Policies] -->|Manage Permissions| AC2[Track Access]
    end

    subgraph "Policy Enforcement"
        PE[Implement Policies] -->|Monitor Compliance| PE2[Generate Reports]
    end

    subgraph "Compliance Monitoring"
        CM[Monitor Compliance] -->|Track Metrics| CM2[Generate Reports]
    end

    DQM2 --> DLT
    DLT2 --> AC
    AC2 --> PE
    PE2 --> CM
    CM2 --> DQM
```

## Notes
- Focuses on comprehensive data governance
- Works with data systems for governance
- Generates strategic governance recommendations
- Maintains governance baselines
- Supports continuous governance improvement

---

## Future Architecture & Roadmap



### Vision
The long-term architecture for business term conversion and NLP/data governance will combine the power of LLMs with a governed, dynamic, and auditable business glossary registry. This hybrid approach will enable:
- **LLM-accelerated planning and analysis**: LLMs will parse, strategize, and adapt analysis plans, but all business term/attribute mapping decisions will be validated against the business glossary registry.
- **Retrieval-Augmented Generation (RAG)**: LLMs will retrieve the latest glossary mappings and definitions at inference time, ensuring up-to-date, compliant mapping.
- **Agentic Deep Analysis**: Autonomous agents will decompose high-level goals into multi-step analysis plans, orchestrate sub-queries, and synthesize results, collaborating with Data Governance, Context, Data Product, and HITL agents.
- **Situation and Context Awareness**: Agents will maintain and update persistent situation models, supporting multi-turn, context-rich workflows.
- **Feedback & Learning Loops**: All mapping requests, errors, and human escalations will be logged, reviewed, and used to continuously improve glossary quality, LLM prompts, and analysis strategies.
- **Marketplace & Extensibility**: The architecture will support agent discoverability, reuse, and compliance for internal and external marketplace scenarios.

### Roadmap Highlights
- Integrate LLM-driven planning and adaptive querying in Deep Analysis Agent
- Implement RAG pattern for glossary lookups at inference time
- Enable multi-agent orchestration and persistent situation models
- Establish feedback and learning loops for continuous improvement
- Ensure all components are compliant, auditable, and marketplace-ready

---

## Post-MVP Enhancement: Business Glossary & Synonym Management

**Requirement:**
After MVP, the Data Governance Agent shall maintain and expose a centralized business glossary, including:
- Canonical business terms (e.g., “revenue”, “gross margin”, “customer”)
- Approved synonyms and alternate terms
- Mappings from business terms to technical fields/metrics/entities in SAP and non-SAP systems
- APIs/message types for term resolution, synonym lookup, and validation
- Governance workflows for adding, updating, and deprecating terms (with audit trails)

**Rationale:**
This enables consistent, governed, and explainable business semantics across all Agent9 agents and workflows. It supports NLP/LLM integration, cross-agent consistency, and semantic compliance.

**Integration Points:**
- NLP Integration Agent
- Data Product Agent
- Deep Analysis Agent
- Principal Context Agent

**Backlog Reference:**
Add to Agent9 backlog as “Data Governance Agent: Business Glossary & Synonym Management (Post-MVP Upgrade)”.

<!-- End docs\prd\agents\a9_data_governance_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_data_product_agent_prd.md -->

# A9_Data_Product_Agent Product Requirements Document

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## 1. Overview

### 1.1 Purpose
The A9_Data_Product_Agent manages the lifecycle of data products, including creation, version management, deployment tracking, and usage monitoring. It leverages the Unified Registry Access Layer to discover, register, and provide access to data products. It follows Agent9's architectural principles of simplicity, independence, and reliability.

### 1.2 Scope
This document outlines the requirements for version 1.0 of the A9_Data_Product_Agent, focusing on core data product management capabilities.

### 1.3 Target Audience
- System Administrators
- Data Engineers
- Business Analysts
- Data Scientists

## 2. Business Requirements

### 2.1 Business Objectives
1. Provide a standardized interface for data product management
2. Enable efficient data product lifecycle management
3. Support data quality and performance monitoring
4. Facilitate integration with data systems

### 2.2 Key Metrics
- Data processing latency: < 1 second
- Data quality metrics accuracy: > 95%
- System availability: 99.9% uptime
- Error rate: < 1%

## 3. Functional Requirements

### 3.1 Core Data Product Management

#### 3.1.1 Data Product Creation and Registry Integration
- Create new data products with unique IDs
- Configure product properties including version and capabilities
- Set up versioning and status tracking
- Define data sources and transformations
- Register data products with the Unified Registry Access Layer
- Support loading data products from various sources (YAML, JSON, Python modules)
- Provide registry access to data product definitions for other agents

#### 3.1.2 Data Processing
- All data access, joining, filtering, and aggregation must be performed exclusively by the MCP (DuckDB backend) service.
- The agent must never use direct pandas, file I/O, or agent-side data transformation in production.
- The agent only requests and processes business-ready, pre-joined, and pre-aggregated data from MCP endpoints.
- Handle data quality validation and monitor processing performance based on MCP responses only.

#### 3.1.3 Analysis
- Calculate quality metrics (completeness, consistency, accuracy, timeliness)
- Track performance metrics (processing time, resource usage, throughput)
- Monitor usage metrics (requests, errors, success rate)
- Generate comprehensive analysis reports

#### 3.1.4 Data Product Discovery & Matching with Registry Integration (NEW, MVP Alignment)
- Leverage the Unified Registry Access Layer to discover data products
- Map business processes and KPIs to relevant data products
- Use the Data Product Provider to find data products by attribute, domain, or business process
- Support both legacy enum-based discovery and new registry-based discovery

### 3.1.5 LLM Explainability Compliance (2025-06-24)
- All summary and recommendation text fields are routed through the A9_LLM_Service_Agent for explainability and business-user-friendly output.
- LLM calls are protocol-compliant, orchestrator-driven, and fully async with structured event logging and error handling.
- See agent card for implementation and compliance details.
- Accept technical process names and code values as input (not business English)
- Example interface:
  - `async def find_products_for_processes(processes: List[str]) -> List[Dict]`
- Input must be output from Data Governance Agent (already translated)
- Output: List of data products/KPIs, each with technical metadata (e.g., name, description, attributes)
- Example:
```python
# Input (from governance agent)
tech_processes = ["REV_BY_BU", "COST_BY_PROD"]
kpis = await data_product_agent.find_products_for_processes(tech_processes)
# Output
[
  {"name": "REV_BY_BU", "description": "Revenue by Business Unit", "attributes": ["BU_CODE", "REV_AMT"]},
  {"name": "COST_BY_PROD", "description": "Cost by Product Line", "attributes": ["PROD_CODE", "COST_AMT"]}
]
```
- Must validate all input terms and handle unmapped processes with robust error handling
- Log all matching attempts, successes, and failures

### 3.2 Error Handling

#### 3.2.1 Error Response Format
All error responses must be in the following format:
```python
{
    'status': str,  # Must be one of: 'success', 'partial_error', 'error'
    'error': str,   # Detailed error message
    'timestamp': str,  # ISO format timestamp
    'method': str   # Calling method name
}
```

#### 3.2.2 Status Codes
- 'success': Operation completed successfully
- 'partial_error': Validation error occurred, some data may be processed
- 'error': Processing or connection error occurred, operation failed

#### 3.2.3 Error Types and Messages
- ConfigurationError: "Invalid configuration: {specific_error}"
- RegistrationError: "Failed to register with registry: {specific_error}"
- ProcessingError: "Processing error: {specific_error}"
- ValidationError: "Validation error: {specific_error}"
- TypeError: "Invalid type: {expected_type}. Received: {received_type}"
- FormatError: "Invalid format: {expected_format}. Received: {received_format}"
- MissingFieldError: "Missing required field: {field_name}"
- EmptyValueError: "Empty value for required field: {field_name}"
- ConnectionError: "Connection error: {specific_error}"

#### 3.2.4 Error Handling Requirements
- All methods must validate input parameters before processing
- Must use template's error handling patterns
- Must return error responses instead of raising exceptions
- Error logging must include:
  - Method name
  - Error type
  - Detailed error message
  - Timestamp
  - Stack trace (for processing errors)
- Error responses must be consistent across all methods
- Must handle all error types with appropriate status codes
- Must validate configuration on initialization
- Must validate data format before processing

#### 3.2.5 Registry & Integration Requirements (NEW, MVP Alignment)
- Must use template's register_with_registry method
- Must properly handle registration errors
- Must return proper error responses on failure
- Must properly set up registry reference
- Must validate agent ID format before registration
- Must handle concurrent registration attempts
- All discovery/matching functions must be async
- Input must be compatible with Data Governance Agent output
- Registry integration must follow A9_Agent_Template
- Log all integration attempts and failures

#### 3.2.6 Test Requirements (NEW, MVP Alignment)
- All methods must have test cases for:
  - Success scenarios
  - Validation error scenarios
  - Processing error scenarios
  - Connection error scenarios (where applicable)
  - Integration with Data Governance Agent (input translation)
  - Integration with Principal Context Agent (end-to-end flow)
- Test cases must verify:
  - Return type (Dict[str, Any]) or List[Dict]
  - Status code
  - Error message (if applicable)
  - Timestamp presence
  - Method name in response
  - Input validation for technical terms/code values
- Error messages must match exactly what's expected
- Status codes must be properly checked
- Timestamps must be present in responses
- Method names must be correct in responses

### 3.3 Configuration Management
- Support default configuration values
- Validate configuration on initialization
- Maintain configuration state
- Handle agent-specific settings
- Configuration fields:
  - name
  - version
  - capabilities
  - error_handling

### 3.4 Logging
- Initialize agent-specific logger
- Log initialization and major operations
- Support different log levels (INFO, ERROR)
- Include timestamps in logs
- Log error details with stack traces

## 4. Non-Functional Requirements

### 4.1 Performance
- Process data in real-time
- Handle large data volumes
- Maintain consistent performance
- Support concurrent operations

### 4.2 Reliability
- High availability architecture
- Automatic failover
- Data backup and recovery
- Error handling and recovery

## 5. Technical Requirements

### 5.0 Registry Architecture Integration
- Must integrate with the Unified Registry Access Layer
- Must use the Registry Factory to access providers
- Must use the Data Product Provider for all data product operations
- Must support loading data products from YAML contracts
- Must support data product registration from multiple sources
- Must provide backward compatibility for legacy code using enum values

### 5.1 Protocol Compliance Update (2025-06-24)
- All entrypoints accept only protocol-compliant models: `DataProductNLQSearchInput` and `DataAssetPathRequest` as input; `DataProductNLQSearchOutput` and `DataAssetPathResponse` as output.
- No legacy, deprecated, or stub models are permitted.
- Agent must be instantiated and registered only by the orchestrator; constructor requires a registry reference.
- `register_with_registry` only stores the registry reference, never performs registration.
- All event logging is async/awaited and uses `A9_SharedLogger`.
- All protocol-compliant workflows must propagate and utilize `yaml_contract_text` from the context kwarg.
- All usage and test examples must reflect orchestrator-driven patterns.

### 5.1 System Architecture
- Microservices-based architecture
- RESTful API interface
- Containerized deployment

### 5.2 Performance Optimization
- Implements YAML contract caching mechanism (`_yaml_contract_cache` and `_get_yaml_contract` method)
- Optimizes repeated contract access during agent operation
- Reduces redundant parsing and processing of YAML contracts

### 5.3 Testing and Integration Support
- Provides `load_synthetic_data` method for testing and integration scenarios
- Implements `get_kpi_values` method for querying synthetic KPI data
- Supports development and testing environments with synthetic data generation

### 5.4 User Preference Management
- Includes `get_user_variance_threshold` method for retrieving user-specific preferences
- Supports personalized data presentation based on user settings
- Enables customized threshold configuration for variance reporting

### 5.5 Agent-to-Agent Communication
- Implements `context_handoff_a2a` method for protocol-compliant context handoff
- Provides `product_documentation_qna_a2a` method for documentation Q&A functionality
- Supports standardized inter-agent communication patterns
- Cloud-native design

### 5.2 Technology Stack
- Python 3.10+
- FastAPI for REST API
- SQLAlchemy for database
- Docker for containerization
- Kubernetes for orchestration

### 5.3 Integration Points
- Data systems
- Logging infrastructure
- Monitoring system
- Agent registry

### 5.4 Module Structure and Imports
- Error handling module structure:
  - All error classes should be exported through the package's __init__.py
  - Error classes should be organized in a dedicated sub-module
  - Common error types should be accessible via the package root
- Import patterns:
  - Use relative imports within agent packages
  - Use explicit import paths for external dependencies
  - Follow PEP 8 import ordering:
    1. Standard library imports
    2. Third-party imports
    3. Local application imports
  - Example import structure:
    ```python
    from typing import Dict, Any  # Standard library
    import pandas as pd            # Third-party
    from ..agent_registry import AgentRegistry  # Local relative import
    from ..errors import ConfigurationError  # Local relative import
    ```

## 6. Implementation Phases

### Phase 1: Core Implementation (1 week)
- Basic data product management
- Core data processing
- Basic error handling
- Initial logging setup

### Phase 2: Advanced Features (1 week)
- Advanced error handling
- Configuration management
- Enhanced logging
- Performance optimization

## 7. Dependencies

### 7.1 External Dependencies
- Data storage systems
- Logging services
- Monitoring services
- Configuration management

### 7.2 Internal Dependencies
- Unified Registry Access Layer
- Registry Factory and Providers
- Logging Infrastructure
- Monitoring System

## 7.5 Implementation Alignment (2025-04-17)

### Recent Updates (v1.1+)
- **MCP Service-Only Architecture:**
  - All data access, joining, filtering, and aggregation logic is now handled exclusively by the MCP (DuckDB backend) service.
  - The agent no longer uses pandas, direct file I/O, or agent-side data transformation in production.
  - All registry integration and data product onboarding must be performed via MCP endpoints.
- **Return Structure:**
  - `create_data_product` now returns `{'status': 'created', 'data': {...}}` on success, and a structured error dict on failure.
- **Error Handling:**
  - Product creation requires `name` and `description` (both non-empty).
  - Missing/empty fields yield `partial_error` status and a detailed error message.
- **Logging:**
  - All errors and validation failures are logged with method name, error type, and timestamp.
- **Imports:**
  - All imports are absolute (e.g., `from src.agents.new.A9_Data_Product_Agent import ...`).
- **Test Coverage:**
  - 100% coverage for core functionality, error handling, config validation, and async ops.
  - Tests assert on return structure, error handling, and logging patterns.
- **Registry Integration:**
  - Follows A9_Agent_Template registry and creation patterns.
- **Example Code:**
  ```python
  result = await agent.create_data_product({'name': 'foo', 'description': 'bar'})
  assert result['status'] == 'created'
  assert 'data' in result
  ```




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Data_Product_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_data_product_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting
- Integrates with the Unified Registry Access Layer for data products and query mappings

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for data products, contracts, and query templates
- Must use registry data for context-aware data product discovery and access
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## 8. Modification History

### 8.1 Version 1.0
- Date: 2025-04-14
- Changes:
  - Initial implementation of core functionality
  - Added structured error handling
  - Implemented configuration management
  - Added comprehensive logging
  - Added test coverage
- Affected Test Cases:
  - Registry Integration Tests
  - Error Handling Tests
  - Configuration Tests
  - Data Processing Tests

### 8.2 Planned Modifications

#### 8.2.1 Enhanced Error Handling
- Purpose: Improve error handling robustness
- Impact Analysis:
  - Input Changes: None
  - Output Changes: More detailed error responses
  - Data Flow Changes: None
- Test Impact:
  - Affected Test Cases: Error Handling Tests
  - New Test Cases Needed: None
  - Test Data Changes: None
- Implementation Plan:
  1. Add more specific error types
  2. Improve error recovery mechanisms
  3. Add retry logic
- Documentation Updates:
  - [ ] Update error handling documentation
  - [ ] Update error response structure
  - [ ] Update usage examples

## 9. Acceptance Criteria

### 9.0 Protocol Compliance (2025-06-24)
- All entrypoints use only protocol-compliant models as defined in code.
- Orchestrator-driven lifecycle is enforced; no agent-side registration or direct instantiation in documentation or code samples.
- All event logging is async/awaited and uses `A9_SharedLogger`.
- YAML contract context is always propagated and accessed via the context kwarg.
- All tests and usage examples are orchestrator-driven.

### 9.1 Functional
- Successful data product creation
- Accurate data processing
- Proper error handling
- Complete documentation
- **Compliance:** All data access, joining, filtering, and aggregation must be performed exclusively by the MCP (DuckDB backend) service. No agent-side pandas/file I/O is permitted in production.

### 9.2 Non-Functional
- Performance meets requirements
- Error handling implemented
- Documentation complete
- Testing coverage verified

## 10. Agent Requirements

### 10.1 Core Agent Interface
- Follow Agent9 agent registry interface requirements
- Implement required Unified Registry integration methods
- Use the Data Product Provider for registry operations
- Use standard error handling patterns
- Maintain consistent logging

### 10.2 Configuration Management
- Support default configuration values
- Validate configuration on initialization
- Maintain configuration state
- Handle agent-specific configuration

### 10.3 Error Handling
- Implement structured error handling
- Log errors appropriately
- Return consistent error responses
- Handle common error types

### 10.4 Logging
- Initialize agent-specific logger
- Log initialization and major operations
- Support different log levels
- Include timestamps in logs

### 10.5 Core Methods
- Implement _initialize for setup
- Implement _setup_logging for logging
- Implement _setup_error_handling for error management
- Implement create_from_registry class method
- Implement _validate_input for input validation
- Implement _format_error for consistent error messages

### 10.6 Error Types
- ConfigurationError: Invalid configuration
- RegistrationError: Failed to register with registry
- ProcessingError: Failed to process data
- ValidationError: Invalid input data
- TypeError: Invalid data type
- FormatError: Invalid ID format
- MissingFieldError: Required field is missing
- EmptyValueError: Required field is empty
- ConnectionError: Connection failures

## 11. Testing Requirements

### 11.1 Test Coverage
- Core functionality: 100%
- Error handling: 100%
- Performance: 80%
- Integration: 80%

### 11.2 Test Scenarios
1. Data product creation
2. Data processing
3. Error handling
4. Configuration management
5. Integration with registry

## 12. Maintenance

### 12.1 Regular Updates
- Performance optimization
- Bug fixes
- Feature enhancements
- Documentation updates

### 12.2 Support
- User documentation
- API documentation
- Troubleshooting guide
- Support channels

<!-- End docs\prd\agents\a9_data_product_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_deep_analysis_agent_prd.md -->

# A9_Deep_Analysis_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Overview
**Purpose:** Deliver actionable, interpretable insights to support decision-making by providing transparent analysis, clear recommendations, and confidence scores. Emphasizes explainability, auditability, and compliance. Leverages the Unified Registry Access Layer for consistent business glossary, KPI, and data access.




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Deep_Analysis_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_deep_analysis_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Integrates with the Unified Registry Access Layer for KPIs, business processes, and data products
- Uses Registry Factory for provider initialization and configuration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation
- Direct enum usage (use registry providers instead)
- Hardcoded business logic or KPI definitions (use registry data)
- Initializing registry providers directly (use Registry Factory)

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## LLM Integration Prioritization (2025-05-07)

A9_Deep_Analysis_Agent is a high-priority candidate for LLM integration, benefiting from LLM-powered insight extraction, summarization, and recommendations. See the "Agents Prioritized for LLM Integration" table in BACKLOG_REFOCUS.md for a complete rationale and comparison across agents.

---
**Purpose:** Deliver actionable, interpretable insights to support decision-making by providing transparent analysis, clear recommendations, and confidence scores. Emphasizes explainability, auditability, and compliance. Utilizes the Unified Registry Access Layer to access KPIs, business processes, and data products.
**Agent Type:** Clarity Team
**Version:** 1.0

## Functional Requirements

### Core Capabilities (MVP)

**Structured Root Cause Analysis (Kepner-Tregoe “Is/Is Not”)**
- The agent MUST implement a structured root cause analysis process based on the Kepner-Tregoe (KT) “is/is not” methodology.
- This includes:
    - Programmatically generating an “is/is not” table for each flagged issue, specifying:
        - What, Where, When, and Extent: segments/times where the problem IS and IS NOT observed.
        - Change-point detection: When the problem started or deviated from baseline.
        - Cross-segment comparison: Has the problem happened elsewhere, or is it unique?
    - Using LLMs to:
        - Interpret and summarize the KT table.
        - Generate root cause hypotheses, distinguishing features, and targeted diagnostic questions.
        - Recommend next steps based on KT findings.
- The KT-driven analysis must be included in both programmatic and LLM-assisted outputs, and reflected in the agent’s output models and audit logs.

**HITL (Human-in-the-Loop) Enablement:**
- HITL is NOT required for this agent. The configuration model includes a `hitl_enabled` field for protocol consistency, but it should remain False in all environments (DEV/QA/Prod).
- Rationale: Deep Analysis Agent workflows are highly technical and automated, not suitable for human-in-the-loop intervention. All review and approval is automated and orchestrator-controlled. See agent card for further details.

**Registration & Orchestrator Compliance:**
- Registration is orchestrator-controlled. There is no self-registration or legacy registration logic in the agent; all registration is managed by the orchestrator and AgentRegistry for full protocol compliance. See BACKLOG_REFOCUS.md for traceability.

6. General-Purpose LLM for Problem Framing & Clarification
    - Integrate a general-purpose LLM via the centralized A9_LLM_Service, with all requests routed through the Orchestrator Agent for logging and compliance
    - All LLM-powered analysis, summarization, and hypothesis generation must use A9_LLM_Service (no direct LLM API calls)
    - All LLM requests and responses must be validated using strict Pydantic models
    - All LLM outputs must include source attribution, operation metadata, and confidence scores
    - Outputs must be ready for downstream agent and UI consumption
    - Capture and surface natural language (NLP) clarification questions to iteratively refine the analysis

7. Opt-In LLM Enhancement (Pilot)
    - Backend supports LLM-powered analysis as an optional feature, always using the centralized A9_LLM_Service and Orchestrator Agent
    - Direct LLM API calls are strictly forbidden; protocol compliance is mandatory
    - UI surfaces an "Enhance with LLM" button for users to invoke LLM features on demand
    - Implement risk evaluation and user feedback mechanisms to monitor LLM performance and safety

1. Actionable Insights
   - Deliver interpretable analysis results with clear explanations and recommendations
   - Provide confidence scores for all results
   - Output concise text summaries for decision support
   - Leverage registry data for contextual awareness and business relevance

2. Transparency & Lineage
   - Ensure transparency for input data, transformation steps, and analysis lineage
   - Document all analysis steps

3. Audit Logging
   - Log all analysis steps, input parameters, and results
   - Provide exportable audit trails for compliance and review

4. Compliance
   - Implement compliance checks for sensitive data handling
   - Support audit trails for all compliance-relevant actions

5. Metrics & Performance
   - Define and report metrics to track business impact and adoption
   - Monitor and document performance and scalability

---

### Iterative, Orchestrated, Principal-Centric Deep Analysis (Enhancement)

**8. Iterative Problem Investigation & Refinement**
- The Deep Analysis Agent must plan and execute a sequence of data queries and analyses to progressively narrow down the root cause(s) of complex problems.
- The agent adapts its investigation based on intermediate findings, drilling down by segment, time, or other relevant dimensions.
- The agent quantifies the scope and severity of the problem, or determines if no material problem exists, and documents the rationale for its conclusion.

**9. LLM-Driven Investigation Planning with Registry Integration**
- The agent uses an LLM not only for summarization, but also to propose investigation steps, next queries, and hypotheses based on evolving context.
- Investigation planning leverages registry data (KPIs, business processes, data products) for context-aware analysis.
- LLM prompts include relevant registry information to enhance investigation quality.
- The LLM is prompted to act as a business analyst working on behalf of the principal, ensuring all outputs are principal-centric and actionable.

**10. Multi-Agent Orchestration via Orchestration Agent**
- All requests for additional analysis, risk assessment, or agent collaboration must be routed through the Orchestration Agent.
- The Deep Analysis Agent communicates its needs (e.g., risk quantification, stakeholder mapping, data enrichment) to the Orchestration Agent, which delegates requests to appropriate specialized agents and returns results.
- This ensures centralized coordination, logging, protocol enforcement, and auditability.

**11. Quantification & Reporting**
- The agent provides quantitative measures of problem extent (e.g., % of affected revenue, number of impacted customers).
- If no material problem is found, the agent clearly states this and explains the basis for that conclusion.
- All investigation steps, queries, agent engagements, and rationale are logged and can be surfaced for review.

**12. Transparent Investigation Plan**
- The agent maintains a transparent log of its investigation plan, steps taken, queries run, agents engaged, and rationale for each step.
- This plan can be presented to users or auditors on request.

---

### Input Requirements (MVP)
- Pydantic input model with relevant fields for context and analysis parameters
- Field: `data_product_schema` (optional; used for escalation logic and protocol compliance)
- Minimal set of analysis parameters (e.g., thresholds, weights, scope)
- All input models must be strictly validated and protocol-compliant (Agent9 standards)
- See test suite for escalation scenarios using ambiguous `data_product_schema`.

### Output Specifications (MVP)
- Pydantic output model with:
  - Analysis results (interpretable, actionable)
  - Confidence scores
  - Status (success/error)
  - Text summary and recommendation
  - Timestamp
  - Audit log (full trace of analysis, escalation, and LLM involvement)
  - All escalation and LLM output merging logic must be protocol-compliant and auditable
  - Output models and merging logic tested for compliance as of 2025-06-09

## Technical Requirements
- Modular, maintainable architecture
- Registry integration and async operations
- Secure configuration and error handling
- Full compliance with A2A and MCP protocols (see Protocol Compliance)
   - Role-based access
   - Secure data sharing
   - Audit trail for changes
   - Analysis approval workflows

## Monitoring and Maintenance
1. Regular model updates
2. Continuous accuracy monitoring
3. Periodic validation
4. Regular optimization

## Success Metrics
1. Analysis accuracy
2. Prediction confidence
3. Pattern recognition
4. Decision support effectiveness
5. Analysis efficiency

## Input/Output

### Input
```python
{
    "data": List[Dict[str, Any]],  # Pre-fetched query results from A9_Database_Query_Agent
    "context": {
        "analysis_type": "type_of_analysis",
        "focus_areas": ["area1", "area2"],
        "time_window": {
            "start": "2025-04-12",
            "end": "2025-04-12"
        }
    }
}
```

### Output
```python
{
    "status": "success" | "error",
    "analysis": {
        "metrics": Dict[str, float],
        "trends": List[Dict],
        "patterns": List[str],
        "correlations": Dict[str, float],
        "anomalies": List[Dict],
        "data_quality": Dict,
        "business_context": Dict
    },
    "insights": List[Dict],  # With detailed action plans
    "recommendations": List[Dict],  # With implementation plans
    "metadata": {
        "analysis_time": "ISO timestamp",
        "confidence_scores": {
            "overall": float,
            "metrics": float,
            "trends": float
        }
    },
    "timestamp": "ISO timestamp"
}
```

## Modification History

### 2025-06-09
- Test suite fully updated and passing (escalation logic, LLM output merging, audit logging, protocol compliance)
- Input model refactored to use `data_product_schema` (removes legacy `schema`/`data` ambiguity)
- Output model and escalation logic updated for strict Agent9 protocol compliance and auditability
- RuntimeWarning on unawaited coroutine in test mocks mitigated (see test suite for details)
- Compliance with Agent9 Agent Design Standards and audit logging requirements confirmed
- All changes traceable to test cases and standards (see BACKLOG_REFOCUS.md for traceability)

### Current Version
- Version: 1.0

## 6. Implementation Details

### 6.1 Data Processing and Formatting
- Implements timestamp formatting utilities for consistent date/time representation
- Provides standardized DataFrame handling for data analysis operations
- Includes data quality assessment and validation mechanisms

### 6.2 Security and Compliance
- Implements sensitive data detection logic for compliance with data governance policies
- Provides compliance reporting structure for audit requirements
- Ensures proper data handling and masking for sensitive information

### 6.3 Analysis and Insights Generation
- Implements structured insights and recommendations generation approach
- Provides confidence scoring methodology for analysis results
- Includes data quality assessment metrics for reliability evaluation
- Supports anomaly detection and pattern recognition algorithms

### 6.4 Performance and Optimization
- Implements caching mechanisms for repeated analysis operations
- Provides optimized DataFrame operations for large dataset handling
- Includes performance monitoring and logging for analysis operations
- Date: [Release Date]
- Changes: Initial implementation
- Affected Test Cases: All

### 2025-05-12
- Documented HITL rationale: HITL is not required for Deep Analysis Agent; config model includes `hitl_enabled` for protocol consistency only. Card and PRD updated to reflect this compliance and rationale.


### Planned Modifications

#### [Modification Name]
- Purpose: [Brief description]
- Impact Analysis:
  - Input Changes: [List changes]
  - Output Changes: [List changes]
  - Data Flow Changes: [Description]
- Test Impact:
  - Affected Test Cases: [List]
  - New Test Cases Needed: [List]
  - Test Data Changes: [Description]
- Implementation Plan:
  1. [Task 1]
  2. [Task 2]
  3. [Task 3]
- Documentation Updates:
  - [ ] Update input parameters
  - [ ] Update output structure
  - [ ] Update usage examples
  - [ ] Update error handling

## Acceptance Criteria
1. The agent successfully conducts advanced analysis to uncover insights and support decision-making.
2. The agent provides accurate and reliable results.
3. The agent integrates with data systems and output systems as specified.
4. The agent meets performance requirements.
5. The agent is secure and follows access control requirements.
6. All input/output models, escalation logic, and audit logging are protocol-compliant and validated by automated test suite as of 2025-06-09.
7. Test suite must remain fully passing after any future changes (see Modification History for last validated date).


---

## LLM Integration Architecture

All LLM operations for the Deep Analysis Agent will be centralized through a dedicated A9_LLM_Service. This service abstracts LLM provider details, prompt formatting, error handling, and logging. All LLM requests are routed via the Orchestrator Agent to ensure protocol compliance, centralized logging, and future extensibility (multi-model, ensemble, escalation, etc.).

- Agents must never call LLM APIs directly; all LLM usage is through the orchestrator and A9_LLM_Service.
- All LLM input and output payloads must use strict Pydantic models for validation and auditability.
- This architecture supports future features such as ensemble querying and model/provider selection.
- **Backlog:** Enable dynamic LLM provider switching via configuration/environment variable (see BACKLOG_REFOCUS.md, post-MVP). This will allow pluggable providers (Cascade, OpenAI, Azure) with full backward compatibility and test coverage.

---

## Future Consideration: Ensemble LLM Hallucination Mitigation

**Objective:**
Reduce the risk of LLM hallucinations in critical business outputs by leveraging an ensemble of multiple, independent LLM instances.

**Description:**
- For principal-facing or high-stakes queries, the same prompt will be sent to three or more inexpensive LLM instances.
- Responses will be aggregated using consensus or semantic similarity methods.
- If the ensemble agrees, the consensus response is returned. If not, escalate to a stronger model or human review.
- All ensemble responses and aggregation logic will be logged for transparency and auditability.
- This feature will be considered and potentially implemented after initial single-LLM testing and evaluation.

**Benefits:**
- Reduces hallucination risk
- Improves trust and transparency
- Enables cost-effective, robust LLM-based insights

**Status:**
- Not in MVP. To be revisited after single-LLM solution is tested and evaluated.

<!-- End docs\prd\agents\a9_deep_analysis_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_implementation_tracker_agent_prd.md -->

# A9_Implementation_Tracker_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


> **2025-05-13 Update:**
> The A9_Implementation_Tracker_Agent is now fully refactored and compliant with Agent9 protocol and architectural standards. It uses a Pydantic config model, structured logging, orchestrator-driven registry integration (via `create_from_registry`), and protocol entrypoints with Pydantic models. HITL field is present for protocol compliance (not required for this agent). Card/config/code are now fully synchronized. Next steps: update/add tests, compliance, and monitoring as needed.
>
> **2025-07-16 Update:**
> The Implementation Tracker Agent now features asynchronous processing of action items, parallel status and alert processing, ISO-standardized timestamps, comprehensive error handling with context preservation, and structured recommendations and critical alerts generation. All features are fully implemented and tested.


## Overview

> **COMPLIANCE NOTE:**
> - This agent must ONLY be invoked via the AgentRegistry and orchestrator pattern.
> - Do NOT instantiate directly; always use the async factory method `create_from_registry`.
> - **Usage Example:**
> ```python
> tracker = await AgentRegistry.get_agent("A9_Implementation_Tracker_Agent")
> result = await tracker.track_implementation(...)
> ```

**Purpose:** The Implementation Tracker Agent manages the execution, tracking, and delivery of action items, tasks, and milestones for any change initiative. It does **not** govern overall change readiness, adoption, or value realization—that is the responsibility of the Change Management Agent. This agent receives implementation plans and milestones from the Change Management Agent and provides real-time progress, accountability, and escalation.
**Agent Type:** Core Implementation Agent
**Version:** 1.0
**Template:** Inherits from A9_Agent_Template (refactored 2025-05-12)

**Config Validation:** Uses Pydantic config model (`A9ImplementationTrackerAgentConfig`) with `hitl_enabled` field for protocol compliance (not implemented, present for A2A/A9 protocol consistency).

**Logging:** All agent logging and error handling use `A9_SharedLogger` for structured, orchestrator-propagated logs.

**Registration:** Agent must be created via the async `create_from_registry` method and registered only by the orchestrator/AgentRegistry. No self-registration or bootstrapping is allowed.

**HITL:** HITL escalation is protocol-compliant (field present, logs when enabled) but escalation logic is not implemented unless required.




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Implementation_Tracker_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_implementation_tracker_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## Distinction from Change Management Agent
- **Implementation Tracker Agent:** Focuses on operational execution, tracking individual tasks, milestones, blockers, and delivery.
- **Change Management Agent:** Governs overall change process, readiness, adoption, and value realization.

| Agent                       | Scope                | Key Outputs                           | Focus                          | Integration Points                |
|-----------------------------|----------------------|---------------------------------------|--------------------------------|-----------------------------------|
| Change Management Agent     | Org-wide change      | Change plans, readiness, audit logs    | Governance, adoption, value    | Stakeholder, Risk, Implementation |
| Implementation Tracker Agent| Implementation tasks | Task dashboards, status, escalations  | Execution, delivery, blockers  | Change Management, Project Teams  |

## Core Capabilities (MVP)
1. **Action Item Intake & Structuring**
   - Accepts action items/tasks from Change Management Agent (implementation plans/milestones), Solution Finder, Stakeholder Engagement, or user
   - Structures action items using Pydantic models (title, description, owner, due date, dependencies, status, etc.)
   - Implements defensive data handling with fallback mechanisms for incomplete data
2. **Assignment & Ownership Tracking**
   - Assigns owners and tracks responsibility for each action item
   - Supports re-assignment and delegation workflows
   - Provides structured tracking with standardized timestamps
3. **Progress Monitoring & Status Updates**
   - Tracks progress, status changes, and completion of action items
   - Supports milestone tracking and deadline reminders
   - Calculates progress percentages automatically based on status
   - Processes statuses asynchronously for performance optimization
4. **Dependency & Risk Management**
   - Identifies and tracks dependencies between tasks
   - Flags risks and blockers for escalation
   - Generates critical alerts for blocked and overdue items
   - Processes alerts asynchronously in parallel with status updates
5. **Reporting & Transparency**
   - Generates progress, status, and accountability reports
   - Provides dashboards for leadership, stakeholders, and owners
   - Produces structured recommendations with priority levels and justifications
   - Includes detailed metrics (completed, open, blocked counts)
6. **Audit Logging & Compliance**
   - Logs all action item changes, assignments, and status updates
   - Ensures compliance with documentation and reporting standards
   - Implements ISO-standardized timestamps for all tracking events
   - Provides comprehensive error handling with context preservation

---

## Input Requirements
- Pydantic input model for action items/tasks and context
- Intake of implementation plans and milestones from Change Management Agent
- Optional: user-supplied priorities, deadlines, dependencies

---

## Output Specifications
- Pydantic output model with:
  - Action item/task status and progress
  - Owner and accountability tracking
  - Milestone and deadline reports
  - Audit log reference
  - Escalation and completion events for Change Management Agent
  - Structured recommendations with priority levels, justifications, and expected impact
  - Critical alerts with severity levels and recommended actions
  - Comprehensive summary statistics (completed, open, blocked counts)
  - ISO-standardized timestamps for all events and updates

---

## Technical Requirements
- Modular, maintainable architecture
- Registry integration and async operations
- Secure configuration and error handling
- Full compliance with A2A and MCP protocols (see Protocol Compliance)
- Asynchronous processing using `asyncio.gather` for parallel operations
- Defensive data handling with type checking and fallback mechanisms
- Comprehensive error handling with context preservation
- ISO-standardized timestamp handling
- Structured logging with A9_SharedLogger integration

---

## Protocol Compliance
- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning Pydantic models for type safety, validation, and interoperability.
- The agent must implement all MCP (Minimum Compliance Protocol) requirements, including compliance checks, reporting, and error handling.
- Protocol compliance is mandatory for registry integration and agent orchestration.

---

## Success Metrics
- Action item completion rate
- On-time delivery of milestones
- Accountability and ownership clarity
- Reduction of blockers and risks
- Stakeholder satisfaction

---

## Future Scope (Not in MVP)
- Automated integration with external project management tools
- Predictive analytics for risk and delay forecasting
- Real-time collaboration and chat integration
- Automated reminders and nudges

---

## Change Log
- **2025-05-12:** Refactored agent to inherit from A9_Agent_Template, use Pydantic config model with `hitl_enabled` field, structured logging via `A9_SharedLogger`, async orchestrator-controlled registration, and protocol-compliant HITL field (not implemented). Updated documentation for A2A and MCP compliance.
- **2025-04-20:** Created initial PRD for Implementation Tracker Agent MVP.

<!-- End docs\prd\agents\a9_implementation_tracker_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_innovation_driver_agent_prd.md -->

# A9_Innovation_Driver_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Overview




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Innovation_Driver_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_innovation_driver_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting
- Integrates with the Unified Registry Access Layer for business processes, KPIs, and innovation metrics

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for business processes, innovation data, and market trends
- Must use registry data for context-aware innovation analysis and recommendations
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## LLM Integration Prioritization (2025-05-07)

A9_Innovation_Agent is the highest-priority candidate for LLM integration, benefiting from LLM-powered idea generation, creative synthesis, and debate simulation. See the "Agents Prioritized for LLM Integration" table in BACKLOG_REFOCUS.md for rationale and comparison.

---
**Purpose:** Identify, prioritize, and drive innovation opportunities across Agent9 initiatives by surfacing new ideas, evaluating feasibility and impact, and coordinating innovation projects from ideation to implementation.
**Agent Type:** Innovation/Orchestration Agent
**Version:** 1.0
**Template:** Follows A9_Agent_Template patterns

---

## Core Capabilities (MVP)
1. **Opportunity Intake & Ideation**
   - Accepts innovation ideas/opportunities (from Market Analysis, Solution Finder, user, or external sources)
   - Structures ideas using Pydantic models (source, description, value, feasibility, etc.)
2. **Feasibility & Impact Evaluation**
   - Evaluates potential value, feasibility, and alignment with strategy
   - Supports automated and user-guided evaluation workflows
3. **Prioritization & Selection**
   - Prioritizes innovation opportunities based on defined criteria (impact, feasibility, urgency, strategic fit)
   - Maintains an innovation backlog
4. **Project Coordination & Tracking**
   - Coordinates innovation projects from selection to implementation
   - Tracks milestones, status, and outcomes
   - Integrates with Implementation Tracker Agent
5. **Feedback & Continuous Improvement**
   - Gathers feedback from stakeholders and project outcomes
   - Iteratively refines innovation processes and criteria
6. **Audit Logging & Compliance**
   - Logs all innovation ideas, evaluations, decisions, and outcomes
   - Ensures compliance with documentation and reporting standards

---

## Input Requirements
- Pydantic input model for innovation ideas/opportunities and context
- Optional: user-supplied evaluation criteria, priorities, or constraints

---

## Output Specifications
- Pydantic output model with:
  - Evaluated and prioritized innovation opportunities
  - Project coordination and tracking data
  - Feedback and improvement metrics
  - Audit log reference

---

## Technical Requirements
- Modular, maintainable architecture
- Registry integration and async operations
- Secure configuration and error handling
- Full compliance with A2A and MCP protocols (see Protocol Compliance)
- Agent card loading and validation using AgentCard Pydantic schema
- Structured audit logging for all innovation analyses
- Comprehensive error handling with fallback logging
- Threshold-based alerting for innovation signals
- Signal-based innovation analysis with metrics, opportunities, alerts, and recommendations
- Timestamp-based audit logging for all operations

---

## Protocol Compliance & Implementation Status (2025-06-08)
- All agent entrypoints strictly comply with the A2A protocol, accepting and returning Pydantic models for type safety, validation, and interoperability.
- Implements all MCP (Minimum Compliance Protocol) requirements, including compliance checks, reporting, error handling, and audit logging.
- **Agent instantiation is orchestrator-controlled only:**
    - Direct instantiation is forbidden; the async factory `create_from_registry(registry, config)` must be used for all agent creation.
    - The agent enforces registry injection and will raise on fallback instantiation attempts.
    - Example usage and test patterns are documented in the agent card and test suite.
- **Agent card loading and validation:**
    - Implements `get_agent_card()` class method for loading and validating the agent card using AgentCard Pydantic schema
    - Handles YAML frontmatter parsing with proper error handling
    - Returns validated card dictionary for orchestrator-driven registration
- **Configuration validation and management:**
    - Uses A9InnovationDriverAgentConfig Pydantic model for strict config validation
    - Implements `_validate_config()` method to ensure required fields are present
    - Validates capabilities as a list and config as a dictionary
- **Comprehensive error handling:**
    - Implements try-except blocks with detailed error messages
    - Uses A9_SharedLogger for structured error logging
    - Provides fallback logging for agent initialization errors
    - Raises AgentExecutionError with context for runtime errors
- **Signal-based innovation analysis:**
    - Processes innovation signals with type, value, and source attributes
    - Calculates metrics by signal type
    - Generates opportunities based on signal values and types
    - Creates alerts for high-value signals with severity levels
    - Provides recommendations based on aggregated metrics
- Audit logging is implemented for all innovation analyses; logs are structured and handled via `A9_SharedLogger`.
- Test suite uses async factory, registry injection, and test data factories for protocol-compliant, maintainable tests.
- See `src/agents/new/cards/A9_Innovation_Driver_Agent_card.md` for up-to-date example usage and compliance status.
- All protocol, config, and test patterns match Agent9 standards as of 2025-06-08.

---

## Success Metrics
- Number and quality of innovation opportunities surfaced
- Feasibility and impact of implemented innovations
- Stakeholder satisfaction
- Timeliness and effectiveness of project execution

---

## Future Scope (Not in MVP)
- Automated ideation and brainstorming using AI/LLMs
- Integration with external innovation platforms
- Predictive analytics for innovation success
- Real-time collaborative innovation management

---

## Change Log
- **2025-04-20:** Created initial PRD for Innovation Driver Agent MVP.

<!-- End docs\prd\agents\a9_innovation_driver_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_innovation_genai_expert_agent_prd.md -->

# A9_Innovation_GenAI_Expert_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Overview
**Purpose:** Specialize in AI/ML solution development, model management, and optimization to deliver intelligent business capabilities
**Agent Type:** Innovation Team
**Version:** 1.0

## Implementation Status (2025-07-16)
**Phase 1 (MVP) Complete**
- Protocol-compliant agent structure implemented
- Core model evaluation and integration recommendation capabilities implemented
- Comprehensive model capabilities, integration patterns, and optimization strategies defined
- Modular architecture with clear separation of concerns

## Functional Requirements

### Core Capabilities
1. AI/ML Solution Design
   - Define AI/ML requirements
   - Select appropriate models
   - Design AI/ML architectures
   - Create training pipelines
   - Define evaluation metrics
   - Evaluate model suitability based on requirements
   - Recommend appropriate model types (language, embedding, vision)

2. Model Development
   - Create training datasets
   - Implement model training
   - Perform hyperparameter tuning
   - Validate model performance
   - Create model documentation
   - Estimate model performance metrics (latency, throughput, resource utilization)
   - Track model capabilities and resource requirements

3. Model Management
   - Manage model versions
   - Track model performance
   - Monitor model drift
   - Implement model updates
   - Manage model deployment
   - Estimate model costs (compute, storage, total)
   - Categorize models by capabilities and requirements

4. Performance Optimization
   - Optimize model performance
   - Implement inference optimization
   - Scale model processing
   - Monitor model metrics
   - Create optimization plans
   - Apply specific optimization strategies (caching, batching, model pruning)
   - Track optimization metrics (latency, throughput, resource utilization)
   - Implement cost optimization techniques (model selection, resource scaling)

5. Integration Management
   - Design AI/ML APIs
   - Create integration points
   - Define data interfaces
   - Manage model dependencies
   - Create deployment configurations
   - Recommend integration patterns (streaming, batch, hybrid)
   - Design architecture components and data flows
   - Optimize integration based on requirements

### Input Requirements
1. AI/ML Requirements
   - Business requirements
   - Performance criteria
   - Integration needs
   - Resource constraints
   - Maintenance requirements

2. Context Information
   - Data availability
   - Performance constraints
   - Security requirements
   - Integration points
   - Maintenance considerations

### Output Specifications
1. AI/ML Artifacts
   - Model specifications
   - Training pipelines
   - Evaluation metrics
   - Deployment configurations
   - Integration documentation
   - Structured model recommendations with performance metrics
   - Integration pattern recommendations with architecture designs
   - Optimization recommendations for performance, cost, and accuracy

2. Documentation
   - Model documentation
   - API specifications
   - Training guides
   - Performance metrics
   - Maintenance procedures

3. Metrics
   - Model performance
   - Training metrics
   - Inference metrics
   - Resource utilization
   - Maintenance metrics

## Technical Requirements

### Implementation Details
1. **Model Capabilities Database**
   - Comprehensive database of language, embedding, and vision models
   - Detailed capability tracking for each model type
   - Resource requirement specifications (CPU, memory, storage)
   - Performance metrics (latency, throughput)

2. **Integration Pattern Library**
   - Streaming, batch, and hybrid integration patterns
   - Use case mapping for each pattern
   - Performance requirements for each pattern
   - Architecture templates for common integration scenarios

3. **Optimization Strategy Framework**
   - Performance optimization techniques and metrics
   - Cost optimization approaches and tracking
   - Accuracy improvement methods and evaluation

### Integration Points
1. AI/ML Tools
   - Connect to training platforms
   - Interface with model repositories
   - Integrate with monitoring tools
   - Connect to deployment systems

2. Output Systems
   - Generate model documentation
   - Create monitoring dashboards
   - Export metrics
   - Generate reports

### Performance Requirements
1. Model Training
   - Basic training: < 1 hour
   - Complex training: < 24 hours
   - Inference: < 1 second

2. System Requirements
   - Handle large datasets
   - Process multiple models simultaneously
   - Maintain model consistency

### Scalability
1. Support for large-scale training
2. Handle multiple models
3. Scale with increasing data volume

## Security Requirements
1. Model Security
   - Secure model access
   - Model encryption
   - Access control
   - Audit trails
   - Compliance monitoring

2. Data Security
   - Secure data access
   - Data encryption
   - Access control
   - Audit trail for data access
   - Compliance monitoring

## Monitoring and Maintenance
1. Regular model performance checks
2. Performance optimization
3. Security compliance monitoring
4. Documentation updates
5. Version control maintenance

## Success Metrics
1. Model accuracy
2. Training efficiency
3. Inference performance
4. Security compliance
5. Developer satisfaction



### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_GenAI_Expert_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_genai_expert_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation


<!-- End docs\prd\agents\a9_innovation_genai_expert_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_llm_service_prd.md -->

# A9_LLM_Service PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Project Testing Standard (Mandatory)

**All agent tests in Agent9 must use Pydantic model instances for both input and output.**

- No raw dicts/lists are permitted in agent-to-agent or agent-to-test communication.
- All test fixtures, scenario data, and assertions must use Pydantic models and their attributes.
- This is required for:
  - Strict A2A protocol compliance
  - Early validation and error catching
  - Production-like test coverage
  - Demo and audit readiness
- This standard applies to all new and refactored tests, and is tracked in the project backlog (see: 'Refactor all integration and unit tests to use Pydantic model instances').

*See below for rationale and implementation details.*


## Purpose
Centralize and standardize all LLM (Large Language Model) operations for Agent9 agents, ensuring protocol compliance, auditability, and extensibility.

## Objectives
- Provide a single service interface for all LLM operations (generation, summarization, analysis, validation, etc.).
- Abstract away LLM provider details (OpenAI, Anthropic, local models, etc.) from agents.
- Enforce all LLM requests and responses to use strict Pydantic models for validation.
- Route all LLM calls through the Orchestrator Agent for logging, access control, and policy enforcement.
- Support future extensibility: multi-model, ensemble querying, escalation, and environment awareness.

## Key Features

- **Trade-Off Analysis Deliverable:** The Solution Finder Agent generates a structured trade-off analysis whenever a decision point arises (e.g., speed vs. rigor, cost vs. benefit). This deliverable compares all viable options across criteria such as time, confidence, risk, business impact, and cost, and is presented to the principal for review and decision. All decisions and rationales are logged for audit and learning.
- **Centralized LLM Service:** All agents use A9_LLM_Service for LLM operations; no direct LLM API calls from agents.
- **Provider Abstraction:** Switch between LLM providers/models without changing agent code.
- **Task-Based Model Selection:** The LLM service can select or accept a model/provider for each task or operation, based on request parameters or internal policy.
- **Pydantic Validation:** Strict input/output validation for all LLM payloads.
- **Orchestrator Routing:** All LLM calls go through the Orchestrator Agent for logging and compliance.
- **Extensible Architecture:** Ready for ensemble querying, multi-model support, and escalation logic.
- **Environment Awareness:** Use stubs/mocks in dev/test, real APIs in prod.

## Inputs/Outputs
- **Input:** LLMRequest (Pydantic model: prompt, context, operation, etc.)
- **Output:** LLMResponse (Pydantic model: response, confidence, timestamp, etc.)

## Acceptance Criteria
1. All LLM operations in Agent9 are performed via A9_LLM_Service.
2. No agent directly calls LLM APIs.
3. All LLM requests and responses are validated using Pydantic models.
4. All LLM calls are logged and auditable via the Orchestrator Agent.
5. The service supports configuration for multiple providers/models and future ensemble features.
6. The LLM service can select or accept a model per task/operation, using the `model` field in LLMRequest.
7. The Solution Finder Agent produces a trade-off analysis deliverable at decision points, presenting options, criteria (time, confidence, risk, business impact, cost), and capturing the principal's choice for audit and learning.




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_LLM_Service_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_llm_service_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting
- Integrates with the Unified Registry Access Layer for LLM provider configuration and model selection

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for LLM models, prompt templates, and operation types
- Must use registry data for context-aware LLM provider and model selection
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## Best Practices for LLM-Agent Integration

### Overview

> **2025-05-13 Update:**
> The A9_LLM_Service_Agent is now fully refactored and compliant with Agent9 protocol and architectural standards. It uses a Pydantic config model, structured logging (`A9_SharedLogger`), orchestrator-controlled registry integration, and protocol entrypoints with Pydantic models. HITL is documented as not required for this agent. Card/config/code are now fully synchronized.

Agent9 agents must interact with LLMs exclusively via the centralized A9_LLM_Service, with all requests and responses routed through the Orchestrator Agent. This ensures protocol compliance, auditability, security, and extensibility for all LLM-powered features.

### Key Principles
- **Centralized LLM Service:** All LLM operations (generation, summarization, analysis, validation, etc.) are performed via A9_LLM_Service. Agents never call LLM APIs directly.
- **Pydantic Validation:** Strict input/output validation using Pydantic models for all LLM requests and responses.
- **Orchestrator Routing:** All LLM calls are routed through the Orchestrator Agent for logging, access control, and compliance.
- **Provider Abstraction:** The LLM service abstracts away provider/model details and supports task-based model selection.
- **Operation-Based API:** LLM requests specify an operation (e.g., "summarize", "analyze", "ideate"), enabling multi-purpose LLM usage.
- **Source Attribution:** All LLM-derived outputs are explicitly marked with `source: "llm"` and include operation/type metadata for traceability and downstream clarity.
- **Extensible Output Block:** LLM outputs are structured in a flexible, typed block (e.g., insights, summaries, recommendations, citations) to support current and future use cases.
- **Prompt Flexibility:** Agents may supply prompt fragments or context, but final prompt assembly and validation are handled centrally in the LLM service.
- **Auditability:** All LLM interactions are logged and auditable via the Orchestrator.
- **Environment Awareness:** Stubs/mocks are used in dev/test environments, with real APIs in prod.

### Industry Alignment
This approach aligns with enterprise best practices for security, compliance, observability, and future-proofing. It supports both current and anticipated agent-LLM interaction patterns, including multi-turn, streaming, and ensemble querying.

### Recommendations
- Document all supported LLM output types and operations in the LLM service PRD and models.
- Maintain strict adherence to protocol and validation standards for all LLM agent interactions.
- Plan for future extensibility (e.g., streaming, multi-turn sessions, new output types).

---

## Out of Scope
- Direct LLM API usage by agents
- Provider-specific logic in agent code

## Status
- MVP: Centralized LLM service, orchestrator routing, Pydantic validation
- Future: Ensemble querying, advanced provider selection, escalation, and environment-based logic

---

## Testing Implications: Integration by Default

**All tests involving the A9_LLM_Service and Orchestrator Agent are now integration tests.**

- **Agent Interdependence:** Every test exercises at least two agents (the agent under test and the orchestrator/LLM service), and often more.
- **No True Unit Isolation:** Unit tests that mock the orchestrator/LLM service are discouraged for dev/prod parity; all tests are integration tests by default.
- **Test Setup:** Fixtures must spin up the orchestrator and LLM service agent for all test runs.
- **Test Coverage:** Tests must cover agent-to-agent communication, error propagation, and end-to-end validation using Pydantic models.
- **Naming/Documentation:** Test files and comments should reflect their integration nature. Update PRD/testing docs to clarify this paradigm shift.
- **Edge Cases:** Ensure tests cover both happy path and error scenarios across agent boundaries, not just within a single agent.

This approach ensures production-like coverage and validates the orchestration patterns that are now core to Agent9's architecture.

<!-- End docs\prd\agents\a9_llm_service_prd.md -->

---
<!-- Begin docs\prd\agents\a9_market_analysis_agent_prd.md -->

# A9_Market_Analysis_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Overview




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Market_Analysis_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_market_analysis_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting
- Integrates with the Unified Registry Access Layer for business processes, KPIs, and market data

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for business processes, market data, and competitor information
- Must use registry data for context-aware market analysis decisions
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## LLM Integration Prioritization (2025-05-07)

A9_Market_Research_Agent is a high-priority candidate for LLM integration, benefiting from LLM-powered market trend synthesis and competitive intelligence. See the "Agents Prioritized for LLM Integration" table in BACKLOG_REFOCUS.md for rationale and comparison.

---
**Purpose:** Continuously monitor, analyze, and report on market trends, competitor actions, and external signals to inform Agent9 strategy, product development, and risk/opportunity identification.
**Agent Type:** Intelligence/Analysis Agent
**Version:** 1.0
**Template:** Follows A9_Agent_Template patterns

---

## Core Capabilities (MVP)
1. **Market Signal Intake & Structuring**
   - Accepts market data/signals (feeds, reports, user input, and LLM-powered research)
   - Supports direct integration with LLM APIs (e.g., Perplexity, OpenAI, Hugging Face, Together, etc.) for rapid market intelligence gathering.
   - Structures data using Pydantic models (source, type, timestamp, content, etc.)
   - LLM-sourced insights must be parsed into structured Pydantic models and clearly marked as LLM-derived in outputs.
2. **Trend & Competitor Analysis**
   - Identifies and tracks market trends, competitor moves, and external factors
   - Supports automated and user-guided analysis workflows
   - Leverages LLMs for summarization, trend detection, and competitor research when appropriate.
3. **Alerting & Opportunity/Risk Surfacing**
   - Flags significant changes, risks, or opportunities based on analysis
   - Notifies relevant agents or stakeholders
4. **Reporting & Visualization**
   - Generates market analysis reports and dashboards
   - Supports export/sharing of insights
5. **Integration & Coordination**
   - Shares findings with Solution Finder, Risk Management, and Change Management agents
   - Supports cross-agent workflows (e.g., trigger solution ideation or risk analysis)
6. **Audit Logging & Compliance**
   - Logs all market signals, analyses, and reports
   - Ensures compliance with documentation and reporting standards
   - All LLM queries and responses must be logged for auditability and compliance.
   - LLM-derived insights must include source attribution and, where possible, references/citations from the LLM output.

---

## Input Requirements
- Pydantic input model for market signals/data and context
- Optional: user-supplied analysis parameters or filters

---

## Output Specifications
- Pydantic output model with:
  - Market trend/competitor analysis
  - Alerts and surfaced risks/opportunities
  - Reports and dashboards
  - Audit log reference

---

## Technical Requirements
- Modular, maintainable architecture
- Registry integration and async operations
- Secure configuration and error handling
- Full compliance with A2A and MCP protocols (see Protocol Compliance)
- **MCP Integration Pattern:** All join, filter, and aggregation operations are performed by the MCP (Managed Compute Platform) as a service. Agent9 requests summarized, filtered, and pre-joined data products from MCP endpoints. Agent9 acts only as an orchestrator, passing business requests and registry metadata, and receives business-ready results. This ensures performance, governance, and maintainability at scale.
- **HITL (Human-in-the-Loop) Enablement:** Supported via the config-driven `hitl_enabled` flag. When enabled, allows optional human intervention (e.g., prompt review/approval) in DEV/QA and, if desired, in production per customer workflow or trust requirements. Default is fully automated.
- **Integration-First Testing:** All integration and unit tests must use Pydantic model instances for agent input and output, ensuring strict A2A protocol compliance and production-like validation. Orchestrator-driven integration tests are required, using only real agent input/output models (no raw dicts/mocks).
- **Orchestrator-Controlled Registration:** Agent instantiation is strictly orchestrator-controlled with registry injection required. Direct instantiation is forbidden.
- **Pydantic Model Validation:** Uses A9MarketAnalysisAgentConfig for strict config validation and enforces Pydantic models for all inputs and outputs.
- **Fallback Logging System:** Implements a fallback logging mechanism with A9_SharedLogger integration and NullLogger fallback for testing.
- **LLM Integration Implementation:** Direct integration with OpenAI API (compatible with openai>=1.0.0) for market research using GPT-4.1.
- **Signal Aggregation and Analysis:** Processes market signals by type, calculates metrics, and generates trends based on signal values and sources.
- **Threshold-Based Alerting:** Creates alerts for high-value signals with severity levels and escalation flags.
- **Insight Extraction from LLM Responses:** Parses LLM responses to extract structured insights, recommendations, and trends.
- **Comprehensive Error Handling:** Implements try-except blocks with AgentExecutionError for runtime errors and AgentInitializationError for configuration issues.

---

## Protocol Compliance
- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning Pydantic models for type safety, validation, and interoperability.
- The agent must implement all MCP (Minimum Compliance Protocol) requirements, including compliance checks, reporting, and error handling.
- Protocol compliance is mandatory for registry integration and agent orchestration.
- **HITL Compliance:** HITL logic must be gated by the `hitl_enabled` config flag. No blocking or human intervention occurs unless explicitly enabled. All HITL actions must be logged for auditability.
- **Test Protocol:** All tests must use Pydantic models and orchestrator-driven flows; legacy mock-based or dict-based tests are not permitted.

---

## Success Metrics
- Timeliness and relevance of market insights
- Actionability of surfaced risks/opportunities
- Stakeholder satisfaction
- Integration effectiveness with other agents

---

## Future Scope (Not in MVP)
- Automated sentiment analysis and NLP on market signals
- Predictive analytics for market shifts
- Deeper integration with additional external data providers/APIs (beyond LLMs)
- Real-time alerting and collaborative analysis

---

## Change Log
- **2025-05-12:** Documented config-driven HITL enablement (`hitl_enabled` flag), integration-first/orchestrator-driven testing, and strict A2A protocol compliance for all tests and entrypoints. Clarified that HITL is optional and fully documented for DEV/QA/Prod use.
- **2025-04-30:** Updated PRD to explicitly support LLM-powered research (Perplexity, OpenAI, Hugging Face, etc.) as a first-class data source for market signal intake, trend/competitor analysis, and reporting. Added requirements for audit logging, source attribution, and structured Pydantic model output for LLM-derived insights.
- **2025-04-20:** Created initial PRD for Market Analysis Agent MVP.

<!-- End docs\prd\agents\a9_market_analysis_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_nlp_interface_agent_prd.md -->

# A9_NLP_Interface_Agent Product Requirements Document

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Modification History
- 2025-05-26: Added async method `parse_business_query` for orchestrator-driven, LLM-powered business query parsing. Updated Functional Requirements and compliance notes. (Cascade)

## 1. Overview

### 1.1 Purpose
The A9_NLP_Interface_Agent provides a unified interface for natural language processing capabilities across different ERP systems, enabling seamless integration of business intelligence and analytics with enterprise systems. It leverages the Unified Registry Access Layer to access business glossary terms, KPIs, and data products for context-aware language processing.

**YAML Contract Context:**
The agent must read and respond to `yaml_contract_text` provided in the context by the orchestrator, supporting protocol-compliant workflows that leverage YAML-driven data product contracts for schema, mapping, and constraints.

### 1.2 Scope
This document outlines the requirements for version 1.0 of the A9_NLP_Interface_Agent, focusing on core NLP capabilities and ERP system integration.

### 1.3 Target Audience
- System Administrators
- Integration Developers
- Business Analysts
- Data Scientists

## 2. Business Requirements

### 2.1 Business Objectives
1. Provide a standardized NLP interface for enterprise systems
2. Enable business context-aware document processing
3. Support enterprise-grade data processing and analysis
4. Facilitate integration with existing ERP systems

### 2.2 Key Metrics
- Processing latency: < 1 second for small documents
- Accuracy: > 90% for standard document types
- System availability: 99.9% uptime
- Error rate: < 1%




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_NLP_Interface_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_nlp_interface_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Integrates with the Unified Registry Access Layer for business glossary terms, KPIs, and data products
- Uses Registry Factory for provider initialization and configuration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for business terms, entity mapping, and synonym resolution
- Must use registry data for context-aware natural language processing and intent extraction
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation
- Direct enum usage (use registry providers instead)
- Hardcoded business glossary terms (use registry data)
- Initializing registry providers directly (use Registry Factory)
- Bypassing the Unified Registry Access Layer
- Duplicating registry access logic
- Caching registry data locally instead of using the registry providers

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

## Compliance & Integration Update (2025-05-12)
- HITL (Human-in-the-Loop) enablement is fully implemented and enforced for all key actions:
  - The agent config supports a `hitl_enabled` flag.
  - Output protocol fields (`human_action_required`, `human_action_type`, `human_action_context`) are present and validated.
  - When HITL is enabled, all outputs require human approval and set these fields accordingly.
- All integration and testing for this agent is orchestrator-driven and production-like:
  - Agent inputs and outputs are always real Pydantic model instances, never mocked or stubbed.
  - Integration tests simulate true production workflows, with the orchestrator coordinating all agent calls and data flows.
- Logging uses `A9_SharedLogger` and is propagated to the Orchestrator Agent.
- Error handling is standardized and protocol-compliant.
- Strict A2A protocol compliance (Pydantic models for all entrypoints/outputs) is maintained.
- HITL escalation and LLM integration priorities are referenced per the Agent9 LLM Benefit Table and scientific method enforcement in the backlog.

## 3. Functional Requirements

### 3.0 Protocol Compliance Update (2025-06-24)
- All entrypoints use Pydantic models: `NLPBusinessQueryInput` (input) and `NLPBusinessQueryResult` (output).
- HITL protocol fields (`human_action_required`, `human_action_type`, `human_action_context`) are present and validated.
- All event logging is async/awaited and structured.
- YAML contract context (`yaml_contract_text`) is always propagated via the orchestrator and accessed in all protocol-compliant methods.
- Orchestrator controls all agent instantiation and registration; no self-registration logic.
- All integration and end-to-end tests must use orchestrator-driven workflows.
- Example usage and documentation must show orchestrator-driven invocation and registry integration.
- All input/output model references must match those in `agent_config_models.py`.

### 3.1 Core NLP Capabilities

#### 3.1.5 Natural Language to Data Query (NLQ) Capability

- The agent must support matching business questions to KPIs using both the `kpi_name` and all synonyms defined in the KPI registry. The registry now includes a `synonyms` column (comma-separated), and the agent must check both the technical name and all synonyms when resolving a KPI from a business question.
- This enables robust matching for business terms such as "sales", "revenue", "discounts", and other common synonyms, ensuring user queries are mapped to the correct technical KPI.


- When present, the agent must use `yaml_contract_text` from the context to inform schema mapping, business term resolution, and query validation.

- The agent must detect and extract TopN/BottomN intent (e.g., "top 5 regions by revenue", "bottom 10 products by sales") from the user's natural language query. This must be passed as a structured parameter (e.g., {"limit_type": "top", "limit_n": 5, "limit_field": "revenue"}) in the protocol output for downstream query assembly and enforcement. If the user's request is ambiguous (e.g., "top regions" without a number), default to a configurable N (e.g., 10).

- The agent pipeline must invoke the Data Governance Agent to resolve business terms to technical columns/KPIs.
- If any terms cannot be mapped, the agent must escalate to HITL by setting output fields:
    - `human_action_required: True`
    - `human_action_type: "clarification"`
    - `human_action_context`: includes unmapped terms and a user-facing message.
- The LLM or UI must prompt the principal to clarify or select mappings for these terms before continuing.
- The agent must support interpreting natural language business queries from Principals or Agents and transforming them into structured data queries against the MCP data service.
- The pipeline must include:
  - **Workflow Components:**
    1. **Input Processing:** Parse natural language inputs for key entities, intents, and context.
    2. **Business Term Mapping:** Map extracted terms to business glossary entries using the Unified Registry Access Layer.
    3. **Principal Context Enhancement:** Combine extracted context with principal context from the orchestrator.
    4. **Data Product Resolution:** Identify the correct data product/view for the query using the Data Product Registry Provider.
    5. **Registry Access:** Use the appropriate registry providers through the Unified Registry Access Layer for all registry data access.
  - **Query Generation**: Translate the parsed intent into a structured API call (HTTP GET with query parameters) to the MCP service. Optionally, support SQL query generation if the MCP exposes a SQL endpoint.
  - **MCP Integration**: Call the MCP service endpoint, handle the response, and format it for the user or downstream agent.
  - **Synonym/Mapping Support**: Map business terms (e.g., "customer" or "revenue") to technical attribute names used in the data registry.
  - **Robust Error Handling**: Handle unmapped terms, unsupported queries, and MCP errors gracefully, providing actionable feedback.

- Example workflow:
  - Input: "Show me total value by customer for product X in 2024"
  - Intent Parsing Output: {product_id: ..., groupby: "customer", filters: {product: "X", year: 2024}, aggregation: "sum", measure: "value"}
  - API Call: `/data-product/{product_id}?groupby=customer&filter=product:X,year:2024`
  - MCP returns JSON result, which is formatted for the end user.

- The agent must be extensible to support new query types, filters, and aggregations as the MCP evolves.
- The NLQ capability must be covered by integration and end-to-end tests with example business queries and expected API calls/results.

#### 3.1.6 Entity Extraction as Core Capability (MVP)

#### 3.1.7 Unified Registry Access Layer Integration for Entity Resolution
- The NLP Interface Agent MUST use the Business Glossary Provider from the Unified Registry Access Layer to resolve entities and business terms.
- Entity extraction MUST reference and validate against the canonical business terms stored in the registry.
- All extracted entities MUST be mapped to their technical counterparts using registry relationships.

#### 3.1.8 LLM Explainability Compliance (2025-06-24)
- All NLP model outputs that require business user-friendly explanations are routed through the A9_LLM_Service_Agent for explainability.
- LLM calls are protocol-compliant, orchestrator-driven, and fully async with structured event logging and error handling.
- See agent card for implementation and compliance details.

- When present, the agent must use `yaml_contract_text` from the context to improve entity mapping and validation.

- **Objective:**
  - Extract business-relevant entities (e.g., company, department, region, product, date) from user business questions and natural language queries.
  - Output structured entities that can be mapped to principal context and used as filters in downstream data product/API calls.

- **Justification:**
  - Many filters required for secure, context-aware query generation are derived from the Principal context (e.g., department, region, business unit).
  - Robust entity extraction is essential to ensure that queries are properly scoped, access-controlled, and relevant to the user's business context.
  - Without entity extraction, Agent9 cannot reliably map user intent to secure, personalized queries.

- **Requirements:**
  1. The agent MUST implement and expose only the `entity_extraction` NLP model for the MVP. All other NLP models (document analysis, relationship analysis, sentiment analysis) are to be omitted or stubbed until post-MVP.
  2. The agent MUST use the Registry Factory to access the Business Glossary Provider and KPI Provider for entity resolution and validation.
  2. Entity extraction MUST:
     - Accept a natural language business question as input.
     - Return a list of entities, each with at least `type` and `value` fields (e.g., `{type: "department", value: "Finance"}`).
     - Support extensibility for additional entity types as business needs evolve.
     - Be directly integrated with the orchestrator's principal context filter application logic.
     - Be covered by integration and end-to-end tests using real-world business queries and principal contexts.
  3. The entity extraction method MUST be robust to synonyms, abbreviations, and common business language variations by leveraging the Business Glossary through the Unified Registry Access Layer.
  4. The agent MUST escalate to HITL if ambiguous or unmapped entities are detected, with actionable feedback for the user.
  5. The agent MUST use the RegistryFactory to initialize and access all registry providers.
  6. The agent MUST NOT cache registry data locally; instead, it should always access the latest data through the Unified Registry Access Layer.

- **Example Workflow:**
  - Principal context: `{ department: "Finance", region: "EMEA" }`
  - User question: "Show payroll for April 2023"
  - Entity extraction output: `[{'type': 'date', 'value': 'April 2023'}]`
  - Orchestrator applies principal context: `[{'type': 'department', 'value': 'Finance'}, {'type': 'region', 'value': 'EMEA'}]`
  - Final query is filtered by both extracted and principal context entities.

- **Notes:**
  - Entity extraction is the foundation for secure, context-aware NLQ-to-SQL/data product workflows in Agent9.
  - All future NLP model enhancements should build on this robust entity extraction core.

#### 3.1.6 Business Query Parsing via LLM (NEW)
- The agent provides an async method `parse_business_query(input: NLPBusinessQueryInput) -> NLPBusinessQueryResult`.
- This method is orchestrator-driven, protocol-compliant (Pydantic input/output), and delegates intent parsing to the LLM Service Agent.
- It takes a business query (as business terms) and a data product agent reference, retrieves the KPI registry, and uses the LLM to extract structured intent (KPI, groupings, filters).
- Handles errors according to protocol, logs all actions, and enforces HITL protocol if enabled (fields: `human_action_required`, `human_action_type`, `human_action_context`).
- Only accessible via the orchestrator/registry pattern; not for direct instantiation.
- Example input/output:
  - Input: `NLPBusinessQueryInput(business_terms=["total revenue by region 2024"], data_product_agent=...)`
  - Output: `NLPBusinessQueryResult(matched_views=[{"kpi_name": ..., "groupings": ..., "time_filter": ...}], unmapped_terms=[...], ... )`
- Test coverage must include integration with the orchestrator, LLM agent, error/HITL handling, and edge cases for unmapped terms.
- Usage documentation and examples:

  **parse_business_query usage example:**
  ```python
  from src.agents.new.A9_NLP_Interface_Agent import A9_NLP_Interface_Agent
  from src.agents.new.agent_config_models import A9NLPInterfaceAgentConfig, NLPBusinessQueryInput
  config = {
      "name": "A9_NLP_Interface_Agent",
      "version": "1.0",
      "capabilities": ["nlq", "document_analysis", "parse_business_query"],
      "config": {},
      "hitl_enabled": False
  }
  agent = A9_NLP_Interface_Agent(config)
  input_model = NLPBusinessQueryInput(
      business_terms=["Show me the top 5 regions by revenue for 2024"],
      data_product_agent=...,
      principal_context={"filters": {"region": "North America"}}
  )
  result = await agent.parse_business_query(input_model)
  # Access result.topn, result.filters, result.principal_context, result.human_action_required, etc.
  ```

- **Output Protocol Fields:**
  - `matched_views`: List[dict] — NLQ intent resolution (KPI/groupings/time/filter)
  - `unmapped_terms`: List[str] — Terms not mapped during parsing
  - `filters`: dict — Final filters applied (business or technical)
  - `topn`: dict — TopN/BottomN extraction (e.g., {"limit_type": "top", "limit_n": 5, "limit_field": "revenue"})
  - `principal_context`: dict — Principal context used for defaults/filters
  - `human_action_required`: bool — True if HITL escalation is required
  - `human_action_type`: str — Type of HITL action (e.g., "clarification")
  - `human_action_context`: dict — Context for HITL (e.g., unmapped terms, user message)


#### 3.1.1 Document Analysis
- Process structured and unstructured documents
- Extract key information and entities
- Perform sentiment analysis
- Identify relationships between entities
- Accept technical attribute names and code values as filters (not business English)
- Example interface:
  - `async def analyze_document(document: str, filters: Dict[str, Any]) -> DocumentAnalysis`
- Input must be output from Data Governance Agent (already translated)
- Output: Structured summary, key points, entities, relationships, and confidence score
- Example:
```python
# Input (from governance agent)
filters = {"REGION_CODE": "NA", "PROD_CODE": "CE"}
result = await nlp_agent.analyze_document("Revenue by Business Unit for Q1", filters=filters)
# Output (DocumentAnalysis)
{
  "summary": "Revenue for North America, Consumer Electronics is above target.",
  "key_points": ["Growth YoY: 8%", "Margin: 12%"],
  "entities": [...],
  "relationships": [...],
  "confidence": 0.95
}
```
- Must validate all filter keys/values and handle unmapped filters with robust error handling
- Log all analysis attempts, successes, and failures

#### 3.1.2 Entity Recognition
- Identify business-specific entities
- Classify entities by type
- Extract relationships between entities
- Handle enterprise-specific terminology

#### 3.1.3 Relationship Analysis
- Identify relationships between business entities
- Analyze document relationships
- Create entity relationship graphs
- Generate relationship visualizations

#### 3.1.4 Business Context
- Understand business-specific terminology
- Process industry-specific documents
- Handle enterprise-specific data formats
- Maintain business context awareness

### 3.2 ERP System Integration

#### 3.2.1 Supported Systems
- SAP (S/4HANA, BW, HANA)
- Oracle (E-Business Suite, Cloud ERP)
- Microsoft Dynamics
- Other major ERP systems

#### 3.2.2 Integration Features
- Secure connection to ERP systems
- Data format conversion
- Error handling and recovery
- Performance optimization

### 3.3 Security Requirements
- Secure data transmission
- Role-based access control
- Audit logging
- Compliance with data protection regulations

## 4. Non-Functional Requirements

### 4.1 Performance
- Process documents in real-time
- Handle large document volumes
- Maintain consistent performance
- Support concurrent processing

### 4.2 Scalability
- Scale horizontally
- Support multiple ERP connections
- Handle increasing document volumes
- Maintain performance under load

### 4.3 Reliability
- High availability architecture
- Automatic failover
- Data backup and recovery
- Error handling and recovery

## 5. Technical Requirements

### 5.1 System Architecture
- Microservices-based architecture
- RESTful API interface
- Containerized deployment
- Cloud-native design

### 5.2 Technology Stack
- Python 3.10+
- FastAPI for REST API
- SQLAlchemy for database
- Docker for containerization
- Kubernetes for orchestration

### 5.3 Integration Points
- ERP system APIs
- Authentication services
- Logging infrastructure
- Monitoring system

### 5.4 Performance Optimization
- Implements YAML contract caching mechanism for improved performance
- Provides optimized contract access during agent operation
- Reduces redundant parsing and processing of YAML contracts

### 5.5 Entity Extraction Implementation
- Implements regex-based entity extraction for standard entity types
- Provides extensible pattern matching for custom entity recognition
- Supports entity normalization and standardization
- Includes confidence scoring for extracted entities

### 5.6 HITL Escalation
- Implements protocol-compliant HITL escalation logic
- Provides structured context for human review and intervention
- Supports configurable escalation thresholds
- Includes detailed logging of escalation events

### 5.7 Error Handling
- Implements comprehensive error handling for all agent operations
- Provides standardized error response format
- Supports graceful degradation for partial failures
- Includes detailed error logging for troubleshooting

## 6. Implementation Phases

### Phase 1: Core NLP Engine (2 weeks)
- Basic NLP processing
- Core document analysis
- Basic entity recognition
- Initial error handling

### Phase 2: ERP Integration (2 weeks)
- ERP system connections
- Data format conversion
- Basic integration tests
- Security implementation

### Phase 3: Advanced Features (2 weeks)
- Relationship analysis
- Business context processing
- Performance optimization
- Advanced error handling

### Phase 4: Testing and Documentation (1 week)
- Comprehensive testing
- Performance testing
- Security testing
- Documentation creation

## 7. Dependencies

### 7.1 External Dependencies
- ERP system licenses
- NLP service subscriptions
- Cloud infrastructure
- Security services

### 7.2 Internal Dependencies
- Agent Registry
- Authentication System
- Logging Infrastructure
- Monitoring System

## 8. Agent Requirements

### 8.1 Core Agent Interface
- Follow Agent9 agent registry interface requirements
- Implement required registry integration methods
- All document analysis functions must be async
- Input must be compatible with Data Governance Agent output
- Use standard error handling patterns
- Maintain consistent logging
- Log all integration attempts and failures

### 8.2 Configuration Management
- Support default configuration values
- Validate configuration on initialization
- Maintain configuration state
- Handle agent-specific configuration

### 8.3 Error Handling
- Implement structured error handling
- Log errors appropriately
- Return consistent error responses
- Handle common error types (connection, processing, validation)
- Validate all technical attribute names and filter code values
- Handle unmapped filters/attributes with robust error handling

### 8.4 Logging
- Initialize agent-specific logger
- Log initialization and major operations
- Support different log levels
- Include timestamps in logs

### 8.5 Core Methods
- Implement _initialize for setup
- Implement _setup_logging for logging
- Implement _setup_error_handling for error management
- Implement create_from_registry class method
- Implement analyze_document as async, accepting technical filters

### 8.6 Error Types
- ConfigurationError: Invalid configuration
- RegistrationError: Failed to register with registry
- ProcessingError: Failed to process data
- ValidationError: Invalid input data
- ConnectionError: Connection failures
- UnmappedFilterError: Provided filter could not be mapped to a technical attribute

## 9. Testing Requirements (NEW, MVP Alignment)
- Test orchestrator-driven business query parsing (parse_business_query) with LLM Service Agent
- Validate protocol compliance (Pydantic input/output)
- Test error and HITL handling paths
- Test unmapped terms and edge cases
- Document test coverage and assumptions (pending usage example update)

- Test document analysis with technical filters (integration with Data Governance Agent)
- Test input validation for technical attribute names/code values
- Test error handling for unmapped filters/attributes
- Test async operation and registry integration
- Test integration with Data Product Agent (end-to-end flow)
- Document test coverage and assumptions

## 10. Acceptance Criteria

### 10.0 Protocol Compliance (2025-06-24)
- All entrypoints use only protocol-compliant models as defined in code.
- Orchestrator-driven lifecycle is enforced; no agent-side registration or direct instantiation in documentation or code samples.
- All event logging is async/awaited and uses `A9_SharedLogger`.
- YAML contract context is always propagated and accessed via the context kwarg.
- All tests and usage examples are orchestrator-driven.

### 10.1 Functional
- Successful ERP system integration
- Accurate document processing
- Proper error handling
- Complete documentation

### 10.2 Non-Functional
- Performance meets requirements
- Security requirements met
- Scalability verified
- Documentation complete

## 11. Maintenance and Support

## 12. Compliance: YAML Contract Context
- The agent must always check for and apply `yaml_contract_text` from the context if present, for all entrypoints. This is required for A2A protocol compliance and is enforced by orchestrator-driven workflow execution and tests.

### Example: Accessing YAML Contract Context in a Protocol-Compliant Method
```python
# In any protocol-compliant agent method:
def some_method(self, input_model, context=None):
    yaml_contract = context.get('yaml_contract_text') if context else None
    # Use yaml_contract for schema mapping, validation, etc.
```

## 13. Roadmap and Future Enhancements

- **SQL Query Support:**
  - Future versions will support generating SQL statements from natural language queries and submitting them to the MCP data service if a SQL endpoint is available.
  - The agent will be able to translate NLQ into SQL for advanced analytics and BI tool compatibility.
  - Planned support for SQL dialects compatible with DuckDB and major enterprise data platforms.

### 13.1 Maintenance
### 11.1 Maintenance
- Regular updates
- Security patches
- Performance optimization
- Documentation updates

### 11.2 Support
- User documentation
- API documentation
- Troubleshooting guide
- Support channels


<!-- End docs\prd\agents\a9_nlp_interface_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_opportunity_analysis_agent_prd.md -->

# A9_Opportunity_Analysis_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->





### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Opportunity_Analysis_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_opportunity_analysis_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting
- Integrates with the Unified Registry Access Layer for opportunity models, business processes, and KPIs

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for opportunity analysis, business processes, and KPI evaluation
- Must use registry data for context-aware opportunity identification and scoring
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

## Compliance & Integration Update (2025-07-16)
- This agent is now fully compliant with Agent9 async, registry, error handling, and centralized logging standards.
- Legacy patterns and non-compliant integration logic have been removed.
- All integration with this agent is now orchestrator-driven and dynamic; static workflow tests have been removed.
- Test coverage is maintained via orchestrator-based persona and workflow scenarios.
- A9Agent inheritance implemented for standardized agent behavior.
- Pydantic model validation enforced for all inputs and outputs.

## Purpose
Conducts opportunity analysis to identify, score, and recommend actionable business opportunities based on market, product, partnership, innovation, and optimization metrics. The agent evaluates different types of opportunities and provides structured recommendations with quantified potential and risk assessments.

## Key Capabilities
- Opportunity scoring and ranking
- Multi-factor analysis (market, product, partnership, innovation, optimization)
- Recommendation generation
- Integration with orchestrator workflows
- Structured opportunity type classification
- Quantitative risk and potential assessment
- Metrics generation for opportunity categories
- Protocol-compliant input/output model validation

## Inputs
- Opportunity analysis input model (Pydantic)
- Contextual data including:
  - Market context (market size, competition, growth rate, entry barriers, trend score)
  - Product context (innovation, differentiation, market fit, implementation, tech readiness)
  - Partnership context (synergy, alignment, value add, risks, compatibility)
  - Innovation context (potential, impact, feasibility, resources, market window)
  - Optimization context (efficiency, cost reduction, quality, time to market, ROI)

## Outputs
- Structured opportunity lists by category:
  - Market opportunities
  - Product opportunities
  - Partnership opportunities
  - Innovation opportunities
  - Optimization opportunities
- Overall opportunity score (0-100)
- Actionable recommendations
- Metrics by opportunity category
- Status indicator

## Integration
- Registered with Agent9 orchestrator and agent registry
- Accepts only Pydantic models for all entrypoints
- Uses shared logging and error handling patterns
- Inherits from A9Agent base class
- Implements protocol-compliant analyze_opportunities method
- Enforces strict input validation with ValidationError handling

## Test Coverage
- Covered by orchestrator-driven integration/persona scenario tests
- No legacy or static workflow tests remain
- Factory pattern implemented for test data generation
- Comprehensive model factories for all input contexts
- Deterministic output generation for test stability

<!-- End docs\prd\agents\a9_opportunity_analysis_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_orchestrator_agent_prd.md -->

# A9_Orchestrator_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Overview
**Purpose:** Coordinate innovation workflow and agent collaboration through workflow orchestration, agent coordination, and decision making. Integrate with the Unified Registry Access Layer to access business processes, KPIs, data products, and principal profiles for context-aware orchestration.

> **COMPLIANCE NOTE:**
> - This agent must ONLY be invoked via the AgentRegistry and orchestrator pattern.
> - Do NOT instantiate directly; always use the async factory method `create_from_registry`.
> - **Usage Example:**
> ```python
> orchestrator = await AgentRegistry.get_agent("A9_Orchestrator_Agent")
> result = await orchestrator.run_workflow(...)
> ```

**Agent Type:** Core Agent
**Version:** 1.0

## Functional Requirements

### Core Capabilities
1. Workflow Orchestration
   - Manage workflow state
   - Track workflow progress
   - Create workflow plans
   - Generate workflow metrics
   - Handle workflow exceptions
   - Integrate with Registry Factory and providers

2. Agent Coordination
   - Manage agent registry
   - Track agent status
   - Create coordination plans
   - Generate coordination metrics
   - Handle agent handoffs
   - Provide registry access to agents

3. Decision Making
   - Make workflow decisions
   - Track decision history
   - Generate decision metrics
   - Handle decision conflicts
   - Create decision documentation

4. State Management
   - Manage workflow state
   - Track agent state
   - Create state snapshots
   - Generate state metrics
   - Handle state transitions

5. Communication Management
   - Route messages between agents
   - Track communication status
   - Generate communication logs
   - Handle communication failures
   - Create communication metrics

### Input Requirements
1. Workflow Data
   - Workflow definitions
   - Agent configurations
   - State information
   - Communication requirements
   - Decision criteria

2. Context Information
   - Workflow requirements
   - Agent capabilities
   - State constraints
   - Communication needs
   - Decision parameters

### Output Specifications
1. Workflow Artifacts
   - Workflow plans
   - State snapshots
   - Decision records
   - Communication logs
   - Metrics reports

2. Analytics
   - Workflow metrics
   - Agent metrics
   - State metrics
   - Communication metrics
   - Decision metrics

3. Reports
   - Workflow status
   - Agent coordination
   - State changes
   - Communication status
   - Decision outcomes

## Technical Requirements

### Registry Architecture Integration
- Must initialize the Registry Factory during startup
- Must configure and register all required registry providers
- Must provide registry access to agents during orchestration
- Must use registry data for context-aware workflow decisions
- Must support backward compatibility with legacy code

### Integration Points
1. Agent Systems
   - Connect to all agents
   - Interface with workflows
   - Integrate with state management
   - Connect to communication systems
   - Support dynamic agent loading via AGENT_MODULE_MAP
   - Implement workflow-driven agent imports to maintain protocol compliance
   - Initialize and provide access to the Unified Registry Access Layer
   - Create and configure registry providers for all business domains

### Implementation Details

#### Dynamic Agent Loading
- Implements a dynamic agent import mechanism using AGENT_MODULE_MAP
- Supports both class-based and string-based agent registration
- Provides runtime agent class resolution to maintain orchestrator control
- Enforces strict A2A protocol compliance for all dynamically loaded agents

#### Concurrency Management
- Implements semaphore-based concurrency control for workflows
- Limits simultaneous workflow execution to prevent resource contention
- Provides proper async/await patterns for all workflow operations
- Ensures thread safety for shared resources and registry access

#### Error Handling and Logging
- Implements comprehensive error handling with full traceback capture
- Uses structured logging via A9_SharedLogger for all operations
- Provides detailed audit logs for all workflow steps and agent interactions
- Captures and formats exceptions for proper debugging and monitoring

#### Model Handling and Conversion
- Implements recursive model-to-dict conversion utility for serialization
- Ensures Pydantic v2 compliance with proper model validation
- Handles legacy BaseModel instances with backward compatibility
- Validates all workflow inputs and outputs against protocol requirements

#### Backward Compatibility
- Provides method aliases for backward compatibility (e.g., run_workflow)
- Supports both new and legacy input/output model formats
- Ensures seamless integration with existing agent implementations
- Maintains consistent behavior across API versions

#### Workflow Validation and Execution
- Implements detailed step validation before execution
- Provides comprehensive agent and method validation
- Supports dynamic method resolution and invocation
- Ensures protocol compliance for all workflow steps

2. Output Systems
   - Generate reports
   - Create logs
   - Export metrics
   - Generate snapshots

### Performance Requirements
1. Workflow Management
   - Workflow updates: < 100ms
   - State changes: < 1 second
   - Communication: < 50ms

2. System Requirements
   - Handle multiple workflows
   - Process multiple messages
   - Maintain state consistency

### Scalability
1. Support for multiple workflows
2. Handle multiple agents
3. Scale with increasing complexity

## Security Requirements
1. Workflow Security
   - Secure workflow data
   - Protect state information
   - Secure communication

2. Access Control
   - Role-based access
   - Secure data sharing
   - Audit trail for changes
   - Workflow approval workflows

## Monitoring and Maintenance
1. Regular workflow updates
2. Continuous state monitoring
3. Periodic validation
4. Regular performance optimization

## Success Metrics
1. Workflow consistency
2. Agent coordination
3. State accuracy
4. Communication efficiency
5. Decision quality




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Orchestrator_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_orchestrator_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation
- Direct enum usage (use registry providers instead)
- Hardcoded business logic (use registry data)
- Initializing registry providers directly (use Registry Factory)

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## Best Practices for LLM-Agent Integration

### Overview
Agent9 agents must interact with LLMs exclusively via the centralized A9_LLM_Service, with all requests and responses routed through the Orchestrator Agent. This ensures protocol compliance, auditability, security, and extensibility for all LLM-powered features.

### Key Principles
- **Centralized LLM Service:** All LLM operations (generation, summarization, analysis, validation, etc.) are performed via A9_LLM_Service. Agents never call LLM APIs directly.
- **Pydantic Validation:** Strict input/output validation using Pydantic models for all LLM requests and responses.
- **Orchestrator Routing:** All LLM calls are routed through the Orchestrator Agent for logging, access control, and compliance.
- **Provider Abstraction:** The LLM service abstracts away provider/model details and supports task-based model selection.
- **Operation-Based API:** LLM requests specify an operation (e.g., "summarize", "analyze", "ideate"), enabling multi-purpose LLM usage.
- **Source Attribution:** All LLM-derived outputs are explicitly marked with `source: "llm"` and include operation/type metadata for traceability and downstream clarity.
- **Extensible Output Block:** LLM outputs are structured in a flexible, typed block (e.g., insights, summaries, recommendations, citations) to support current and future use cases.
- **Prompt Flexibility:** Agents may supply prompt fragments or context, but final prompt assembly and validation are handled centrally in the LLM service.
- **Auditability:** All LLM interactions are logged and auditable via the Orchestrator.
- **Environment Awareness:** Stubs/mocks are used in dev/test environments, with real APIs in prod.

## Implementation Details

### Protocol-Compliant Entrypoints

#### 1. `orchestrate_workflow`
- **Purpose**: Primary method for workflow execution with comprehensive error handling
- **Input**: `OrchestratorWorkflowInput` model
  ```python
  class OrchestratorWorkflowInput(BaseModel):
      workflow_name: str
      steps: List[WorkflowStep]
      context: Optional[Dict[str, Any]] = None
  ```
- **Output**: `OrchestratorWorkflowOutput` model with status and results
- **Status Codes**:
  - `success`: All steps completed successfully
  - `partial_success`: Some steps completed successfully, but others failed
  - `error`: The workflow failed to execute

#### 2. `register_agent`
- **Purpose**: Register a new agent in the registry
- **Input**: 
  - `agent_class`: The agent class to register
  - `agent_id`: Unique identifier for the agent
  - `config`: Optional configuration dictionary
- **Output**: Dictionary with registration status and metadata

#### 3. `run_workflow` (Legacy)
- **Deprecated**: Maintained for backward compatibility
- **Recommendation**: Use `orchestrate_workflow` for new code

### Error Handling

#### Workflow Execution Errors
- Each step's error is captured in the `AgentErrorOutput` model:
  ```python
  class AgentErrorOutput(BaseModel):
      error: str
      agent_name: str
      step_index: int
      details: str = ""
  ```
- Workflow continues on step failure when `continue_on_error` is True
- Comprehensive error messages with stack traces in debug mode

#### Input Validation
- All input models validated using Pydantic
- Workflow steps validated before execution
- Type checking for agent method parameters

### Configuration

#### Environment Variables
- `A9_ORCHESTRATOR_LOG_LEVEL`: Logging level (default: INFO)
- `A9_ORCHESTRATOR_ENABLE_AUDIT`: Enable audit logging (default: true)
- `A9_ORCHESTRATOR_CONTINUE_ON_ERROR`: Continue workflow execution on error (default: false)

#### Agent Configuration
- Loaded from agent card YAML frontmatter
- Validated against agent's Pydantic model
- Cached for performance

### Audit Logging
- All workflow executions logged with timing information
- Agent registration and instantiation events
- Detailed error logging with context
- Structured log format for easy querying

## Usage Examples

### Basic Workflow Execution
```python
workflow = OrchestratorWorkflowInput(
    workflow_name="data_processing",
    steps=[
        WorkflowStep(
            agent_name="data_loader",
            method="load_data",
            input=DataLoaderInput(source="api", endpoint="/data")
        ),
        WorkflowStep(
            agent_name="data_processor",
            method="process_data",
            input=DataProcessorInput(transformations=["clean", "transform"])
        )
    ]
)
result = await orchestrator.orchestrate_workflow(workflow)
```

### Error Handling
```python
try:
    result = await orchestrator.orchestrate_workflow(workflow)
    if result.status == "error":
        print(f"Workflow failed: {result.output}")
    elif result.status == "partial_success":
        print("Workflow completed with some errors")
        for agent, output in result.output.items():
            if isinstance(output, AgentErrorOutput):
                print(f"Error in {agent}: {output.error}")
    else:
        print("Workflow completed successfully")
except Exception as e:
    print(f"Fatal error: {str(e)}")
```

## Best Practices

1. **Use Pydantic Models**
   - Define input/output models for all agent methods
   - Use field validators for complex validation
   - Document all model fields with docstrings

2. **Error Handling**
   - Handle expected errors gracefully
   - Provide meaningful error messages
   - Include context for debugging

3. **Workflow Design**
   - Keep steps focused and single-purpose
   - Minimize dependencies between steps
   - Use context to pass data between steps

4. **Testing**
   - Test all error conditions
   - Mock external dependencies
   - Verify audit logs

## Compliance

### A2A Protocol
- All agent communications use the A2A protocol
- Input/output models follow protocol specifications
- Error handling follows protocol guidelines

### Security
- Input validation for all external data
- Sensitive configuration stored securely
- Audit logging for all operations

### Performance
- Lazy loading of agent instances
- Configurable timeouts for long-running operations
- Efficient resource cleanup

## Future Enhancements

1. **Performance Optimization**
   - Parallel step execution
   - Caching of agent instances
   - Batch processing support

2. **Enhanced Monitoring**
   - Real-time workflow visualization
   - Performance metrics collection
   - Alerting for failed workflows

3. **Developer Experience**
   - Improved error messages
   - Better debugging tools
   - Enhanced documentation

---

### Industry Alignment
This approach aligns with enterprise best practices for security, compliance, observability, and future-proofing. It supports both current and anticipated agent-LLM interaction patterns, including multi-turn, streaming, and ensemble querying.

### Recommendations
- Document all supported LLM output types and operations in the LLM service PRD and models.
- Maintain strict adherence to protocol and validation standards for all LLM agent interactions.
- Plan for future extensibility (e.g., streaming, multi-turn sessions, new output types).

---

## Hybrid Orchestration Strategy

### Purpose
Enable Agent9 agents to operate seamlessly both:
- **Within the Agent9 orchestrator** (for A2A protocol enforcement, fine-grained audit, and portability)
- **Inside enterprise agentic environments** (SAP, Google, Salesforce, etc.), leveraging their native orchestration for infra-level workflow, scaling, and monitoring

### Key Principles

1. **A2A/MCP Enforcement:**  
   Agent9’s orchestrator is the source of truth for Pydantic model validation, agent-to-agent handoff, and compliance/audit logging, regardless of host environment.

2. **Interoperability:**  
   The orchestrator exposes APIs and interfaces that allow it to be:
   - Called as a microservice or workflow step from SAP/Google orchestrators
   - Embedded as a library within larger enterprise workflows
   - Operate standalone for local/demo or hybrid cloud deployments

3. **Delegation:**  
   - **Enterprise orchestrators** handle scheduling, scaling, retries, and external integration.
   - **Agent9 orchestrator** manages agent registry, dynamic team formation, protocol enforcement, and workflow-specific logging/metrics.

4. **Portability:**  
   - Agent9 agents and orchestrator can be deployed on-prem, in the cloud, or in hybrid environments without code changes.
   - All A2A/MCP features work identically across environments.

### Functional Requirements (Hybrid-Specific)

- Expose REST/gRPC endpoints for workflow invocation from enterprise orchestrators
- Support callback/webhook integration for state updates and event notifications
- Allow configuration to delegate workflow steps to SAP/Google-native orchestrators where appropriate
- Maintain audit and compliance logs within Agent9, even when running inside enterprise orchestrators

### Example Integration Patterns

| Pattern                | Enterprise Orchestrator | Agent9 Orchestrator | Use Case                                |
|------------------------|------------------------|---------------------|-----------------------------------------|
| Standalone             | No                     | Yes                 | Local demo, MVP, full Agent9 control    |
| Embedded Microservice  | Yes                    | Yes                 | SAP/Google calls Agent9 for core logic  |
| Hybrid Delegation      | Yes                    | Yes                 | SAP/Google manages flow, Agent9 manages agent handoff, compliance, logging |

### MVP Scope

- Demonstrate orchestration both standalone and as a callable service from an enterprise workflow (mocked if needed)
- Show audit trail and compliance logs regardless of invocation source
- Document configuration for both deployment models

### Success Criteria

- All agent handoffs are A2A-compliant (Pydantic models) in every environment
- Audit logs and compliance reports are generated for every workflow, regardless of host
- Orchestrator APIs are callable from at least one external orchestrator (demo/mocked is acceptable for MVP)

<!-- End docs\prd\agents\a9_orchestrator_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_performance_optimization_agent_prd.md -->

# A9_Performance_Optimization_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


> **2025-05-13 Update:**
> The A9_Performance_Optimization_Agent is now fully refactored and compliant with Agent9 protocol and architectural standards. It uses a Pydantic config model, structured logging (`A9_SharedLogger`), orchestrator-driven registry integration, and protocol entrypoints with Pydantic models. HITL is documented as not required for this agent. Card/config/code are now fully synchronized. Next steps: update/add tests, compliance, and monitoring as needed.
>
> **2025-05-15 Update:**
> The agent now includes LLM integration for advanced analysis, protocol-compliant input/output models, detailed calculation methods for various business metrics, and integration with Risk Analysis, Change Management, and LLM Service agents.

## Overview
**Purpose:** Analyze and optimize business performance through comprehensive business metrics analysis, trend identification, and continuous improvement. This agent does not generate or evaluate solutions to specific diagnosed problems; it focuses on performance measurement, benchmarking, and generic optimization recommendations.
**Agent Type:** Core Agent
**Version:** 1.0
**Template:** Follows A9_Agent_Template patterns

## Functional Requirements

### Core Capabilities
1. Business Performance Analysis
   - Analyze key business metrics
   - Track performance KPIs
   - Identify performance trends
   - Generate business insights
   - Create performance benchmarks

2. Strategic Optimization
   - Analyze business processes
   - Identify optimization opportunities
   - Create strategic recommendations
   - Track optimization impact
   - Generate ROI analysis

3. Resource Optimization
   - Analyze resource allocation
   - Optimize workforce utilization
   - Improve resource efficiency
   - Create resource optimization plans
   - Track resource utilization

4. Process Improvement
   - Analyze business processes
   - Identify bottlenecks
   - Create process optimization plans
   - Track improvement metrics
   - Generate process guidelines

5. Performance Management
   - Set performance targets
   - Track progress against goals
   - Generate performance reports
   - Create improvement plans
   - Monitor performance trends

6. LLM-Powered Analysis
   - Generate AI-powered insights
   - Provide executive summaries
   - Identify advanced optimization opportunities
   - Enhance recommendations with AI analysis
   - Support configurable prompts for specialized analysis

7. Metric Calculation Methods
   - Calculate profit margins and financial metrics
   - Analyze efficiency trends and resource utilization
   - Measure quality metrics and defect rates
   - Calculate employee productivity and engagement
   - Analyze competitive position and market metrics

### Input Requirements
1. Business Data
   - Financial metrics
   - Operational metrics
   - Customer metrics
   - Employee metrics
   - Market metrics

2. Analysis Parameters
   - Performance goals
   - Optimization targets
   - Resource constraints
   - Business requirements
   - Strategic priorities

### Output Specifications
1. Business Artifacts
   - Performance reports
   - Optimization plans
   - ROI analysis
   - Process guidelines
   - Resource allocation plans

2. Analytics
   - Business dashboards
   - Performance metrics
   - Optimization metrics
   - Trend analysis
   - Impact analysis

3. Reports
   - Performance analysis
   - Optimization progress
   - Resource utilization
   - Process improvement
   - Strategic recommendations

## Technical Requirements

### Agent Implementation
- Follow A9_Agent_Template patterns
- Use absolute imports for all dependencies
- Implement create_from_registry method
- Use standard error handling patterns
- Support async operations

### Integration Points
1. Business Systems
   - Connect to business intelligence
   - Interface with financial systems
   - Integrate with HR systems
   - Connect to customer systems
   - Interface with operational systems

2. Output Systems
   - Generate business reports
   - Create dashboards
   - Export metrics
   - Generate optimization plans

3. Agent Integrations
   - Risk Analysis Agent: For risk assessment of optimization strategies
   - Change Management Agent: For change impact analysis
   - LLM Service Agent: For AI-powered insights and recommendations

### Performance Requirements
1. Analysis Time
   - Basic analysis: < 1 hour
   - Comprehensive analysis: < 4 hours
   - Real-time monitoring: < 15 minutes

2. Processing
   - Handle multiple metrics
   - Process complex business data
   - Maintain data accuracy
   - Support concurrent analyses

### Scalability
1. Support for multiple business units
2. Handle large data volumes
3. Scale with increasing complexity
4. Support cross-departmental analysis

## Error Handling
- Use standard error classes from A9_Agent_Template
- Error types:
  - ConfigurationError: Invalid configuration
  - RegistrationError: Failed to register with registry
  - ProcessingError: Failed to process data
  - ValidationError: Invalid input data
  - ConnectionError: Connection failures

## Security Requirements
1. Data Security
   - Secure business data
   - Protect financial information
   - Secure performance metrics
   - Secure optimization plans

2. Access Control
   - Role-based access
   - Secure data sharing
   - Audit trail for changes
   - Optimization approval workflows

## Monitoring and Maintenance
1. Regular performance monitoring
2. Continuous optimization tracking
3. Periodic business reviews
4. Regular metric updates

## Success Metrics
1. Business performance improvement
2. ROI on optimization initiatives
3. Process efficiency gains
4. Resource utilization improvement
5. Strategic alignment success

## Protocol Compliance
- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning Pydantic models for type safety, validation, and interoperability.
- The agent must implement all MCP (Minimum Compliance Protocol) requirements, including compliance checks, reporting, and error handling.
- Protocol compliance is mandatory for registry integration and agent orchestration.
- The agent uses dedicated Pydantic models from `performance_optimization_models.py`:
  - `A9_Performance_Optimization_Input`: Structured input model with financial, operational, customer, employee, and market metrics
  - `A9_Performance_Optimization_Output`: Structured output model with benchmarks, trends, opportunities, recommendations, and explanations

## Change Log
- **2025-04-20:** Refocused agent as business optimizer (not solution-finder/driver); added Protocol Compliance section.

## Usage Flow
```
graph TD
    subgraph "Business Performance Analysis"
        BPA[Analyze Metrics] -->|Identify Trends| BPA2[Generate Insights]
    end

    subgraph "Strategic Optimization"
        SO[Analyze Business] -->|Find Opportunities| SO2[Create Recommendations]
    end

    subgraph "Implementation"
        I[Create Action Plan] -->|Implement Changes| I2[Monitor Impact]
    end

    subgraph "Review"
        R[Analyze Results] -->|Adjust Strategy| R2[Update Plans]
    end

    BPA2 --> SO
    SO2 --> I
    I2 --> R
    R2 --> SO
```

## Notes
- Focuses on business performance optimization
- Works with business systems for data collection
- Generates strategic optimization recommendations
- Maintains business performance baselines
- Supports continuous business improvement
- Provides optional LLM-powered analysis for deeper insights
- Includes specialized calculation methods for various business metrics
- Integrates with Risk Analysis, Change Management, and LLM Service agents
- Uses protocol-compliant Pydantic models for input/output validation



### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Performance_Optimization_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_performance_optimization_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation


<!-- End docs\prd\agents\a9_performance_optimization_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_principal_context_agent_prd.md -->

# A9 Principal Context Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Overview

> **COMPLIANCE NOTE:**
> - This agent must ONLY be invoked via the AgentRegistry and orchestrator pattern.
> - Do NOT instantiate directly; always use the async factory method `create_from_registry`.
> - **Usage Example:**
> ```python
> principal_agent = await AgentRegistry.get_agent("A9_Principal_Context_Agent")
> result = await principal_agent.set_principal_context(...)
> ```

The A9 Principal Context Agent manages principal context and relationships in business operations, providing essential context-aware functionality for other agents. It leverages the Unified Registry Access Layer to store, retrieve, and manage principal profiles as well as their relationships to business processes and KPIs.

## Purpose
- Manage principal context and relationships
- Provide context-aware recommendations
- Handle access control and permissions
- Maintain context history
- Integrate with HCM systems
- Register and retrieve principal profiles using the Unified Registry Access Layer
- Map principals to business processes and KPIs using registry relationships

## Key Features

- **HITL protocol fields** (`human_action_required`, `human_action_type`, `human_action_context`) are included in all outputs where human approval may be required. HITL enforcement is configurable and defaults to OFF unless explicitly enabled.

### 1. Principal Context Management
- Set and maintain principal context
- Fetch principal profiles from HCM systems
- Register and update principal profiles in the Principal Profile Provider
- Track context history
- Clear context when needed
- Provide responsibilities and filter criteria in business terms (English descriptions) for downstream translation
- Use registry relationships to map principals to business processes, KPIs, and data products
- Automatically default dimensional filters based on principal profile context (e.g., job description, region, business unit) for relevant data products. For example, if the principal is a Sales Manager for Northern Region, automatically add {"Region": "Northern"} to filters for Sales data queries.
- The agent must use an LLM or NLP pipeline to extract potential dimensional filters (e.g., region, business unit, product line) from the principal's job or role description text. These extracted filters should be merged with explicit profile filters and included in the output filters dictionary for downstream translation and query assembly. Example: For a role description "Sales manager for Northern Region, responsible for consumer electronics", the agent should extract and default filters such as {"Region": "Northern", "Product": "Consumer Electronics"}.
- Output structure must include:
  - `responsibilities`: List of business responsibilities (e.g., ["Revenue by Business Unit"])
  - `filters`: Dict of filter criteria (e.g., {"Region": "North America", "Product": "Consumer Electronics"})
- Example output:
```json
{
  "profile": {
    "responsibilities": ["Revenue by Business Unit", "Cost by Product Line"],
    "filters": {"Region": "North America", "Product": "Consumer Electronics"}
  }
}
```

### 2. Access Control
- Role-based access control
- Permission-based access
- Data governance integration
- Context-aware access decisions

### 3. Context Recommendations
- Generate role-based recommendations
- Provide data governance recommendations
- Prioritize recommendations
- Context-aware suggestions

#### LLM Explainability Compliance (2025-06-24)
- All context-aware recommendation descriptions are routed through the A9_LLM_Service_Agent for explainability and business-user-friendly output.
- LLM calls are protocol-compliant, orchestrator-driven, and fully async with structured event logging and error handling.
- See agent card for implementation and compliance details.

### 4. Integration
- HCM system integration
- Registry integration (A9_Agent_Template pattern, async)
- Output must be compatible with Data Governance Agent for business-to-technical translation
- Error handling
- Logging

## Technical Requirements

- Logging is standardized using `A9_SharedLogger`.
- All agent input/output is strictly Pydantic model-based and compliant with A2A/MCP protocols.

### 1. Agent Implementation
- Follow A9_Agent_Template patterns for:
  - Configuration management
  - Registry integration
  - Error handling
  - Logging
  - Async operations
- Use absolute imports for all dependencies
- Implement create_from_registry method
- Use standard error handling patterns

### 2. Dependencies
- Agent Registry
- Unified Registry Access Layer
- Registry Factory and Providers
- HCM System Connector
- Data Governance System
- Logging Framework

### 3. Error Handling
- Use standard error classes from A9_Agent_Template
- Error types:
  - ConfigurationError: Invalid configuration
  - RegistrationError: Failed to register with registry
  - ProcessingError: Failed to process data

### 4. KPI Registry Integration
- Integrates with KPI registry for entity canonicalization
- Uses KPI_REGISTRY for mapping extracted entities to standardized dimensions
- Supports entity extraction from job descriptions with standardized mapping

### 5. Implementation Details
- Uses Pydantic v2 validation with `__private_attributes__` checks for model integrity
- Implements debug logging in initialization for configuration verification
- Provides `extract_entities_from_job_description` method for standardized entity extraction
- Supports fallback handling when KPI registry mapping fails
  - ValidationError: Invalid input data
  - ConnectionError: Connection failures
- Log all context retrieval and output generation attempts, including failures

## API Specification

### Core Methods
```python
async def set_principal_context(self, principal_id: str, context_data: Dict[str, Any] = None) -> Dict[str, Any]
async def fetch_principal_profile(self, principal_id: str) -> Dict[str, Any]
async def check_access(self, item: Dict[str, Any]) -> bool
async def get_context_recommendations(self) -> List[Dict[str, Any]]
async def get_context_history(self) -> List[Dict[str, Any]]
async def clear_context(self) -> None
```

### Error Types
- ConfigurationError: Invalid configuration
- RegistrationError: Failed to register with registry
- ProcessingError: Failed to process data
- ValidationError: Invalid input data
- ConnectionError: Connection failures

## Testing Requirements

- All tests use Pydantic model instances (no dicts/mocks).
- Integration tests are orchestrator-driven and production-like.

### Unit Tests
- Registry integration
- Context management
- Access control
- Recommendations
- History management
- Error handling
- Output structure for responsibilities and filters (business terms)

### Integration Tests
- HCM system integration
- Registry integration
- Error scenarios
- Configuration validation
- Integration with Data Governance Agent (output must be translatable)

## Security Considerations
- Role-based access control
- Permission validation
- Data governance integration
- Secure context storage
- Error handling security

## Performance Considerations
- Efficient context retrieval
- Caching strategies
- Async operations
- Resource management

## Monitoring & Logging
- Operation logging
- Error tracking
- Performance metrics
- Context history

## Future Considerations
- Enhanced recommendation system
- Additional access control features
- Expanded HCM system integration
- Advanced context analysis
- Performance optimizations



### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Principal_Context_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_principal_context_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Integrates with the Unified Registry Access Layer for principal profiles
- Uses the Principal Profile Provider for registry operations
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation
- Direct enum usage (use registry providers instead)
- Hardcoded principal profile logic (use registry data)
- Initializing registry providers directly (use Registry Factory)

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation


<!-- End docs\prd\agents\a9_principal_context_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_quality_assurance_agent_prd.md -->

# A9_Quality_Assurance_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


> **2025-07-16 Update:**
> The A9_Quality_Assurance_Agent is now fully refactored and compliant with Agent9 protocol and architectural standards. It uses a Pydantic config model, structured logging (`A9_SharedLogger`), orchestrator-driven registry integration, and protocol entrypoints with Pydantic models. HITL is documented as not required for this agent. Card/config/code are now fully synchronized. Next steps: update/add tests, compliance, and monitoring as needed.





### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Quality_Assurance_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_quality_assurance_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

## Compliance & Integration Update (2025-05-01)
- This agent is now fully compliant with Agent9 async, registry, error handling, and centralized logging standards.
- Legacy patterns and non-compliant integration logic have been removed.
- All integration with this agent is now orchestrator-driven and dynamic; static workflow tests have been removed.
- Test coverage is maintained via orchestrator-based persona and workflow scenarios.

## Purpose
Ensures solution quality and compliance through comprehensive testing and validation.

## Key Capabilities
- Quality criteria definition
- Test case generation and management
- Test suite creation and execution
- Compliance verification
- Performance and security testing
- Code quality analysis and recommendations
- Integration with orchestrator workflows

## Implementation Details

### Test Case Management
- Generates test cases based on requirements
- Creates and manages test suites
- Estimates code coverage
- Maintains test configurations for unit, integration, performance, and security testing

### Test Execution
- Executes test suites and returns results
- Runs individual tests within suites
- Calculates test metrics (pass rate, coverage, duration)
- Generates test execution reports

### Code Quality Analysis
- Analyzes code quality metrics
- Calculates complexity, coupling, and other code metrics
- Identifies code quality issues
- Generates code quality recommendations

### Registry Integration
- Registers with the Agent9 registry
- Creates agent instances from registry
- Handles registration errors robustly

## Inputs
- Quality assurance input model (Pydantic)
- Solution specifications
- Compliance standards
- Test requirements
- Codebase information
- Test data
- Performance metrics

## Outputs
- Quality test plans
- Test cases and test suites
- Test execution results
- Code quality analysis reports
- Compliance reports
- Performance metrics
- Security assessment reports
- Code quality recommendations

## Integration
- Registered with Agent9 orchestrator and agent registry
- Accepts only Pydantic models for all entrypoints
- Uses shared logging and error handling patterns
- Integrates with A9_SharedLogger for consistent logging

## Test Coverage
- Covered by orchestrator-driven integration/persona scenario tests
- No legacy or static workflow tests remain

<!-- End docs\prd\agents\a9_quality_assurance_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_risk_analysis_agent_prd.md -->

# A9_Risk_Analysis_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


> **2025-07-16 Update:**
> The A9_Risk_Analysis_Agent is now fully refactored and compliant with Agent9 protocol and architectural standards. It uses a Pydantic config model, structured logging (`A9_SharedLogger`), orchestrator-driven registry integration (via `create_from_registry`), and protocol entrypoints with Pydantic models. HITL is documented as not required for this agent. Card/config/code are now fully synchronized. Next steps: update/add tests, compliance, and monitoring as needed.
> 
> **KNOWN ISSUE:** There is a bug in the `__init__` method where `validated_config` is referenced but not defined. This needs to be fixed by properly validating the config using `A9RiskAnalysisAgentConfig(**(config or {}))`.


## Overview
**Purpose:** Provide a minimal, reliable risk assessment for core business risks using simple, explainable logic.
**Agent Type:** Core Agent (MVP)
**Version:** 1.0
**Location:** src/agents/new/

## MVP Functional Requirements

### Core Capabilities (MVP)
1. Risk Type Analysis
   - Analyze **market**, **operational**, and **financial** risks only (expandable later).

2. Risk Assessment
   - Quantify risk likelihood and impact for each core risk type.
   - Generate a single risk score per type.
   - Provide a concise text summary and basic recommendation.

3. Risk Scoring
   - Use simple weighted sum or threshold-based logic.
   - No advanced analytics, heatmaps, or composite scoring in MVP.

4. Basic Reporting
   - Output results as a JSON or CSV artifact.
   - No dashboards, visualizations, or external reporting system integration in MVP.

### Input Requirements (MVP)
- Pydantic input model with fields for context, market, operational, and financial data only.
- Minimal set of analysis parameters (e.g., thresholds, weights).

### Output Specifications (MVP)
- Pydantic output model with:
  - Risk scores (one per type)
  - Status (success/error)
  - Text summary and recommendation
  - Timestamp

## Future Scope (Not in MVP)
- Support for additional risk types (regulatory, reputational, cybersecurity, supply chain, compliance).
- Advanced analytics (correlations, trends, impact analysis).
- Integration with external data sources, regulatory feeds, or threat intelligence.
- Dashboards, visualizations, and export to Tableau/PowerBI (via MCP).
- Multi-agent orchestration and hierarchical risk modeling.
- Automated reporting, documentation, and alerting.




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Risk_Analysis_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_risk_analysis_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---
**Principles:**
- Keep the MVP simple, testable, and easy to extend.
- All agent entrypoints must accept Pydantic models (no raw dicts/lists).
- Defer all complex integrations and analytics to future phases.

### Input Requirements
1. Risk Data
   - Market data
   - Operational data
   - Financial data
   - Regulatory data
   - Reputational data

2. Analysis Parameters
   - Risk thresholds
   - Analysis criteria
   - Scoring parameters
   - Reporting requirements
   - Integration settings

### Output Specifications
1. Analysis Artifacts
   - Risk assessments
   - Risk scores
   - Risk profiles
   - Risk recommendations
   - Risk documentation

2. Analytics
   - Risk dashboards
   - Risk metrics
   - Risk correlations
   - Risk trends
   - Risk impact analysis

3. Reports
   - Risk analysis reports
   - Risk scoring reports
   - Risk assessment reports
   - Risk integration reports
   - Risk documentation

## Technical Requirements

### Technical HITL enablement:
- The `hitl_enabled` field is present in the agent config for protocol consistency. For the Risk Analysis Agent, HITL is not required or implemented at this time. See config model for rationale.

### Architecture
- Simple class implementation
- Direct functionality
- No separate interfaces
- Uses AgentRegistry for registration
- Follows error handling system
- Logging:
- All logging is performed via `A9_SharedLogger`, ensuring structured, JSON-serialized logs that propagate to the Orchestrator Agent for compliance and auditability.
- Configuration:
- The agent validates its config using the `A9RiskAnalysisAgentConfig` Pydantic model, including the `hitl_enabled` field.
- All entrypoints use Pydantic models for input/output.
- No self-registration; registration is orchestrator-driven via AgentRegistry. The agent never registers itself or performs bootstrapping logic.

### Integration Points
1. Data Systems
   - Connect to data sources
   - Interface with analytics systems
   - Integrate with monitoring systems
   - Connect to reporting systems
   - Interface with compliance systems

2. Output Systems
   - Generate reports
   - Create dashboards
   - Export metrics
   - Generate documentation

### Performance Requirements
1. Analysis Time
   - Basic analysis: < 1 hour
   - Comprehensive analysis: < 4 hours
   - Real-time monitoring: < 15 minutes

2. Processing
   - Handle multiple risk factors
   - Process complex risk data
   - Maintain data accuracy
   - Support concurrent analyses

### Scalability
1. Support for multiple risk types
2. Handle large data volumes
3. Scale with increasing complexity
4. Support cross-dimensional analysis

## Security Requirements
1. Data Security
   - Secure risk data
   - Protect sensitive information
   - Secure analysis results
   - Secure documentation

2. Access Control
   - Role-based access
   - Secure data sharing
   - Audit trail for changes
   - Analysis approval workflows

## Migration Path
1. Current Implementation: src/agents/A9_Risk_Analysis_Agent.py
2. New Implementation: src/agents/new/A9_Risk_Analysis_Agent.py
3. Legacy: src/agents/legacy/A9_Risk_Analysis_Agent.py

## Monitoring and Maintenance
1. Regular risk assessments
2. Continuous data monitoring
3. Periodic analysis updates
4. Regular metric reviews

## Success Metrics
1. Risk analysis accuracy
2. Risk scoring effectiveness
3. Analysis coverage
4. Integration efficiency
5. Reporting quality

## Usage Flow
```
graph TD
    subgraph "Risk Type Analysis"
        RTA[Analyze Risks] -->|Calculate Scores| RTA2[Create Profiles]
    end

    subgraph "Risk Assessment"
        RA[Generate Scores] -->|Create Profiles| RA2[Generate Recommendations]
    end

    subgraph "Risk Scoring"
        RS[Calculate Metrics] -->|Generate Scores| RS2[Create Heatmaps]
    end

    subgraph "Risk Reporting"
        RR[Generate Reports] -->|Create Documentation| RR2[Generate Visualizations]
    end

    subgraph "Risk Integration"
        RI[Connect Systems] -->|Integrate Data| RI2[Generate Integration Reports]
    end

    RTA2 --> RA
    RA2 --> RS
    RS2 --> RR
    RR2 --> RI
    RI2 --> RTA
```

## Notes
- Focuses on comprehensive risk analysis
- Works with business systems for data collection
- Generates strategic risk recommendations
- Maintains risk analysis baselines
- Supports continuous risk improvement

<!-- End docs\prd\agents\a9_risk_analysis_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_risk_management_agent_prd.md -->

# A9_Risk_Management_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


> **2025-05-13 Update:**
> The A9_Risk_Management_Agent is now fully refactored and compliant with Agent9 protocol and architectural standards. It uses a Pydantic config model, structured logging, orchestrator-driven registry integration (via `create_from_registry`), and protocol entrypoints with Pydantic models. HITL is enforced for all actions. Card/config/code are now fully synchronized. Next steps: update/add tests, compliance, and monitoring as needed.


## Overview
**Purpose:** Manage business risks through comprehensive risk management, mitigation strategies, and continuous monitoring, leveraging detailed risk analysis
**Agent Type:** Core Agent
**Version:** 1.0
**Location:** src/agents/new/




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Risk_Management_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_risk_management_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting
- Integrates with the Unified Registry Access Layer for business processes, risk metrics, and compliance requirements

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for business processes, risk data, and compliance information
- Must use registry data for context-aware risk assessment and mitigation planning
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

## Compliance & Integration Update (2025-05-01)

- This agent is now fully compliant with Agent9 async, registry, error handling, and centralized logging standards.
- Legacy patterns and non-compliant integration logic have been removed.
- **All integration and testing for this agent is orchestrator-driven and production-like:**
  - Agent inputs and outputs are always real Pydantic model instances, never mocked or stubbed.
  - Integration tests simulate true production workflows, with the orchestrator coordinating all agent calls and data flows.
  - Static workflow tests and direct agent invocation in tests have been deprecated.
- Test coverage is maintained via orchestrator-based persona and workflow scenarios, ensuring compliance with real-world business logic and end-to-end flows.

## Functional Requirements

### Core Capabilities
1. Risk Management
   - Manage risk analysis results
   - Implement mitigation strategies
   - Monitor risk status
   - Generate management reports
   - Create action plans
   - Ensure audit logging for all risk management actions and exportable audit trails for compliance and review

2. Risk Mitigation
   - Create mitigation plans
   - Implement risk controls
   - Monitor mitigation effectiveness
   - Generate mitigation reports
   - Create action items

3. Risk Monitoring
   - Track risk indicators
   - Monitor risk trends
   - Generate monitoring reports
   - Update risk assessments
   - Create monitoring dashboards

4. Risk Communication
   - Create management reports
   - Generate risk summaries
   - Produce risk documentation
   - Create risk presentations
   - Generate risk updates

5. Risk Integration
   - Integrate with risk analysis
   - Interface with monitoring systems
   - Connect with compliance systems
   - Interface with reporting systems
   - Connect with governance systems

### Input Requirements
1. Risk Management Data
   - Risk analysis results
   - Mitigation plans
   - Monitoring data
   - Communication records
   - Integration data

2. Management Parameters
   - Management thresholds
   - Mitigation requirements
   - Monitoring schedules
   - Communication preferences
   - Integration requirements

### Output Specifications
1. Management Artifacts
   - Risk management plans
   - Mitigation strategies
   - Monitoring dashboards
   - Communication materials
   - Integration reports

#### Human-in-the-Loop (HITL) Output Fields
The output model includes HITL fields for compliance, audit, and operational flexibility:
- `human_action_required` (bool): Signals if human intervention is needed
- `human_action_type` (Optional[str]): Type of intervention (e.g., approval, review)
- `human_action_context` (Optional[dict]): Context for UI/user
- `human_action_result` (Optional[str]): Human's decision/result
- `human_action_timestamp` (Optional[str]): When action was taken

HITL is always triggered for every risk management action. Every call to the agent requires explicit human approval before proceeding. The output always includes `human_action_required = True`, `human_action_type = "approval"`, and a context reason: "All risk management actions require explicit human approval (HITL)". This ensures that every risk management operation is subject to human oversight, regardless of configuration or risk score.

2. Analytics
   - Management dashboards
   - Mitigation metrics
   - Monitoring effectiveness
   - Communication metrics
   - Integration reports

3. Reports
   - Management reports
   - Mitigation reports
   - Monitoring reports
   - Communication reports
   - Integration reports

## Technical Requirements

### Technical Implementation

### Architecture
- Simple class implementation
- Direct functionality
- No separate interfaces
- Uses AgentRegistry for registration
- Follows error handling system
- Uses logging utilities
- Implements configuration management

### Integration Points
1. Risk Systems
   - Connect to risk analysis
   - Interface with monitoring
   - Integrate with compliance
   - Connect to reporting
   - Interface with governance

2. Output Systems
   - Generate management reports
   - Create dashboards
   - Export metrics
   - Generate documentation

### Performance Requirements
1. Management Time
   - Basic management: < 1 hour
   - Comprehensive management: < 4 hours
   - Real-time monitoring: < 15 minutes

2. Processing
   - Handle multiple risks
   - Process complex mitigation
   - Maintain data accuracy
   - Support concurrent operations

### Scalability
1. Support for multiple risks
2. Handle large data volumes
3. Scale with increasing complexity
4. Support cross-system integration

## Security Requirements
1. Data Security
   - Secure management data
   - Protect sensitive information
   - Secure mitigation plans
   - Secure documentation

2. Access Control
   - Role-based access
   - Secure data sharing
   - Audit trail for changes
   - Management approval workflows

## Migration Path
1. Current Implementation: src/agents/A9_Risk_Management_Agent.py
2. New Implementation: src/agents/new/A9_Risk_Management_Agent.py
3. Legacy: src/agents/legacy/A9_Risk_Management_Agent.py

## Monitoring and Maintenance
1. Regular risk management
2. Continuous monitoring
3. Periodic review cycles
4. Regular update cycles

## Success Metrics
1. Risk management effectiveness
2. Mitigation success rate
3. Monitoring coverage
4. Communication quality
5. Integration efficiency

## Usage Flow
```
graph TD
    subgraph "Risk Management"
        RM[Manage Risks] -->|Implement Mitigation| RM2[Monitor Status]
    end

    subgraph "Mitigation"
        M[Create Plans] -->|Implement Controls| M2[Monitor Effectiveness]
    end

    subgraph "Monitoring"
        MO[Track Indicators] -->|Update Assessments| MO2[Generate Reports]
    end

    subgraph "Communication"
        C[Create Reports] -->|Generate Documentation| C2[Produce Updates]
    end

    subgraph "Integration"
        I[Integrate Systems] -->|Connect Data| I2[Generate Integration Reports]
    end

    RM2 --> M
    M2 --> MO
    MO2 --> C
    C2 --> I
    I2 --> RM
```

## Protocol Compliance
- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning Pydantic models for type safety, validation, and interoperability.
- The agent must implement all MCP (Minimum Compliance Protocol) requirements, including compliance checks, reporting, and error handling.
- Protocol compliance is mandatory for registry integration and agent orchestration.

---

## A2A Protocol Enforcement
- **All agent entrypoints (e.g., `manage_risk`) must only accept and return Pydantic models, never raw dicts or lists.**
- This ensures type safety, validation, and contract-driven communication between agents and orchestrator.
- Legacy calls and tests using dicts/lists must be refactored to use models as part of the A2A upgrade plan.
- The orchestrator is responsible for enforcing A2A protocol and routing validated model-based requests between agents.
- This is a documented migration and enforcement step for all future agent and orchestrator development.

## C-Level Executive Risk Mapping
| Risk Type           | Most Interested C-Level Executives                        |
|---------------------|----------------------------------------------------------|
| Operational         | COO, CIO, CTO                                            |
| Financial           | CFO, CEO                                                 |
| Reputational        | CEO, CMO, CHRO                                           |
| Compliance          | CCO, CFO, CHRO, CIO, CEO                                 |

- See agent and orchestrator design docs for how risk types are surfaced for persona-specific reporting and workflows.

---

## Testing
- All integration and scenario tests for A9_Risk_Management_Agent are orchestrator-driven and use real agent inputs/outputs, never mocks or stubs.
- After agent migration is complete, all integration tests will be refactored to:
  - Use the orchestrator as the only entrypoint
  - Validate end-to-end workflows for C-level personas and business scenarios
  - Assert on real outputs, with no direct agent calls in test bodies
- This ensures tests reflect production-like behavior and validate the full agent ecosystem.

## Notes
- Focuses on comprehensive risk management
- Works with risk analysis for data
- Generates strategic management recommendations
- Maintains management baselines
- Supports continuous risk improvement

<!-- End docs\prd\agents\a9_risk_management_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_situation_awareness_agent_prd.md -->

# A9_Situation_Awareness_Agent Product Requirements Document

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## 1. Overview

### MVP Channels of Engagement
For MVP, situation communications to principals will be delivered via:
- **Decision Studio UI**: Interactive situation cards in the main UI
- **Gmail notifications**: Email alerts for surfaced situations, leveraging existing Gmail integration

Other communication channels (push notifications, Slack, webhooks, etc.) are planned for future releases.

### 1.1 Purpose
The A9_Situation_Awareness_Agent continuously evaluates all relevant KPIs for a given principal, leveraging the Unified Registry Access Layer to retrieve business process, KPI, data product, and principal context information. Its primary function is to detect, record, and proactively surface significant business situations (such as trends, anomalies, and threshold breaches) and alert the principal or their delegate. The agent aggregates and references outputs from specialized agents (risk, opportunity, stakeholder, etc.) as needed, but does not duplicate their deep analysis logic.

### 1.2 Scope
This document outlines the requirements for version 1.0 of the A9_Situation_Awareness_Agent, focusing on KPI monitoring, situation detection, structured reporting, and registry integration. Deep analysis of risk, opportunity, and stakeholders is delegated to specialized agents, with this agent acting as an orchestrator and aggregator.

### 1.3 Target Audience
- System Administrators
- Business Analysts
- Data Scientists
- Decision Makers

## 2. Business Requirements

### 2.1 Business Objectives
1. Provide real-time detection and surfacing of significant business situations by evaluating all KPIs for a principal using the registry-driven architecture
2. Proactively alert principals to anomalies, trends, or threshold breaches using registry data from business processes, KPIs, data products, and principal context
3. Aggregate and contextualize outputs from specialized agents (risk, opportunity, stakeholder, etc.) without duplicating their logic (delegation only; no deep analysis performed)
4. Generate structured, actionable SituationReports for decision support (no deep analysis or recommendations generated by this agent)

### 2.2 Key Metrics
- Analysis accuracy: > 90% for standard scenarios
- Processing latency: < 2 seconds for small datasets
- System availability: 99.9% uptime
- Error rate: < 1%

## 3. Agent Requirements

### 3.1 Core Agent Interface
- Follow Agent9 agent registry interface requirements
- Implement required Unified Registry integration methods
- Use registry providers for business processes, KPIs, data products, and principal profiles
- Use standard error handling patterns
- Maintain consistent logging

### 3.2 Configuration Management
- Support default configuration values
- Validate configuration on initialization
- Maintain configuration state
- Handle agent-specific configuration

### 3.3 Registry Integration
- Support async registration with agent registry
- Use the Unified Registry Access Layer for business data access
- Implement integration with Registry Factory and providers
- Access Business Process Provider for process information
- Access KPI Provider for KPI definitions and thresholds
- Access Data Product Provider for data sources
- Access Principal Profile Provider for context personalization
- Maintain registry references
- Handle registration errors
- Support registry-based lifecycle management
- Use relative imports for registry integration
- Register with full configuration during registration

### 3.4 Error Handling
- Implement structured error handling
- Log errors appropriately
- Return consistent error responses
- Handle common error types (connection, processing, validation)

### 3.5 Logging
- Initialize agent-specific logger
- Log initialization and major operations
- Support different log levels
- Include timestamps in logs

### 3.6 Core Methods
- Implement _initialize for setup
- Implement _setup_logging for logging
- Implement _setup_error_handling for error management
- Implement register_with_registry for registry integration
- Implement create class method for instance creation

### 3.7 Error Types
- ConfigurationError: Invalid configuration
- RegistrationError: Failed to register with registry
- ProcessingError: Failed to process data
- ValidationError: Invalid input data
- ConnectionError: Connection failures

## 4. Functional Requirements

### 4.0 Registry-Driven Operation
- Must use the Unified Registry Access Layer for all business data access
- Must retrieve business processes from the Business Process Provider
- Must retrieve KPI definitions and thresholds from the KPI Provider
- Must retrieve data product information from the Data Product Provider
- Must retrieve principal profiles from the Principal Profile Provider
- Must support legacy integration for backward compatibility

### 4.1 Situation Analysis

### 4.4 User Interface & Experience (MVP UI)

#### MVP Test Run Limiting (Test Harness Only)
- For each test run, the orchestrator will limit the number of new situations surfaced to 3.
- On each subsequent run, up to 3 additional new situations may be surfaced until all are found.
- This limit is for test/development only and can be adjusted or removed for production.
- The orchestrator tracks which situations have already been surfaced using a persistent Situation List (CSV).
- The UI and CSV will only reflect the situations surfaced in each run.

#### Persistent Situation ID & Attribute Tracking
- Each detected situation is assigned a unique Situation ID (composite key of principal, KPI, geography, product group, time period, etc.).
- All situation attributes are stored with each ID in the persistent Situation List (CSV).
- The orchestrator manages status transitions (new, existing, resolved) and persists all metadata for audit and UI rendering.

- The agent must present surfaced situations as a **card** in the Decision Studio UI.
- Each card must include:
  - **Filters applied**: Principal Context (name/role), Geography, Product Group, Time Period.
  - **KPI Name** (e.g., Revenue, Churn Rate).
  - **Current Value** and **Expected/Plan Value**.
  - **Delta** (difference, color-coded, with up/down arrow).
  - **Status** (New/Existing).
  - **Recognized** (timestamp when first detected).
  - **Deep Analysis Requested** (Yes/No).
  - **Actions**:
    - [Explain This]: LLM-powered plain-language explanation
    - [Annotate]: User comments/notes
    - [Request Deep Analysis]: HITL action to initiate a deeper investigation
  - **Visualization**: Simple bar chart (Current vs. Expected), both bars sharing the same x, y axis.
- The card must be the first swipeable card in the Decision Studio situation flow.
- No complex dashboard or visualization is required for MVP; the focus is on clarity, actionability, and context.

- Context evaluation (deep risk, opportunity, and stakeholder analysis is delegated to specialized agents per Agent9 Agent Design Standards and PRD)
- Trend and anomaly detection (KPI-focused)
- Structured reporting (SituationReport)
- Aggregation of specialized agent outputs (no deep analysis logic performed by this agent)

### 4.2 Data Processing
- Support multiple data sources
- Handle structured and unstructured data
- Perform data validation
- Generate confidence scores

### 4.3 Output Generation
- Generate structured situation reports (SituationReport)
- Provide confidence metrics for anomaly/trend detection
- Include metadata and timestamps
- Output must include all metadata necessary to populate the UI card fields and support filtering, annotation, and HITL actions.
- Deep analysis recommendations are out of scope for MVP and are delegated to specialized agents.

## 5. Technical Requirements

### 5.1 Protocol Compliance Update (2025-06-24)
- All agent entrypoints are orchestrator-driven: no self-registration or direct instantiation.
- Entrypoints accept and emit only protocol-compliant (Pydantic or untyped dict/JSON, as specified) input/output models.
- All logging is async, structured, and uses `A9_SharedLogger`, with event logging awaited in all workflows.
- YAML contract context (`yaml_contract_text`) is always propagated by the orchestrator and accessed via the `context` kwarg in all protocol-compliant methods.
- Registry integration is required for all agent lifecycle events; agent must only store registry reference via `register_with_registry` and never call `register_agent` directly.
- All tests and usage examples must show orchestrator-driven instantiation and registry lifecycle.

## 6. Protocol Compliance

- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning Pydantic models for type safety, validation, and interoperability.
- The agent must implement all MCP (Minimum Compliance Protocol) requirements, including compliance checks, reporting, and error handling.
- Protocol compliance is mandatory for registry integration and agent orchestration.
- This agent is fully compliant with the Agent9 Agent Design Standards and the MVP agent card. See docs/Agent9_Agent_Design_Standards.md and src/agents/new/cards/A9_Situation_Awareness_Agent_card.md for boundaries and protocol details.

### 5.1 System Architecture
- Follow Agent9 architectural principles
- Use standard error handling patterns
- Implement consistent logging
- Support async operations

### 5.2 Implementation
- Use type hints for better maintainability
- Implement proper error handling
- Follow logging best practices
- Support configuration validation

### 5.3 Testing Requirements

#### 5.3.1 Core Tests
- Verify agent registration
- Test configuration validation
- Check error handling
- Validate logging
- Test async operations

#### 5.3.2 Test Fixtures
- Use real registry instance
- Provide valid and invalid configurations
- Test edge cases
- Verify error states

#### 5.3.3 Test Cases
- Agent initialization
- Registry integration
- Situation analysis (KPI monitoring, anomaly/trend detection, reporting)
- Error handling
- Logging
- Performance
- Ensure no deep analysis, recommendations, or impact calculation logic is present in test expectations (these are delegated to specialized agents)

## 6. Modification History

### 6.1 Current Version
- Version: 1.0
- Date: [Release Date]
- Changes: Initial implementation
- Affected Test Cases: All

### 6.2 Planned Modifications

## 7. Implementation Details

### 7.1 Notification System
- Implements email notification functionality using Gmail client integration
- Provides configurable notification templates for different situation types
- Supports customizable alert thresholds and notification frequency
- Includes email formatting for improved readability and action items

### 7.2 User Preferences
- Implements static user preferences system for notification settings
- Provides methods for retrieving and applying user-specific thresholds
- Supports customization of situation detection sensitivity
- Includes preference persistence across sessions

### 7.3 Prompt Generation
- Implements deep dive prompt generation method for detailed situation analysis
- Provides structured templates for consistent prompt formatting
- Supports context-aware prompt customization based on situation type
- Includes metadata enrichment for improved analysis quality

### 7.4 Agent Integration
- Implements explicit agent integration workflow in run_situation_check
- Provides standardized interfaces for inter-agent communication
- Supports orchestrator-driven agent collaboration
- Includes context propagation between agent interactions

### 7.5 Context Validation
- Implements explicit context validation logic for input verification
- Provides robust error handling for invalid or incomplete context
- Supports graceful degradation when context is partially available
- Includes logging of context validation results for debugging

#### 6.2.1 Registry Integration Improvements
- Purpose: Enhance registry integration and module structure
- Impact Analysis:
  - Input Changes: None
  - Output Changes: None
  - Data Flow Changes: Changed registry registration flow to use relative imports and simplify agent creation
- Test Impact:
  - Affected Test Cases: Registry integration tests, agent creation tests
  - New Test Cases Needed: None
  - Test Data Changes: None
- Implementation Plan:
  1. Update imports to use relative paths instead of absolute paths
  2. Simplify agent creation process by removing redundant creation methods
  3. Enhance registry registration to include full configuration
  4. Update test cases to reflect new structure
- Documentation Updates:
  - [ ] Update registry integration documentation
  - [ ] Update agent creation documentation
  - [ ] Update error handling documentation
  - [ ] Update usage examples
  - [ ] Update configuration documentation

#### 6.2.2 Error Handling Improvements
- Purpose: Enhance error handling and logging
- Impact Analysis:
  - Input Changes: None
  - Output Changes: Enhanced error responses
  - Data Flow Changes: Improved error propagation
- Test Impact:
  - Affected Test Cases: Error handling tests
  - New Test Cases Needed: Additional error scenarios
  - Test Data Changes: None
- Implementation Plan:
  1. Review and update error types
  2. Enhance error propagation
  3. Improve logging
  4. Add new test cases
- Documentation Updates:
  - [ ] Update error handling documentation
  - [ ] Update logging documentation
  - [ ] Update test documentation

## 7. Acceptance Criteria

### 7.0 Protocol Compliance (2025-06-24)
- All surfaced situations show context derived from YAML contract and principal context.
- All actions (explain, annotate, request deep analysis) trigger orchestrator-driven workflows.
- No legacy or direct agent instantiation patterns allowed in UI or backend.
- All tests invoke the agent via the orchestrator and assert on event logs using public APIs.
- Documentation and usage examples reflect orchestrator-driven instantiation and registry lifecycle.

### 7.1 Functional

### 7.3 UI/UX Acceptance Criteria (MVP UI)
- [ ] Card displays all required fields (filters, KPI, values, delta, status, timestamp, deep analysis status).
- [ ] Bar chart accurately visualizes Current vs. Expected KPI on the same axes.
- [ ] [Request Deep Analysis] button is present and triggers a HITL workflow.
- [ ] [Explain This] and [Annotate] actions are available and functional.
- [ ] Card layout is clear, compact, and actionable, with context filters always visible.
- [ ] No extraneous data or visual clutter is present in the MVP.

- Successful registry integration
- Accurate situation analysis
- Proper error handling
- Complete documentation

### 7.2 Non-Functional
- Performance meets requirements
- Error handling is robust
- Logging is comprehensive
- Documentation is complete

## 8. Maintenance and Support

### 8.1 Maintenance
- Regular updates
- Security patches
- Performance optimization
- Documentation updates

### 8.2 Support
- User documentation
- API documentation
- Troubleshooting guide
- Support channels




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Situation_Awareness_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_situation_awareness_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Integrates with the Unified Registry Access Layer for business processes, KPIs, and principal profiles
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for business processes, KPIs, principal profiles, and situation detection rules
- Must use registry data for context-aware situation detection and analysis
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation
- Direct enum usage (use registry providers instead)
- Hardcoded business logic (use registry data)

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## References

- [Persona Debate Meeting Minutes – Situation Awareness MVP (2025-04-20)](../meetings/a9_situation_awareness_mvp_persona_debate_2025-04-20.md)
- [Situation Awareness Agent Wireframe – Decision Studio Card UI](/docs/prd/agents/a9_situation_awareness_agent_wireframe.md)

## Change Log

- **2025-04-20:** Updated MVP requirements and action items based on Windsurf persona debate. See [meeting minutes](../meetings/a9_situation_awareness_mvp_persona_debate_2025-04-20.md) for details.

- User documentation
- API documentation
- Troubleshooting guide
- Support channels

<!-- End docs\prd\agents\a9_situation_awareness_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_solution_architect_agent_prd.md -->

# A9_Solution_Architect_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Overview
**Purpose:** Manage solution architecture and technical validation through architecture patterns and technology stack management
**Agent Type:** Innovation Team
**Version:** 1.0

## Functional Requirements

### Core Capabilities
1. Architecture Management
   - Create architecture patterns
   - Track pattern usage
   - Generate pattern metrics
   - Handle pattern updates
   - Create pattern documentation

2. Technology Stack Management
   - Manage technology stack
   - Track stack usage
   - Generate stack metrics
   - Handle stack updates
   - Create stack documentation

3. Technical Validation
   - Validate solutions
   - Track validation status
   - Generate validation reports
   - Handle validation failures
   - Create validation metrics

4. Pattern Integration
   - Integrate patterns
   - Track integration status
   - Generate integration metrics
   - Handle integration conflicts
   - Create integration documentation

5. Stack Optimization
   - Optimize technology stack
   - Track optimization progress
   - Generate optimization metrics
   - Handle optimization challenges
   - Create optimization plans

### Input Requirements
1. Architecture Data
   - Architecture patterns
   - Technology stack
   - Validation requirements
   - Integration needs
   - Optimization criteria

2. Context Information
   - Solution requirements
   - Technical constraints
   - Integration requirements
   - Optimization goals
   - Validation criteria

### Output Specifications
1. Architecture Artifacts
   - Architecture patterns
   - Technology stack
   - Validation reports
   - Integration plans
   - Optimization plans

2. Analytics
   - Pattern metrics
   - Stack metrics
   - Validation metrics
   - Integration metrics
   - Optimization metrics

3. Reports
   - Architecture status
   - Stack usage
   - Validation results
   - Integration status
   - Optimization progress

## Technical Requirements

### Integration Points
1. Architecture Systems
   - Connect to pattern management
   - Interface with stack management
   - Integrate with validation
   - Connect to integration systems

2. Output Systems
   - Generate reports
   - Create logs
   - Export metrics
   - Generate documentation

### Performance Requirements
1. Architecture Management
   - Pattern updates: < 1 second
   - Stack updates: < 500ms
   - Validation: < 100ms

2. System Requirements
   - Handle multiple patterns
   - Process real-time updates
   - Maintain data consistency

### Scalability
1. Support for multiple patterns
2. Handle large stacks
3. Scale with increasing complexity

## Security Requirements
1. Architecture Security
   - Secure pattern data
   - Protect stack information
   - Secure validation results

2. Access Control
   - Role-based access
   - Secure data sharing
   - Audit trail for changes
   - Architecture approval workflows

## Monitoring and Maintenance
1. Regular pattern updates
2. Continuous stack monitoring
3. Periodic validation
4. Regular optimization

## Success Metrics
1. Pattern accuracy
2. Stack consistency
3. Validation effectiveness
4. Integration quality
5. Optimization efficiency



### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Solution_Architect_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_solution_architect_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation


<!-- End docs\prd\agents\a9_solution_architect_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_solution_finder_agent_prd.md -->

# A9_Solution_Finder_Agent PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


> **2025-05-13 Update:**
> The A9_Solution_Finder_Agent is now fully refactored and compliant with Agent9 protocol and architectural standards. It uses a Pydantic config model, structured logging, orchestrator-driven registry integration (via `create_from_registry`), and protocol entrypoints with Pydantic models. HITL is supported for all actions. Card/config/code are now fully synchronized. Next steps: update/add tests, compliance, and monitoring as needed.


## Overview
**Purpose:** Systematically generate, evaluate, and recommend solutions to specific problems or diagnoses—especially those surfaced by the Deep Analysis Agent. This agent acts as a driver for problem-solving workflows, scenario evaluation, and decision support, with a human-in-the-loop and iterative refinement. Leverages the Unified Registry Access Layer for business processes, KPIs, and data products.




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Solution_Finder_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_solution_finder_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Integrates with the Unified Registry Access Layer for business processes, KPIs, and data products
- Uses Registry Factory for provider initialization and configuration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation
- Direct enum usage (use registry providers instead)
- Hardcoded business logic or KPI definitions (use registry data)
- Initializing registry providers directly (use Registry Factory)
- Bypassing the Unified Registry Access Layer
- Duplicating registry access logic
- Caching registry data locally instead of using the registry providers

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## LLM Integration Prioritization (2025-05-07)

Agent9 agents are prioritized for LLM integration as follows (see BACKLOG_REFOCUS.md for full table):

- **A9_Innovation_Agent:** Highest benefit from LLMs for idea generation, creative synthesis, and debate simulation.
- **A9_Solution_Finder_Agent:** Major benefit for generating solution options, trade-off analysis, and rationale writing.
- **A9_Deep_Analysis_Agent:** Major benefit for extracting insights, summarizing findings, and generating recommendations.
- **A9_Market_Research_Agent:** High benefit for synthesizing market trends and generating competitive intelligence.
- **A9_Stakeholder_Engagement/Analysis, A9_UI_Design_Agent:** Moderate benefit for summarization, engagement planning, and UI text/design suggestions.
- **Deterministic/Core Agents:** LLMs used only for explainability, not core logic (audit, compliance, access control, etc.).

**Reference:** See "Agents Prioritized for LLM Integration" in BACKLOG_REFOCUS.md for rationale and full table.

---
**Purpose:** Systematically generate, evaluate, and recommend solutions to specific problems or diagnoses—especially those surfaced by the Deep Analysis Agent. This agent acts as a driver for problem-solving workflows, scenario evaluation, and decision support, with a human-in-the-loop and iterative refinement. Utilizes the Unified Registry Access Layer to access business processes, KPIs, and data products for context-aware solution generation.
**Agent Type:** Core/Driver Agent
**Version:** 1.0

## MVP Functional Requirements

### Scientific Method, Evidence-Based Validation & Disciplined Reasoning (Updated 2025-05-06)
- The agent must follow a disciplined, unbiased process: hypothesis formation, evidence gathering, and validation before escalating to human input.
- All solution recommendations must be supported by internal or external evidence of effectiveness. If such evidence is lacking, the agent must propose a pilot/experiment and a plan for gathering validation data before adoption.
- LLMs are used to search, summarize, and present supporting evidence or to draft pilot protocols and study plans.
- Escalation to human (via NLP agent prompt or UI) only occurs when analytic/scientific methods are exhausted or when human wisdom, sensory ability, or creativity is likely to yield a breakthrough.
- If no proven or pilotable solutions are available, and human-in-the-loop brainstorming fails to produce actionable ideas, the Solution Finder Agent will escalate the problem to the A9_Innovation_Driver_Agent for creative, out-of-the-box ideation using innovation frameworks and LLM-supported approaches.
- All reasoning steps, failed attempts, escalation triggers, and innovation escalations must be logged for transparency and learning.

### Core Capabilities (MVP)
1. **Problem Intake**
   - Accepts a problem statement, diagnosis, or root cause (from Deep Analysis Agent or user)
   - Validates and structures the problem context using Pydantic models
   - Enriches problem context with relevant business processes, KPIs, and data products from the registry
   - Integrates directly with Deep Analysis Agent for seamless problem intake
   - Supports direct use of Deep Analysis Agent output (A9_Deep_Analysis_Output) as structured input, including insights and recommendations
   - Integrates directly with Market Analysis Agent for tailored market research and intelligence as structured input (A9_Market_Analysis_Output), including LLM-powered insights when available.
   - Market research can be requested and injected as structured Pydantic models into the solution finding process to enhance relevance and decision quality.
2. **Solution Generation**
   - Systematically generates multiple solution options/alternatives for the problem
   - Supports both automated and user-guided brainstorming
   - Allows user to supply constraints, preferences, and iterative feedback
   - Uses registry data to ensure solutions are relevant to business context and constraints
3. **Solution Evaluation**
    - Evaluates and ranks solutions based on criteria provided by Deep Analysis Agent or user (criteria are context-dependent)
    - Supports trade-off analysis and scenario comparison
    - Leverages registry data (KPIs, business processes) for evaluation criteria and impact assessment
    - Creates a trade-off matrix with a recommendation for conflicting objectives
    - **Generates a Trade-Off Analysis Deliverable:** For every major decision point (e.g., speed vs. rigor, cost vs. benefit), the agent produces a structured deliverable comparing all viable options. This includes: option descriptions, expected time to result, confidence level, risks, business impact, and required resources/cost. The deliverable is presented to the principal for review and decision, and all decisions/rationales are logged for audit and learning.
4. **Recommendation & Human-in-the-Loop**
   - Recommends the best solution(s) with supporting rationale and confidence score
   - Provides a clear, actionable summary for decision makers
   - Requires human review, feedback, and iterative solution refinement before finalizing recommendations
   - Supports user-guided cycles: each solution round must allow user feedback and resubmission, enabling iterative improvement until the user is satisfied
   - **Escalation to human (via NLP agent prompt or UI) is only triggered after all analytic/scientific methods are exhausted, in accordance with the scientific method requirement above.**
5. **Audit Logging**
   - Logs all problem statements, solution options, evaluation criteria, and recommendations
   - Logs all solution generation and evaluation steps in detail, including intermediate iterations, user feedback, and rationale for each decision
   - Logs all market research requests, responses, and usage in solution generation for auditability and compliance (including LLM-powered insights).
   - Supports exportable audit trails for compliance and review

### Input Requirements (MVP)
- Pydantic input model for problem statement/diagnosis and context
- Optional: user-supplied constraints, preferences, and feedback

### Output Specifications (MVP)
- Pydantic output model with:
  - Ranked solution options
  - Recommendation and rationale
  - Confidence scores
  - Trade-off matrix (if applicable)
  - **Trade-Off Analysis Deliverable:**
    - Structured comparison of options (table/matrix format)
    - Narrative summary of tradeoffs, pros/cons, and recommendations
    - Explicit prompt for human-in-the-loop decision
    - Captures principal's choice and rationale for audit
  - Audit log reference

---

### Human-in-the-Loop (HITL) Event Design

**Pattern:**
- Only **one HITL event is emitted per solution-finding cycle** (not per solution option).
- This event prompts the principal (human) to review all proposed solutions and either approve the agent's recommended solution or select a different option for downstream processing.
- The HITL event and context are included in the output model fields:
  - `human_action_required: bool`
  - `human_action_type: str` (e.g., "approval")
  - `human_action_context: dict` (includes summary of all options, agent recommendation, rationale, and any special instructions for the principal)
  - `human_action_result: Optional[str]` (principal's decision, set by orchestrator/UI)
  - `human_action_timestamp: Optional[str]`

**Workflow:**
1. Agent generates and ranks solution options, recommends one, and emits a single HITL event in the output.
2. Orchestrator/UI presents all options and the agent's recommendation to the principal.
3. Principal reviews, approves, or overrides the recommendation, and selects one solution to proceed.
4. Principal's decision is recorded in `human_action_result` and `human_action_timestamp`.
5. The approved solution is then passed to the next agent in the workflow.

**Rationale:**
- Simplifies workflow and user experience.
- Matches real-world decision-making (one principal approval per decision cycle).
- Ensures auditability: all HITL events, triggers, and outcomes are logged for compliance and review.
- Avoids unnecessary complexity and approval fatigue from per-option HITL events.

**Audit & Compliance:**
- All HITL triggers and actions are logged in the agent's audit log.
- HITL fields are surfaced to orchestrator/UI for workflow and compliance management.
- Ensures the agent meets regulatory and operational requirements for human oversight.

---

## Technical Requirements
- Modular, maintainable architecture
- Registry integration and async operations
- Secure configuration and error handling
- Seamless integration with Deep Analysis Agent (accepts A9_Deep_Analysis_Output as input)
- Seamless integration with Market Analysis Agent (accepts A9_Market_Analysis_Output as input, including LLM-powered market research)
- Human-in-the-loop and iterative refinement are required; agent must support user feedback and resubmission cycles
- All solution generation and evaluation steps must be logged for auditability and compliance
- All market research usage (including LLM-sourced insights) must include source attribution and be logged for compliance.

---

### Integration-First Testing & Interface Refactor (2025-05-11)
- As agent interdependencies increase, brittle unit tests are deprioritized in favor of integration and compliance testing.
- All agents are being refactored for clear boundaries and minimal, well-documented interfaces.
- Testing now prioritizes real agent-to-agent workflows, protocol adherence, and end-to-end compliance.
- Excessive mocking is minimized—only true external dependencies (e.g., APIs, databases) are mocked.
- All agent entrypoints and outputs must be designed for seamless integration, not just isolated logic.
- Regression and compliance suites are used to ensure standards across the ecosystem.
- This approach is now the standard for all Solution Finder Agent compliance and regression testing, and will be applied to all future agent migrations.

---

## Protocol Compliance
- All agent entrypoints must strictly comply with the A2A protocol, accepting and returning Pydantic models for type safety, validation, and interoperability.
- The agent must implement all MCP (Minimum Compliance Protocol) requirements, including compliance checks, reporting, and error handling.
- Protocol compliance is mandatory for registry integration and agent orchestration.

## Problem Mindmap & Agent Network Diagram UI (MVP Requirement, 2025-05-05)
- Implement interactive UI components to visualize problem decomposition as a mindmap and show dynamic agent recruitment/orchestration as a network diagram.
- Rationale: Windsurf Persona Debate consensus (2025-05-05) found these features critical for demo, onboarding, explainability, and market differentiation, provided implementation is low-risk and does not delay core agent delivery.
- Action Items:
  - Use existing visualization libraries (e.g., react-flow, Cytoscape.js) for rapid prototyping.
  - Integrate with orchestrator logs/registry for real-time and historical visualization.
  - Ship a minimal, non-blocking version for MVP (static or read-only acceptable if needed).

> **Tracking:** See BACKLOG_REFOCUS.md for backlog status and actionable items.

## Future Scope (Not in MVP)
- Automated scenario simulation and optimization
- Integration with external solution libraries or expert systems
- Real-time collaborative problem-solving
- User-customizable evaluation criteria and workflows
- Agent learning from solution outcomes and user feedback

## Change Log
- **2025-05-12:** Implemented full HITL escalation logic in Solution Finder Agent; output model and logging now strictly protocol-compliant. Documentation and checklist updated; unit tests cover HITL scenarios and protocol compliance.
- **2025-04-20:** Updated PRD to reflect MVP debate outcomes and Product Owner answers (integration with Deep Analysis, mixed solution generation, human-in-the-loop, trade-off matrix, auditability, strict A2A/MCP compliance).

---

## Implementation Details

### Fallback Logger Implementation
- Implements a fallback logger mechanism when the standard logging system is unavailable
- Provides consistent logging interface across all environments
- Ensures critical events are always captured even in degraded operation modes
- Supports seamless transition between logging systems based on availability

### Debug Logging Approach
- Implements comprehensive debug statements for option and tradeoff generation
- Uses standardized `[DEBUG]` prefix for all debug-level logging
- Provides detailed visibility into solution generation and evaluation process
- Supports troubleshooting and performance optimization

### Test Stability Features
- Ensures at least two dummy solution options are always generated for test stability
- Implements fallback mechanisms to guarantee consistent test results
- Provides deterministic behavior for automated testing scenarios
- Supports both unit and integration testing patterns

### Configuration Update Method
- Implements a dynamic configuration update method
- Supports runtime adjustment of agent parameters
- Validates configuration changes against protocol requirements
- Ensures configuration consistency across agent lifecycle

### Deep Analysis Recommendation Extraction
- Implements specialized extraction logic for Deep Analysis Agent recommendations
- Transforms unstructured analysis into actionable solution components
- Supports direct integration with Deep Analysis Agent output models
- Provides confidence scoring for extracted recommendations

### Audit Logging Implementation
- Implements comprehensive audit logging for all agent operations
- Records all solution generation steps, evaluations, and recommendations
- Provides traceability for regulatory compliance
- Supports detailed review of decision-making process

### Fallback Rationale Generation
- Implements fallback mechanisms for rationale generation when dependencies are unavailable
- Ensures solution options always include supporting rationale
- Provides consistent user experience even in degraded operation modes
- Supports graceful degradation of functionality

### Tradeoff Matrix Population
- Implements robust tradeoff matrix population logic
- Handles missing dependencies through fallback mechanisms
- Ensures complete matrix generation for all solution comparisons
- Supports consistent decision support even with partial information

<!-- End docs\prd\agents\a9_solution_finder_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_stakeholder_analysis_agent_prd.md -->

# Stakeholder Analysis Agent PRD

## Overview

The Stakeholder Analysis Agent provides comprehensive stakeholder analysis capabilities for Agent9. It identifies, categorizes, and analyzes stakeholders across different types (customers, employees, partners, regulators, investors, suppliers) to support business change initiatives and decision-making processes.

## Features

### Core Functionality

1. **Stakeholder Identification and Categorization**
   - Categorize stakeholders by type (customer, employee, partner, regulator, investor, supplier)
   - Assign impact levels (low, medium, high, critical)
   - Track stakeholder size and domain ownership

2. **Stakeholder Analysis**
   - Analyze stakeholders by type with specialized analysis for each category
   - Generate comprehensive stakeholder maps
   - Calculate stakeholder impact scores
   - Provide actionable recommendations based on analysis

3. **Engagement Workflow Management**
   - Start guided engagement workflows for stakeholders
   - Support workflow templates for data governance and process ownership
   - Track workflow steps and progress
   - Record engagement outcomes

4. **Domain and Owner Mapping**
   - Map stakeholders to data domains and process domains
   - Track domain ownership
   - Support cross-domain stakeholder analysis

### Integration Points

1. **Event-Driven Handoff**
   - Emit StakeholderAnalysisCompleted events for downstream agents
   - Support event-based integration with Stakeholder Engagement Agent

2. **Registry Integration**
   - Orchestrator-controlled instantiation via AgentRegistry
   - Support for agent_id tracking and registry-based lookup

3. **Context Propagation**
   - Support for situation context from upstream agents
   - Maintain context throughout analysis process

## Technical Requirements

### Data Models

1. **Input Models**
   - StakeholderAnalysisInput: Contains stakeholder segments by type and situation context

2. **Output Models**
   - StakeholderAnalysisResult: Contains analyzed stakeholders, score, and recommendations
   - StakeholderAnalysisCompletedEvent: Event model for downstream agent handoff

3. **Domain Models**
   - StakeholderType: Enumeration of stakeholder types
   - StakeholderImpact: Enumeration of impact levels
   - StakeholderSegment: Core stakeholder data model
   - EngagementWorkflowStep: Model for workflow steps
   - EngagementWorkflowState: Model for workflow state tracking
   - EngagementOutcome: Model for workflow outcomes
   - StakeholderDomainOwnerMap: Model for domain-owner mapping

### Configuration

1. **Agent Configuration**
   - Validated via A9StakeholderAnalysisAgentConfig Pydantic model
   - Support for debug and logging configuration

### Error Handling

1. **Logging**
   - Comprehensive error logging via A9_SharedLogger
   - Context-aware error reporting

2. **Validation**
   - Input validation using Pydantic models
   - Configuration validation at instantiation

## User Experience

### Inputs

1. **Stakeholder Data**
   - Support for structured stakeholder data input
   - Support for situation context from upstream agents

### Outputs

1. **Analysis Results**
   - Stakeholder map with categorization and impact assessment
   - Numerical score representing overall stakeholder landscape
   - Actionable recommendations for stakeholder engagement

2. **Workflow Management**
   - Workflow creation and tracking
   - Step progression and outcome recording

## Implementation Details

### Agent Creation

The agent follows the orchestrator-controlled instantiation pattern:

```python
stakeholder_agent = await AgentRegistry.get_agent("A9_Stakeholder_Analysis_Agent")
result = await stakeholder_agent.analyze(input_data, change_context)
```

### Analysis Process

1. Stakeholder data is provided via StakeholderAnalysisInput
2. Agent analyzes each stakeholder type present in the input
3. Results are aggregated into a comprehensive stakeholder map
4. Score is calculated based on stakeholder impact and other factors
5. Recommendations are generated based on analysis
6. StakeholderAnalysisCompletedEvent is emitted for downstream agents

### Workflow Management

1. Workflows are created for stakeholders with domain ownership
2. Workflow steps are tracked and progressed
3. Outcomes are recorded for auditability




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Stakeholder_Analysis_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_stakeholder_analysis_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting
- Integrates with the Unified Registry Access Layer for stakeholder data, organization structure, and business processes

### Registry Architecture Integration
- Must use the Registry Factory to initialize and access all registry providers
- Must configure and use appropriate registry providers for stakeholder information, organizational structure, and business processes
- Must use registry data for context-aware stakeholder analysis and engagement planning
- Must NOT cache registry data locally; instead, always access the latest data through the Unified Registry Access Layer
- Must support backward compatibility with legacy code
- Must delegate registry operations to the appropriate providers

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

## Future Enhancements

1. **Advanced Analytics**
   - Sentiment analysis for stakeholder feedback
   - Predictive modeling for stakeholder behavior

2. **Integration Enhancements**
   - Direct integration with external stakeholder management systems
   - Real-time stakeholder feedback collection

3. **Visualization**
   - Stakeholder map visualization
   - Impact/influence matrix generation

## Compliance and Security

1. **Data Protection**
   - Stakeholder data is handled according to data governance policies
   - Sensitive stakeholder information is properly secured

2. **Auditability**
   - All stakeholder analysis actions are logged for audit purposes
   - Workflow progression and outcomes are tracked

## Dependencies

1. **Agent Registry**
   - Required for orchestrator-controlled instantiation

2. **Shared Logging Utility**
   - Used for consistent error logging and reporting

3. **Configuration Models**
   - A9StakeholderAnalysisAgentConfig for configuration validation

<!-- End docs\prd\agents\a9_stakeholder_analysis_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_stakeholder_engagement_agent_prd.md -->

# Stakeholder Engagement Agent PRD

## Overview

The Stakeholder Engagement Agent provides comprehensive stakeholder engagement capabilities for Agent9. It analyzes stakeholder engagement levels, generates improvement recommendations, and manages engagement workflows to ensure effective stakeholder management throughout business change initiatives.

## Features

### Core Functionality

1. **Engagement Analysis**
   - Analyze stakeholder engagement levels across multiple dimensions
   - Calculate engagement metrics (communication, satisfaction, trust, influence)
   - Generate comprehensive engagement assessment

2. **Recommendation Generation**
   - Provide actionable recommendations for improving stakeholder engagement
   - Prioritize recommendations based on engagement scores
   - Generate owner-specific and domain-specific recommendations

3. **Event-Driven Integration**
   - Process StakeholderAnalysisCompletedEvent from Stakeholder Analysis Agent
   - Convert stakeholder maps into engagement-focused data models
   - Trigger engagement analysis based on completed stakeholder analysis

4. **Audit and Tracking**
   - Maintain audit log of engagement actions
   - Track engagement metrics over time
   - Provide engagement history for compliance and reporting

### Integration Points

1. **Event-Based Workflow**
   - Receive events from Stakeholder Analysis Agent
   - Support event-based integration with downstream agents

2. **Registry Integration**
   - Orchestrator-controlled instantiation via AgentRegistry
   - Support for agent_id tracking and registry-based lookup

3. **Context Propagation**
   - Maintain context throughout engagement process
   - Support for change context from upstream agents

## Technical Requirements

### Data Models

1. **Input Models**
   - A9_Stakeholder_Engagement_Input: Contains stakeholder information and engagement data

2. **Output Models**
   - A9_Stakeholder_Engagement_Output: Contains engagement metrics, recommendations, and audit information
   - EngagementMetrics: Model for tracking engagement metrics

3. **Event Models**
   - StakeholderAnalysisCompletedEvent: Event received from Stakeholder Analysis Agent

### Configuration

1. **Agent Configuration**
   - Validated via A9StakeholderEngagementAgentConfig Pydantic model
   - Support for debug and logging configuration

### Error Handling

1. **Logging**
   - Comprehensive error logging via A9_SharedLogger
   - Context-aware error reporting

2. **Validation**
   - Input validation using Pydantic models
   - Configuration validation at instantiation

## User Experience

### Inputs

1. **Stakeholder Data**
   - Support for structured stakeholder data input
   - Support for engagement metrics input
   - Support for events from Stakeholder Analysis Agent

### Outputs

1. **Engagement Analysis**
   - Engagement metrics across multiple dimensions
   - Overall engagement assessment
   - Stakeholder-specific engagement insights

2. **Recommendations**
   - Prioritized engagement recommendations
   - Owner-specific and domain-specific recommendations
   - Critical alerts for stakeholders with low engagement scores

## Implementation Details

### Agent Creation

The agent follows the orchestrator-controlled instantiation pattern:

```python
engagement_agent = await AgentRegistry.get_agent("A9_Stakeholder_Engagement_Agent")
result = engagement_agent.analyze_stakeholder_engagement(input_data)
```

### Analysis Process

1. Stakeholder data is provided via A9_Stakeholder_Engagement_Input or received from Stakeholder Analysis Agent
2. Agent maps stakeholders by domain and owner for reporting
3. Engagement scores are calculated across multiple dimensions
4. Recommendations are generated based on engagement scores and stakeholder mapping
5. Results are returned as A9_Stakeholder_Engagement_Output

### Recommendation Generation

1. Generate workflow recommendations for owners
2. Identify consensus gathering needs for domains with multiple owners
3. Flag critical stakeholders with low satisfaction or trust scores
4. Provide metrics summary and critical alerts
5. Include governance integration recommendations




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Stakeholder_Engagement_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_stakeholder_engagement_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

## Future Enhancements

1. **Advanced Analytics**
   - Sentiment analysis for stakeholder feedback
   - Predictive modeling for engagement trends

2. **Integration Enhancements**
   - Direct integration with communication platforms
   - Real-time engagement monitoring

3. **Visualization**
   - Engagement dashboard visualization
   - Trend analysis and reporting

## Compliance and Security

1. **Data Protection**
   - Stakeholder engagement data is handled according to data governance policies
   - Sensitive stakeholder information is properly secured

2. **Auditability**
   - All engagement actions are logged for audit purposes
   - Engagement metrics are tracked over time for compliance reporting

## Dependencies

1. **Agent Registry**
   - Required for orchestrator-controlled instantiation

2. **Shared Logging Utility**
   - Used for consistent error logging and reporting

3. **Configuration Models**
   - A9StakeholderEngagementAgentConfig for configuration validation

4. **Stakeholder Analysis Agent**
   - Source of stakeholder analysis data via events

<!-- End docs\prd\agents\a9_stakeholder_engagement_agent_prd.md -->

---
<!-- Begin docs\prd\agents\a9_ui_design_agent_prd.md -->

# UI Design Agent PRD

## Overview

The UI Design Agent provides user interface design and UX capabilities for Agent9. It enables the design, analysis, and optimization of UI components and layouts to ensure consistent, accessible, and user-friendly interfaces across Agent9 applications.

## Features

### Core Functionality

1. **Component Design**
   - Design UI components based on configuration parameters
   - Generate component properties, styles, and accessibility attributes
   - Support various component types and variants

2. **Design Analysis**
   - Analyze UI designs for usability, accessibility, and consistency
   - Calculate design metrics (complexity, consistency, accessibility, usability)
   - Identify design issues and provide recommendations

3. **Layout Optimization**
   - Optimize UI layouts for different screen sizes and devices
   - Provide grid, responsive, and component layout optimization
   - Support customizable layout configurations

### Integration Points

1. **Registry Integration**
   - Orchestrator-controlled instantiation via AgentRegistry
   - Support for agent_id tracking and registry-based lookup

2. **Context Propagation**
   - Maintain design context throughout the design process
   - Support for configuration-based design customization

## Technical Requirements

### Data Models

1. **Input Models**
   - Component configuration dictionaries
   - Layout configuration dictionaries
   - Design analysis input dictionaries

2. **Output Models**
   - Component design dictionaries
   - Layout optimization dictionaries
   - Design analysis result dictionaries

### Configuration

1. **Agent Configuration**
   - Support for debug and logging configuration
   - Component and layout default configurations

### Error Handling

1. **Logging**
   - Comprehensive error logging via A9_SharedLogger
   - Context-aware error reporting

## User Experience

### Inputs

1. **Component Configuration**
   - Component type, name, and variant
   - Style and property preferences
   - Accessibility requirements

2. **Layout Configuration**
   - Grid configuration (columns, gutters, breakpoints)
   - Responsive design preferences
   - Component spacing and alignment

### Outputs

1. **Component Design**
   - Component properties (variant, size, disabled state)
   - Component styles (color, font size, padding, margin)
   - Accessibility attributes (aria labels, roles)

2. **Layout Optimization**
   - Optimized grid settings
   - Responsive design configurations
   - Component layout recommendations

3. **Design Analysis**
   - Design metrics (complexity, consistency, accessibility, usability)
   - Identified design issues with severity and description
   - Design recommendations with priority and effort estimates

## Implementation Details

### Agent Creation

The agent follows the orchestrator-controlled instantiation pattern:

```python
ui_design_agent = await AgentRegistry.get_agent("A9_UI_Design_Agent")
component = ui_design_agent.design_component(component_config)
```

### Component Design Process

1. Component configuration is provided
2. Agent generates component properties based on configuration
3. Agent generates component styles based on configuration
4. Agent generates accessibility attributes based on configuration
5. Complete component design is returned

### Design Analysis Process

1. Design is provided for analysis
2. Agent calculates design metrics (complexity, consistency, accessibility, usability)
3. Agent identifies design issues with severity and component information
4. Agent generates design recommendations with priority and effort estimates
5. Complete analysis results are returned

### Layout Optimization Process

1. Layout configuration is provided
2. Agent optimizes grid layout (columns, gutters, breakpoints)
3. Agent optimizes responsive design (breakpoints, media queries, flex direction)
4. Agent optimizes component layout (spacing, alignment, nesting)
5. Complete optimized layout is returned




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_UI_Design_Agent_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_ui_design_agent_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

## Future Enhancements

1. **Advanced Design Features**
   - AI-driven design generation
   - Design system integration
   - Theme customization

2. **Integration Enhancements**
   - Integration with design tools (Figma, Sketch)
   - Code generation for UI frameworks (React, Angular, Vue)
   - Design token management

3. **Visualization**
   - Design preview generation
   - Interactive design exploration
   - A/B testing support

## Compliance and Security

1. **Accessibility Compliance**
   - WCAG 2.1 compliance checking
   - Accessibility report generation
   - Remediation recommendations

2. **Design System Compliance**
   - Design system rule validation
   - Component consistency checking
   - Brand guideline enforcement

## Dependencies

1. **Agent Registry**
   - Required for orchestrator-controlled instantiation

2. **Shared Logging Utility**
   - Used for consistent error logging and reporting

<!-- End docs\prd\agents\a9_ui_design_agent_prd.md -->

---
<!-- Begin docs\prd\services\a9_data_product_mcp_service_prd.md -->

# A9 Data Product MCP Service PRD

<!-- 
CANONICAL PRD DOCUMENT
This is the official, canonical PRD document for this agent.
Last updated: 2025-07-17
-->


## Registry-Driven KPI Logic & Pluggable Analytics Backend (2025-05-18)

**Future Consideration:**
Not all numeric columns are additive. In the future, the registry and MCP service may distinguish between additive measures (where SUM is appropriate) and non-additive numeric attributes (where only AVG, MIN, MAX make sense). For now, all numeric columns that could be meaningfully aggregated are listed as direct measures. This can be refined later if needed.

- The MCP service must parse `kpi_calculations` and `kpi_filters` from the registry for each product.
- It must dynamically generate SQL for both direct measures and KPIs (including formulas and standardized filters).
- The query execution layer must be pluggable, supporting local (DuckDB) and cloud (e.g., Google BigQuery) backends via configuration, with no change to business logic.
- This enables future-proof, standardized analytics for all users and easy adoption of new analytics engines.

## Overview
The A9_Data_Product_MCP_Service (Managed Compute Platform for Data Products) is now the single authoritative layer for all dynamic SQL generation for Agent9 data products. All logic for registry-driven SELECT, JOIN, GROUP BY, and aggregation SQL has been migrated here from the agent layer. The legacy DynamicSQLBuilder in agent utilities has been fully removed to avoid duplication and confusion.

This service provides business-ready, summarized, filtered, and pre-joined data products to Agent9 agents and orchestrators. It is initially enabled with tools for SAP Sample CSV test data, but is designed to be extensible to other data source types and platforms (e.g., SAP DataSphere, Snowflake, Google BigQuery).
**Note:** Other MCP services may be created in the future to serve different specialized functions (e.g., workflow orchestration, analytics, or ML serving). This PRD covers only the Data Product MCP Service.




### Development Environment Setup
- Install required dependencies from requirements.txt
- Configure environment variables in .env file based on .env.example

### Key Files and Entry Points
- Main agent implementation: `src/agents/new/A9_Data_Product_MCP_Service_Agent.py`
- Configuration model: `src/agents/new/agent_config_models.py`
- Agent card: `src/agents/new/cards/a9_data_product_mcp_service_agent_card.py`

### Test Data Location
- Sample data available in `test-data/` directory
- Test harnesses in `test-harnesses/` directory

### Integration Points
- Integrates with Agent Registry for orchestration
- Follows A2A protocol for agent communication
- Uses shared logging utility for consistent error reporting

## Implementation Guidance

### Suggested Implementation Approach
1. Start with the agent's core functionality
2. Implement required protocol methods
3. Add registry integration
4. Implement error handling and logging
5. Add validation and testing

### Core Functionality to Focus On
- Protocol compliance (A2A)
- Registry integration
- Error handling and logging
- Proper model validation

### Testing Strategy
- Unit tests for core functionality
- Integration tests with mock registry
- End-to-end tests with test harnesses

### Common Pitfalls to Avoid
- Direct agent instantiation (use registry pattern)
- Missing error handling
- Incomplete logging
- Improper model validation

## Success Criteria

### Minimum Viable Implementation
- Agent implements all required protocol methods
- Agent properly integrates with registry
- Agent handles errors and logs appropriately
- Agent validates inputs and outputs

### Stretch Goals
- Enhanced error handling and recovery
- Performance optimizations
- Additional features from Future Enhancements section

### Evaluation Metrics
- Protocol compliance
- Registry integration
- Error handling
- Logging quality
- Input/output validation

---

## Purpose
- Centralize all join, filter, and aggregation operations for data products (Data Product MCP Service)
- Deliver business-ready data to Agent9 agents via simple, secure API endpoints
- Ensure compliance, auditability, and performance at scale
- Establish a pattern for future MCP services with other specialized roles

---

## Initial Scope
- **Data Sources:** SAP Sample CSV test data (local or networked)
- **Operations:** Summarization (aggregation), filtering, pre-joining of data products as defined in the Agent9 registry
- **API:** REST or gRPC endpoints for requesting data products by product_id, with filter/KPI/group-by parameters
- **Governance:** RBAC, audit logging, registry-driven validation

---

## Future Scope
- Support for additional data source types:
  - SAP DataSphere
  - Snowflake
  - Google BigQuery
  - Other cloud/on-prem data platforms
- Dynamic query pushdown and optimization for large-scale datasets
- Materialized/cached result management for performance
- Enhanced monitoring and lineage tracking

---

## Technical Requirements
- **Single Source of SQL Logic:** All dynamic SQL query generation (including SELECT, JOIN, GROUP BY, aggregation, and KPI logic) is handled exclusively in the MCP service. There is no dynamic SQL generation in the agent layer; the old DynamicSQLBuilder has been fully removed.
- Registry-driven: All data product metadata, joins, and KPIs defined in the Agent9 registry
- Modular connectors for each supported data source type
- Secure API endpoints with RBAC and audit logging
- **Centralized Logging:** All logging, error, and audit events in the Data Product MCP Service must use the `A9_SharedLogger` (see backlog rationale). No local logger instances or ad-hoc logging are permitted. Logging must be structured, centralized, and propagated to the orchestrator where relevant, supporting compliance and maintainability.
- Configurable for dev/test/prod environments
- Extensible architecture for new data platforms
### Temporal Additivity for Account-Like Data Models
- The MCP service must support correct aggregation logic for Account-like data models (e.g., General Ledger, FinancialTransactions) where measures (such as VALUE) can be mixed in terms of temporal additivity.
- The service must leverage AccountType (or equivalent master data) to distinguish between additive (e.g., P&L accounts) and non-additive (e.g., Balance Sheet accounts) measures.
- For additive measures (P&L), aggregation (e.g., SUM) across time is valid.
- For non-additive measures (Balance Sheet), aggregation across time is not valid; only period-end values should be reported.
- KPI definitions must explicitly specify AccountType-based aggregation rules when referencing such measures.

---

## API Contract (MVP, as of 2025-06-22)
- **POST /execute-sql**
  - Accepts: protocol-compliant LLM/user-supplied SQL (DuckDB backend only)
  - Input model: `SQLExecutionRequest` (fields: `sql: str`, `context: Optional[dict]`, `user: str`)
  - Output model: `SQLExecutionResult` (fields: `columns: List[str]`, `rows: List[List[Any]]`, `row_count: int`, `error: Optional[str]`)
  - Security: Only `SELECT` statements allowed (DDL/DML rejected), RBAC stub, audit logging via `A9_SharedLogger`
  - Returns: query result rows and columns, or error if invalid

- (Legacy/Planned) **GET /data-product/{product_id}**
  - Registry-driven, summarized, filtered, pre-joined business-ready data (CSV, JSON, or DataFrame)
- (Legacy/Planned) **POST /data-product/{product_id}/refresh**
  - Triggers rebuild of temp/pre-joined source
- (Legacy/Planned) **GET /data-product/{product_id}/preview**
  - Returns sample rows for UI/testing

---

## Compliance & Governance
- All requests and results logged for audit using `A9_SharedLogger`
- RBAC enforced per product and operation
- Registry is the single source of truth for allowed joins, filters, and KPIs
- All API errors and access denials are structured and logged via the shared logger

---

## Impact Analysis
- **Positive:**
  - Simplifies agent logic, improves maintainability and compliance
  - Centralizes governance, audit, and performance optimization
  - Enables future support for enterprise-scale data sources
- **Risks:**
  - Initial performance may be limited by CSV/local test data
  - Requires robust registry and API validation to prevent errors

---

## Agent9 Design Standards Compliance

The Data Product MCP Service conforms to Agent9's core architectural and compliance standards as follows:

| Design Standard                | Applies to MCP? | Notes                                                      |
|------------------------------- |:--------------:|------------------------------------------------------------|
| Centralized Logging            |      Yes       | Must use `A9_SharedLogger`                                 |
| Registry-Driven                |      Yes       | All product/source metadata from registry                  |
| RBAC & Audit Logging           |      Yes       | Required for all access                                    |
| Environment Awareness          |      Yes       | Dev/test/prod config support                               |
| Extensibility                  |      Yes       | Modular for new source types                               |
| Error Handling Patterns        |      Yes       | Use structured/template patterns                           |
| Documentation & PRD Process    |      Yes       | PRD, impact analysis, test case tracking                   |
| Test Planning                  |      Yes       | Integration-first, document test impact                    |
| SDK/Template Compliance        |   Partial      | Follow patterns, not inheritance                           |
| Agent Template Inheritance     |      No        | Not an agent                                               |
| Agent Registration/Discovery   |      No        | Referenced, not registered                                 |
| HITL Support                   |      No        | Not required                                               |
| Agent-to-Agent Protocols       |      No        | Service endpoints only                                     |
| Agent Naming/Prefix            |      No        | Use service-oriented naming                                |

The MCP service follows all applicable Agent9 standards for logging, registry-driven logic, RBAC, extensibility, and compliance. See the backlog for refactoring and enforcement tasks.

---

## Test Cases (Initial)
- Requesting a summarized data product with filters returns correct, business-ready data
- RBAC prevents unauthorized access to restricted data products
- Audit logs contain all access and error events
- Registry changes are reflected in MCP endpoints
- Extensibility: Add new data source connector and validate with integration test

---

## Documentation
- API reference and usage examples
- Registry integration guide
- Data source connector documentation
- Governance and compliance policies

---

## Implementation Details

### Fallback Logging Implementation
- Implements an in-line `DummyLogger` class as a fallback when the standard `A9_SharedLogger` is not available
- Ensures logging capabilities are maintained even in standalone or test environments
- Provides consistent logging interface regardless of environment context
- Automatically falls back to standard logging patterns when shared logger is unavailable

### RBAC Stub Implementation
- Includes a stub implementation of Role-Based Access Control (RBAC) for future expansion
- Provides placeholder methods for access control validation
- Supports future integration with enterprise authentication systems
- Includes audit logging for all access attempts and authorizations

### Logical-to-Physical Column Mapping
- Implements logical-to-physical column mapping inside the `execute_sql` method
- Translates business-friendly column names to actual database field names
- Supports dynamic mapping based on data source and schema
- Handles special cases and naming conventions across different data sources

### Registry-Driven Measures Extraction
- Implements registry-driven direct measures extraction logic
- Dynamically builds measure selection based on registry definitions
- Supports both direct measures and calculated KPIs
- Handles measure validation against available data sources

### CSV-Based Dynamic Column Extraction
- Implements CSV-based dynamic column extraction for the `select_fields` method
- Supports flexible field selection based on user queries
- Handles CSV file format variations and encoding issues
- Includes validation to ensure selected fields exist in the data source

### SQL Builder Usage and Error Handling
- Implements comprehensive error handling and logging patterns for SQL preview/generation
- Provides detailed error messages for invalid SQL or missing data
- Includes validation for SQL injection prevention
- Supports graceful degradation when errors occur

### Pandas Integration
- Uses pandas for registry and CSV operations
- Provides DataFrame-based intermediate representations
- Supports efficient data transformation and manipulation
- Includes optimized memory usage patterns for large datasets

### Duplicate Endpoint Implementation
- Contains duplicate definition of `execute_sql` to support backward compatibility
- Maintains consistent behavior across different API versions
- Includes deprecation warnings for legacy endpoint usage
- Provides migration path for clients using older API versions

<!-- End docs\prd\services\a9_data_product_mcp_service_prd.md -->
